{"title":"努力，奋斗","description":null,"language":"zh-CN","link":"https://shang.at","pubDate":"Sun, 17 May 2020 15:16:15 GMT","lastBuildDate":"Tue, 19 May 2020 06:07:15 GMT","generator":"hexo-generator-json-feed","webMaster":"王尚","items":[{"title":"Java学习-线程安全的集合类","link":"https://shang.at/post/Java学习-线程安全的集合类/","description":"","pubDate":"Sun, 17 May 2020 15:16:15 GMT","guid":"https://shang.at/post/Java学习-线程安全的集合类/","category":""},{"title":"Java学习-Iterator","link":"https://shang.at/post/Java学习-Iterator/","description":"在看Iterator之前，先看一个早期版本的迭代器java.util.Enumeration public interface Enumeration&lt;E&gt; &#123; boolean hasMoreElements(); E nextElement();&#125; * NOTE: The functionality of this interface is duplicated by the Iterator* interface. In addition, Iterator adds an optional remove operation, and* has shorter method names. New implementations should consider using* Iterator in preference to Enumeration. 现在来看Iterator： public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); default void remove() &#123; throw new UnsupportedOperationException(\"remove\"); &#125; default void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (hasNext()) action.accept(next()); &#125;&#125; 注意的点： Iterator在好的设计下可以在遍历的过程中对列表进行增加和删除和修改元素 Iterator在遍历的过程只能进行一遍，即遍历完的对象不能再次遍历， 因为大多数Iterator在实现的过程中都是维护了了cursor指针，这个指针一般只会增加，不会减少 同时大都没有充值cursor指针的接口 关键是看Iterator的设计如何 例如下面的ListIterator public interface ListIterator&lt;E&gt; extends Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); boolean hasPrevious(); E previous(); int nextIndex(); int previousIndex(); void remove(); void set(E e); void add(E e);&#125; ListIterator在原来的Iterator的基础上扩展了，使之可以往前遍历，同时可以修改和增加元素","pubDate":"Sun, 17 May 2020 07:27:13 GMT","guid":"https://shang.at/post/Java学习-Iterator/","category":"JAVA源码"},{"title":"Java学习-函数式编程","link":"https://shang.at/post/Java学习-函数式编程/","description":"Java8之后加入了一种全新的方式来实现方法(功能)作为参数传递的机制：lambda表达式 像python语言，天生就支持将function作为参数传递给函数. 可以想象，既然是一种实现方法作为参数传递的机制，java是一种面向对象的编程语言，也就是说在java中除了原始数据类型之外，都是对象： &gt; @FunctionalInterface&gt; interface CharBinaryOperator &#123;&gt; String applyAsChar(char left, char right);&gt; &#125;&gt; &gt; CharBinaryOperator charBinaryOperator = (char a, char b) -&gt; &#123;&gt; System.out.println(a);&gt; System.out.println(b);&gt; return String.valueOf(a + b);&gt; &#125;;&gt; charBinaryOperator instanceof CharBinaryOperator charBinaryOperator是CharBinaryOperator的一个实例 CharBinaryOperator.class instanceof Class CharBinaryOperator.class是Class的一个实例 Class.class instanceof Class Class.class同时也是Class的一个实例 在java中，传递的参数要么是原始数据类型，要么是对象(类型也是对象，所以能够传递)，不能是其他的类型。在JDK8之前，要想将一个功能传递到函数内部(这一般会被称为函数回调，是大多数异步编程的常用套路：到达某个时间节点或满足某中情况触发一个操作)，那么就只能显示的先定义一个接口，然后创建一个实现了这个接口的类，然后再实例化这个类得到一个对象，最后将这个对象作为参数传入函数，函数内部调用对象实现的方法，如： CharBinaryOperator charBinaryOperator1 = new CharBinaryOperator() &#123; @Override public String applyAsChar(char a, char b) &#123; System.out.println(a); System.out.println(b); return String.valueOf(a + b); &#125;&#125;;charBinaryOperator.applyAsChar('a', 'b'); 在JDK8及之后，我们不需要再显示的做这一系列的事情(当然你这么做也不会有问题)。 在JDK8及之后，所有满足条件的interface都会被解释函数式接口，即都可以通过lambda表达式的形式代替上述流程 什么样的interface才算满足条件呢？ 只声明了一个未实现的函数的interface就可以。在JDK8及以后，interface中定义的函数也可以有默认的实现 在声明接口的时候，可以使用java.lang.FunctionalInterface注解，表示该interface是一个函数式接口(当然可以不加，compiler会自动判断) 如果在有多个未实现的函数的interface上加这个注解的时候，编译阶段就会报错：Multiple non-overriding abstract methods found in interface OOXX lambda表达式只不过是为了实现这个机制的一种解决方案，可以提高开发效率，同时隐藏了interface的定义细节，compilier完全是按照参数列表来推断当前的lambda表达式是和哪一个interface绑定的(compilier直接找到接口的定义，不是推断的)。如果没有预定义的，那么就会在编译期间报错，所以在java中lambda表达式的使用是有一定的限制的。 同时，在使用JDK预定义的操作时，在内部是调用了接口内定义的那个具体的函数的，所以对于开发者来说也是透明的，如列表的forEach()函数 &gt; default void forEach(Consumer&lt;? super T&gt; action) &#123;&gt; Objects.requireNonNull(action);&gt; for (T t : this) &#123;&gt; action.accept(t);&gt; &#125;&gt; &#125;&gt; // 我们在使用的时候是这样的&gt; List&lt;Integer&gt; integers = new ArrayList&lt;&gt;();&gt; integers.add(1);&gt; integers.forEach(i -&gt; &#123;&gt; System.out.println(i);&gt; &#125;);&gt; 本质上，还是要先有interface的定义(在JDK中已经预定义了大部分的interface，所以我们才不用自己手动定义，我上面的例子中就是一个没有被预定义的例子)，运行结果也是创建了一个实现了指定接口的对象，然后将对象作为参数传递给函数 如果我们完全脱离了JDK预定义的操作，那么我们就需要自己定义innterface，并且在我们使用该接口的地方显示的声明方法的使用，但是在外层传递方法参数的调用，我们仍可以使用简洁明了的lambda表达式，无论怎么说，lambda表达式的这种机制极大的方便了开发人员 Lambda 表达式和匿名类之间的区别 this 关键字。对于匿名类 this 关键字解析为匿名类，而对于 Lambda 表达式，this 关键字解析为包含写入 Lambda 的类。 JDK中预定义的interface在java.util.function可以看到全部的预定义的interface，以下四种是最有代表性的 Function R apply(T) Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) Function&lt;T, T&gt; identity() Consumer void accept(T) Consumer&lt; T&gt; addThen(Consumer&lt;? super T&gt;) Predicate boolean test() Predicate&lt; T&gt; add(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; negate() Predicate&lt; T&gt; or(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; isEqual(Object) Supplier T get() Lambda 表达式的例子1 线程初始化线程可以初始化如下： // Old waynew Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"Hello world\"); &#125;&#125;).start();// New waynew Thread( () -&gt; System.out.println(\"Hello world\")).start(); 2 事件处理事件处理可以用 Java 8 使用 Lambda 表达式来完成。以下代码显示了将 ActionListener 添加到 UI 组件的新旧方式： // Old waybutton.addActionListener(new ActionListener() &#123; @Override public void actionPerformed(ActionEvent e) &#123; System.out.println(\"Hello world\"); &#125;&#125;);// New waybutton.addActionListener( (e) -&gt; &#123; System.out.println(\"Hello world\");&#125;); 3 遍例输出（方法引用）输出给定数组的所有元素的简单代码。请注意，还有一种使用 Lambda 表达式的方式。 // old wayList&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);for (Integer n : list) &#123; System.out.println(n);&#125;// 使用 -&gt; 的 Lambda 表达式list.forEach(n -&gt; System.out.println(n));// 使用 :: 的 Lambda 表达式list.forEach(System.out::println); 6.4 逻辑操作输出通过逻辑判断的数据。 package com.wuxianjiezh.demo.lambda;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class Main &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); System.out.print(\"输出所有数字：\"); evaluate(list, (n) -&gt; true); System.out.print(\"不输出：\"); evaluate(list, (n) -&gt; false); System.out.print(\"输出偶数：\"); evaluate(list, (n) -&gt; n % 2 == 0); System.out.print(\"输出奇数：\"); evaluate(list, (n) -&gt; n % 2 == 1); System.out.print(\"输出大于 5 的数字：\"); evaluate(list, (n) -&gt; n &gt; 5); &#125; public static void evaluate(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) &#123; for (Integer n : list) &#123; if (predicate.test(n)) &#123; System.out.print(n + \" \"); &#125; &#125; System.out.println(); &#125;&#125; 运行结果： 输出所有数字：1 2 3 4 5 6 7 不输出：输出偶数：2 4 6 输出奇数：1 3 5 7 输出大于 5 的数字：6 7 6.4 Stream API 示例java.util.stream.Stream接口 和 Lambda 表达式一样，都是 Java 8 新引入的。所有 Stream 的操作必须以 Lambda 表达式为参数。Stream 接口中带有大量有用的方法，比如 map() 的作用就是将 input Stream 的每个元素，映射成output Stream 的另外一个元素。 下面的例子，我们将 Lambda 表达式 x -&gt; x*x 传递给 map() 方法，将其应用于流的所有元素。之后，我们使用 forEach 打印列表的所有元素。 // old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);for(Integer n : list) &#123; int x = n * n; System.out.println(x);&#125;// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);list.stream().map((x) -&gt; x*x).forEach(System.out::println); 下面的示例中，我们给定一个列表，然后求列表中每个元素的平方和。这个例子中，我们使用了 reduce() 方法，这个方法的主要作用是把 Stream 元素组合起来。 // old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = 0;for(Integer n : list) &#123; int x = n * n; sum = sum + x;&#125;System.out.println(sum);// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = list.stream().map(x -&gt; x*x).reduce((x,y) -&gt; x + y).get();System.out.println(sum);","pubDate":"Sun, 17 May 2020 00:38:33 GMT","guid":"https://shang.at/post/Java学习-函数式编程/","category":"JAVA源码"},{"title":"Java学习-集合类","link":"https://shang.at/post/Java学习-集合类/","description":"[TOC] 总览 注：这里只列举了单一线线程使用的集合对象(Vector除外) JVAV中列表类集合，按照数据的存储方式可以分为两大类：基于数组和基于链表。两种方式各有好处，需要根据实际业务场景做出选择。 数组 优点 支持随机访问，给定下标的访问是O(1)的时间复杂度 缺点 内存必须是连续的，否则会申请空间失败 查找、插入、扩容、删除都是O(n)的时间复杂度 有容量的限制，增加节点时，可能会因为数组大小不够导致扩容，扩容的时间复杂度是O(n)的 使用注意 最好能够预估数据的最大容量，可以预先设计capacity，尽量避免扩容操作 但是在特别的使用场景下，基于数组的实现效率会更好，比如下面要说的ArrayDeque 具体实现 ArrayList Vector Stack ArrayDeque 链表 优点 内存不用是连续的 插入、删除都是O(1)的时间复杂度 没有容量的限制，按理说限制就是JVAV堆的大小限制 缺点 查找是O(n)的时间复杂度 使用 单独使用链表的时候，还挺少的，毕竟一个没有附加特性的链表结构，仅仅只能够做到新增和删除的时间复杂度为O(1)，但是查询却需要O(n)，并且还需要额外的空间存储链表结构。数组可以通过预估容量的方式尽量减少扩容的操作，对比发现，使用基于数组的集合性价比更高 具体实现 LinkList LinkList在定位低index个元素的时候，有个优化的点可以学习 Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 总结：可以发现，基于数组和基于链表的集合实现方式，想要从他们中查询到具体的元素，时间复杂度都是O(n)的，这是因为，这里仅仅考虑了数据的存储方式，并没有额外的信息给出来，所以是没有办法加速查询的。要想实现加速查询，那么就必须在这基础上增加新的特性： 数组 有序性：可以借助有序性使用二分查找，把查找时间复杂度降到O(log n) 链表 建立树结构： 二叉搜索树：前序遍历就是正向排序，可以把查询的时间复杂度降到O(log n)，但是要维护二叉搜索，尽量保证他是平衡的(但是这个的时间复杂度是O(1)的) 堆：查找最大(最小)值是O(1)的时间复杂度 升维：比如跳表，就是在有序的链表上建立多级索引来实现加速查询的，可以把查找时间复杂度降到O(log n)，但是在新增和删除节点时需要维护多级索引(但是这个的时间复杂度是O(1)的) List 接口信息如下： Method Return Comment Insert add(E) boolean 向队列加入元素，如果空间不足，会触发扩容 Insert add(int, E) void 向指定位置插入元素，可能会抛IndexOutOfBoundsException Remove remove(Object) boolean 移除指定的元素，没有的话返回false，有的话返回true Remove remove(int) E 移除指定index的元素，可能会抛IndexOutOfBoundsException Examine get(int) E 返回指定index的元素，可能会抛IndexOutOfBoundsException Update set(int, E) E 更新指定index的元素，可能会抛IndexOutOfBoundsException 在List的源码中发现多处这样的代码： // 只返回第一个遇到的o，当o为null的时候，o.equals会报错public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 详看扩容操作 public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;// DEFAULT_CAPACITY=10 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // newCapacity = int(1.5*oldCapacity) int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; // 整形溢出：Integer.MAX_VALUE + 8&lt;0 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Queue 设计了一套支持队列操作的接口，如下： Method Return Comment Insert add(E) boolean 向队列添加一个元素，如果没有空间会抛出IllegalStateException Insert offer(E) boolean 向队列添加一个元素，如果没有空间会返回false Remove remove() E 移除并返回头结点，如果队列为空的话，会抛NoSuchElementException Remove poll() E 移除并返回头结点，如果队列为空的话，会返回null Examine element() E 返回头结点，如果队列为空的话，会抛NoSuchElementException Examine peek() E 返回头结点，如果队列为空的话，会返回null Deque Stack 总结：通常使用ArrayDeque来作为先进先出的Queue，后进先出的Stack","pubDate":"Sat, 16 May 2020 23:27:00 GMT","guid":"https://shang.at/post/Java学习-集合类/","category":"JAVA源码"},{"title":"分布式服务框架-IPC&RPC","link":"https://shang.at/post/分布式服务框架-IPC-RPC/","description":"","pubDate":"Thu, 14 May 2020 03:38:18 GMT","guid":"https://shang.at/post/分布式服务框架-IPC-RPC/","category":"分布式"},{"title":"Java学习-代理","link":"https://shang.at/post/Java学习-代理/","description":"","pubDate":"Thu, 14 May 2020 03:34:00 GMT","guid":"https://shang.at/post/Java学习-代理/","category":"JAVA"},{"title":"Mysql学习-事务和隔离级别","link":"https://shang.at/post/Mysql学习-事务和隔离级别/","description":"MYSQL事务和隔离级别一、事务事务是由一组SQL语句组成的逻辑处理单元，是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。事务具有以下4个属性，通常简称为事务的ACID属性: 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 二、并发一致性问题1、更新丢失(Lost Update)T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。 如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读一句话：事务B读取到了事务A已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果A事务回滚Rollback，B读取的数据无效，不符合一致性要求。 解决办法: 把数据库的事务隔离级别调整到 READ_COMMITTED T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 3、不可重复读(Non-Repeatable Reads) 在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 一句话：一个事务范围内两个相同的查询却返回了不同数据。 同时操作，事务1分别读取事务2操作时和提交后的数据，读取的记录内容不一致。不可重复读是指在同一个事务内，两个相同的查询返回了不同的结果。 解决办法: 如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。把数据库的事务隔离级别调整到REPEATABLE_READ T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4、幻读一个事务T1按相同的查询条件重新读取以前检索过的数据，却发现其他事务T2插入了满足其查询条件的新数据，这种现象就称为“幻读”。（和可重复读类似，但是事务 T2 的数据操作仅仅是插入和删除，不是修改数据，读取的记录数量前后不一致） 一句话：事务A 读取到了事务B提交的新增数据，不符合隔离性。 解决办法: 如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。把数据库的事务隔离级别调整到 SERIALIZABLE_READ。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 三、事务隔离级别“脏读”、”不可重复读”和”幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。 MYSQL常看当前数据库的事务隔离级别：show variables like &#39;tx_isolation&#39;; 1、读未提交 (Read Uncommitted)最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。 2、读已提交 (Read Committed)被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。 3、可重复读(Repeatable Read)所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。(引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。) MYSQL默认是REPEATABLE-READ。 4、串行化(Serializable)所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。 5、总结 读未提交: 一个事务还没提交时，它做的变更就能被别的事务看到。 读提交: 一个事务提交之后，它做的变更才会被其他事务看到。 可重复读 : 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化: 顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 四个级别逐渐增强，每个级别解决一个问题，事务级别越高，性能越差，大多数环境(Read committed 就可以用了) 隔离级别 读数据一致性 脏读 不可重复读 幻读 未提交读 最低级别 √ √ √ 提交读 语句级 × √ √ 可重复读 事务级 × × √ 可串行化 最高级别,事务级 × × × 参考","pubDate":"Tue, 12 May 2020 17:31:26 GMT","guid":"https://shang.at/post/Mysql学习-事务和隔离级别/","category":"Mysql"},{"title":"Mysql学习-第三范式","link":"https://shang.at/post/Mysql学习-第三范式/","description":"第一范式(1NF) 数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性 符合1NF的关系中的每个属性都不可再分 有两点要求： schema定义：每个属性不可再分，即字段的含义要明确，同一个字段不应该有多于1个的含义 图中的这种schema在RDBMS中是不可能存在的，也就是无法创建的。可以改成如下的schema: 存储的数据：同一列中不能有多个值 图中的一个字段里面存了多个值，这种情况在RDBMS中是可以存在的，但是该字段是可再分的，应该。可以将数据分成多条存储，如下图 第二范式(2NF) 满足第一范式 没有部分依赖 在同一个表中，不能存在某些字段依赖一些键，而另一些字段依赖另外一些键 员工表的一个候选键是{id，mobile，deptNo}，而deptName依赖于deptNo，同样 name 依赖于 id，因此不是 2NF的。为了满足第二范式的条件，需要将这个表拆分成employee、dept、employee_dept、employee_mobile四个表 不满足2NF的表，可能存在的问题：修改异常、新增异常、删除异常 第三范式(3NF) 满足第二范式 没有传递依赖 在同一个表中，不要存在字段A依赖字段B，同时字段B依赖字段C，推导出来字段A间接依赖字段C的关系。 员工表的province、city、district依赖于zip，而zip依赖于id，换句话说，province、city、district传递依赖于id，违反了 3NF 规则。为了满足第三范式的条件，可以将这个表拆分成employee和zip两个表 但是这种关系也不是一定不能存在，视具体的业务而定吧 示例假设有一个名为employee的员工表，它有九个属性：id(员工编号)、name(员工名称)、mobile(电话)、zip(邮编)、province(省份)、city(城市)、district(区县)、deptNo(所属部门编号)、deptName(所属部门名称)、表总数据如下： idnamemobilezipprovincecitydistrictdeptNodeptName101张三 1391000000113910000002100001北京北京海淀区D1部门1101张三1391000000113910000002100001北京北京海淀区D2部门2102李四13910000003200001上海上海静安区D3部门3103王五13910000004510001广东省广州白云区D4部门4103王五13910000004510001广东省广州白云区D5部门 5 将上表改成满足第1范式，如下： idnamemobilezipprovincecitydistrictdeptNodeptName101张三13910000001100001北京北京海淀区D1部门1101张三13910000002100001北京北京海淀区D1部门1101张三13910000001100001北京北京海淀区D2部门2101张三13910000002100001北京北京海淀区D2部门2102李四13910000003200001上海上海静安区D3部门3103王五13910000004510001广东省广州白云区D4部门4103王五13910000004510001广东省广州白云区D5部门5 仍存在的问题 修改异常：上表中张三、王五都有多条记录，因为他隶属于两个部门。如果我们要修改王五的地址，必修修改两行记录。假如一个部门得到了王五的新地址并进行了更新，而另一个部门没有，那么此时王五在表中会存在两个不同的地址，导致了数据不一致 新增异常：假如一个新员工假如公司，他正处于入职培训阶段，还没有被正式分配到某个部门，如果deptNo字段不允许为空，我们就无法向employee表中新增该员工的数据。 删除异常：假设公司撤销了D3部门，那么在删除deptNo为D3的行时，会将李四的信息也一并删除。因为他隶属于D3这一部门。 为了解决上面的问题，我们可以将上述表设计成满足3NF 在关系数据库模型设计中，一般需要满足第三范式的要求。如果一个表具有良好的主外键设计，就应该是满足3NF的表。规范化带来的好处是通过减少数据冗余提高更新数据的效率，同时保证数据完整性。然而，我们在实际应用中也要防止过度规范化的问题。规范化程度越高，划分的表就越多，在查询数据时越有可能使用表连接操作。而如果连接的表过多，会影响查询性能。关键的问题是要依据业务需求，仔细权衡数据查询和数据更新关系，指定最合适的规范化程度。不要为了遵循严格的规范化规则而修改业务需求。 参考","pubDate":"Tue, 12 May 2020 17:19:15 GMT","guid":"https://shang.at/post/Mysql学习-第三范式/","category":"Mysql"},{"title":"Python学习-函数参数传递","link":"https://shang.at/post/Python学习-函数参数传递/","description":"在Python(估计也适用于其他的语言)中，函数参数的传递分为两类 值传递和引用传递，实际上这两类传递类型都是属于变量传值，即： 值传递：将实际参数值复制一份传递到函数内，这样在函数内对参数进行修改，就不会影响到原参数 引用传递：将实际参数的地址直接传递到函数内，那么在函数内对参数所进行的修改，将可能会影响到原参数 要注意的是，在函数内修改参数，实际上又分为两种情况(仅说引用传递)： 1、对参数(a)重新进行赋值操作(a=new_obj)，此时，实际上修改的已经不是传递给函数的最初的参数(a)了，它已经指向了其他的内存地址，这时再修改a，实际上就和之前的对象没有任何关系了 2、直接对a进行修改，比如说a.name=’sdd’，这时，原始的对象就会发生变化","pubDate":"Sun, 19 Apr 2020 04:00:49 GMT","guid":"https://shang.at/post/Python学习-函数参数传递/","category":"Python"},{"title":"Python学习-OrderedDict","link":"https://shang.at/post/Python学习-OrderedDict/","description":"from collections import OrderedDict # 记录插入顺序的dict，操作方式和dict一样。# 是基于dict和双端队列实现，可以用来实现LRUcache","pubDate":"Thu, 16 Apr 2020 08:50:57 GMT","guid":"https://shang.at/post/Python学习-OrderedDict/","category":"Python"},{"title":"Python学习-bisect","link":"https://shang.at/post/Python学习-bisect/","description":"这个模块对有序列表提供了支持，使得他们可以在插入新数据仍然保持有序。对于长列表，如果其包含元素的比较操作十分昂贵的话，这可以是对更常见方法的改进 \"\"\"Bisection algorithms.\"\"\"def insort_right(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the right of the rightmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 a.insert(lo, x)insort = insort_right # backward compatibilitydef bisect_right(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt;= x, and all e in a[i:] have e &gt; x. So if x already appears in the list, a.insert(x) will insert just after the rightmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 return lobisect = bisect_right # backward compatibilitydef insort_left(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the left of the leftmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid a.insert(lo, x)def bisect_left(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt; x, and all e in a[i:] have e &gt;= x. So if x already appears in the list, a.insert(x) will insert just before the leftmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid return lo# Overwrite above definitions with a fast C implementationtry: from _bisect import *except ImportError: pass","pubDate":"Thu, 16 Apr 2020 08:47:50 GMT","guid":"https://shang.at/post/Python学习-bisect/","category":"Python"},{"title":"数据结构与算法学习笔记-查找算法","link":"https://shang.at/post/数据结构与算法学习笔记-查找算法/","description":"总结 查找算法 时间复杂度 二分查找 O($$logn$$) O(logn)","pubDate":"Fri, 10 Apr 2020 00:58:45 GMT","guid":"https://shang.at/post/数据结构与算法学习笔记-查找算法/","category":"数据结构与算法"},{"title":"Spark应用之import spark.implicits._","link":"https://shang.at/post/Spark应用之import-spark-implicits/","description":"在初期使用spark的时候，大家都会遇见一个很奇怪的写法import spark.implicits._ 这里面包含了四个关键字：import、spark、implicits、_ import和_实际上是Scala中包引入的写法，表示引入指定包内的所有成员 本文主要想记录一下另外两个关键字：spark、implicits 关键字一：spark关键字二：implicits","pubDate":"Wed, 25 Mar 2020 09:10:54 GMT","guid":"https://shang.at/post/Spark应用之import-spark-implicits/","category":"Spark"},{"title":"hadoop源码学习一","link":"https://shang.at/post/hadoop源码学习一/","description":"先导知识 JAVA-代理 IPC/RPC","pubDate":"Wed, 10 Jul 2019 03:03:43 GMT","guid":"https://shang.at/post/hadoop源码学习一/","category":"Hadoop"},{"title":"Pandas-学习","link":"https://shang.at/post/Pandas-学习/","description":"","pubDate":"Tue, 11 Jun 2019 01:53:35 GMT","guid":"https://shang.at/post/Pandas-学习/","category":"Pandas"},{"title":"Python学习-时间处理","link":"https://shang.at/post/Python学习-时间处理/","description":"关于时间戳的几个概念时间戳，根据1970年1月1日00:00:00开始按秒计算的偏移量。时间元组（struct_time），包含9个元素。 time.struct_time(tm_year=2017, tm_mon=10, tm_mday=1, tm_hour=14, tm_min=21, tm_sec=57, tm_wday=6, tm_yday=274, tm_isdst=0) 时间格式字符串，字符串形式的时间。time模块与时间戳和时间相关的重要函数 time.time() # 生成当前的时间戳，格式为10位整数的浮点数。time.strftime() # 根据时间元组生成时间格式化字符串。time.strptime() # 根据时间格式化字符串生成时间元组。time.strptime()与time.strftime()为互操作。time.localtime() # 根据时间戳生成当前时区的时间元组。time.mktime() # 根据时间元组生成时间戳。 示例 import time##生成当前时间的时间戳，只有一个参数即时间戳的位数，默认为10位，输入位数即生成相应位数的时间戳，比如可以生成常用的13位时间戳def now_to_timestamp(digits = 10): time_stamp = time.time() digits = 10 ** (digits -10) time_stamp = int(round(time_stamp*digits)) return time_stamp##将时间戳规范为10位时间戳def timestamp_to_timestamp10(time_stamp): time_stamp = int (time_stamp* (10 ** (10-len(str(time_stamp))))) return time_stamp##将当前时间转换为时间字符串，默认为2017-10-01 13:37:04格式def now_to_date(format_string=\"%Y-%m-%d %H:%M:%S\"): time_stamp = int(time.time()) time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将10位时间戳转换为时间字符串，默认为2017-10-01 13:37:04格式def timestamp_to_date(time_stamp, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将时间字符串转换为10位时间戳，时间字符串默认为2017-10-01 13:37:04格式def date_to_timestamp(date, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.strptime(date, format_string) time_stamp = int(time.mktime(time_array)) return time_stamp##不同时间格式字符串的转换def date_style_transfomation(date, format_string1=\"%Y-%m-%d %H:%M:%S\",format_string2=\"%Y-%m-%d %H-%M-%S\"): time_array = time.strptime(date, format_string1) str_date = time.strftime(format_string2, time_array) return str_dateprint(now_to_date())print(timestamp_to_date(1506816572))print(date_to_timestamp('2017-10-01 08:09:32'))print(timestamp_to_timestamp10(1506816572546))print(date_style_transfomation('2017-10-01 08:09:32')) 结果为 15068362240002017-10-01 13:37:042017-10-01 08:09:3215068165721506816572","pubDate":"Thu, 06 Jun 2019 08:37:46 GMT","guid":"https://shang.at/post/Python学习-时间处理/","category":"Python"},{"title":"Spark学习笔记-Configuration","link":"https://shang.at/post/Spark学习笔记-Configuration/","description":"submit 参数 运行时可配置参数：在代码中使用spark.conf.set(‘’， ‘’)的方式设置。运行时设置的参数不会在WebUI中显示","pubDate":"Mon, 03 Jun 2019 09:32:33 GMT","guid":"https://shang.at/post/Spark学习笔记-Configuration/","category":"Spark"},{"title":"Python学习-队列","link":"https://shang.at/post/Python学习-队列/","description":"队列from queue import Queue #LILO队列q = Queue() #创建队列对象q.put(0) #在队列尾部插入元素q.put(1)q.put(2)print('LILO队列',q.queue) #查看队列中的所有元素print(q.get()) #返回并删除队列头部元素print(q.queue)from queue import LifoQueue #LIFO队列lifoQueue = LifoQueue()lifoQueue.put(1)lifoQueue.put(2)lifoQueue.put(3)print('LIFO队列',lifoQueue.queue)lifoQueue.get() #返回并删除队列尾部元素lifoQueue.get()print(lifoQueue.queue)from queue import PriorityQueue #优先队列priorityQueue = PriorityQueue() #创建优先队列对象priorityQueue.put(3) #插入元素priorityQueue.put(78) #插入元素priorityQueue.put(100) #插入元素print(priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.put(1) #插入元素priorityQueue.put(2) #插入元素print('优先级队列:',priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue)priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('全部被删除后:',priorityQueue.queue) #查看优先级队列中的所有元素from collections import deque #双端队列dequeQueue = deque(['Eric','John','Smith'])print(dequeQueue)dequeQueue.append('Tom') #在右侧插入新元素dequeQueue.appendleft('Terry') #在左侧插入新元素print(dequeQueue)dequeQueue.rotate(2) #循环右移2次print('循环右移2次后的队列',dequeQueue)dequeQueue.popleft() #返回并删除队列最左端元素print('删除最左端元素后的队列：',dequeQueue)dequeQueue.pop() #返回并删除队列最右端元素print('删除最右端元素后的队列：',dequeQueue)","pubDate":"Mon, 03 Jun 2019 02:10:48 GMT","guid":"https://shang.at/post/Python学习-队列/","category":"Python"},{"title":"Spark学习笔记-广播变量","link":"https://shang.at/post/Spark学习笔记-广播变量/","description":"Shared Variables通常，当在远程集群节点上执行传递给Spark操作（例如mapor reduce）的函数时，它将在函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上的变量的更新不会传播回驱动程序。支持跨任务的通用，读写共享变量效率低下。但是，Spark确实为两种常见的使用模式提供了两种有限类型的共享变量：广播变量和累加器。 Broadcast广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。 Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 广播变量是v通过调用从变量创建的SparkContext.broadcast(v)。广播变量是一个包装器v，可以通过调用该value 方法来访问它的值。下面的代码显示了这个： &gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 创建广播变量后，应该使用它来代替v群集上运行的任何函数中的值，这样v就不会多次传送到节点。此外，在v广播之后不应修改对象 ，以确保所有节点获得相同的广播变量值（例如，如果稍后将变量发送到新节点）。 Performance Tuning == Physical Plan ==InMemoryTableScan [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075] +- InMemoryRelation [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075], true, 10000, StorageLevel(disk, 1 replicas) +- *(34) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, ((coalesce(nanvl(total_disburse_amount#5203, null), 0.0) - cast(coalesce(total_repay_principal_amount#5974, 0) as double)) - coalesce(nanvl(total_write_off_principal#5986, null), 0.0)) AS total_outstanding_amount_ex_dp90#6075] +- SortMergeJoin [bill_create_date#4955], [write_off_date#4776], LeftOuter :- *(23) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203, total_repay_principal_amount#5974] : +- SortMergeJoin [bill_create_date#4955], [repay_date#5789], LeftOuter : :- *(6) Sort [bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(bill_create_date#4955, 200) : : +- *(5) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203] : : +- Window [sum(disburse_amount#5197) windowspecdefinition(1, bill_create_date#4955 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_disburse_amount#5203], [1], [bill_create_date#4955 ASC NULLS FIRST] : : +- *(4) Sort [1 ASC NULLS FIRST, bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(1, 200) : : +- *(3) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[sum(cast(principal#615 as double))]) : : +- Exchange hashpartitioning(bill_create_date#4955, week_last_day#5015, month_last_day#5075, 200) : : +- *(2) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[partial_sum(cast(principal#615 as double))]) : : +- *(2) Project [principal#615, cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) AS bill_create_date#4955, next_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), Sun) AS week_last_day#5015, last_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date)) AS month_last_day#5075] : : +- *(2) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : :- *(2) Project [id#392] : : : +- *(2) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : +- *(2) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : : +- *(1) Project [loan_id#609, principal#615, create_time#632L] : : +- *(1) Filter ((cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) &lt;= 18043) &amp;&amp; isnotnull(loan_id#609)) : : +- *(1) FileScan parquet [loan_id#609,principal#615,create_time#632L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id)], ReadSchema: struct&lt;loan_id:string,principal:string,create_time:bigint&gt; : +- *(22) Sort [repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(21) Project [repay_date#5789, total_repay_principal_amount#5974] : +- Window [sum(repay_principal_amount#5970) windowspecdefinition(1, repay_date#5789 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_repay_principal_amount#5974], [1], [repay_date#5789 ASC NULLS FIRST] : +- *(20) Sort [1 ASC NULLS FIRST, repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(1, 200) : +- *(19) HashAggregate(keys=[repay_date#5789], functions=[sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(18) HashAggregate(keys=[repay_date#5789], functions=[partial_sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- *(18) Project [repaid_principal#684, write_off_date#4776, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repay_date#5789] : +- SortMergeJoin [loan_id#609], [loan_id#5672], LeftOuter : :- *(12) Sort [loan_id#609 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(loan_id#609, 200) : : +- *(11) Project [loan_id#609, repaid_principal#684, repay_time#692] : : +- *(11) BroadcastHashJoin [id#608], [bill_id#670], Inner, BuildRight : : :- *(11) Project [id#608, loan_id#609] : : : +- *(11) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : : :- *(11) Project [id#392] : : : : +- *(11) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : : +- *(11) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true])) : : : +- *(7) Project [id#608, loan_id#609] : : : +- *(7) Filter (isnotnull(loan_id#609) &amp;&amp; isnotnull(id#608)) : : : +- *(7) FileScan parquet [id#608,loan_id#609] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id), IsNotNull(id)], ReadSchema: struct&lt;id:string,loan_id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(10) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(10) Filter ((isnotnull(rn#5382) &amp;&amp; (rn#5382 = 1)) &amp;&amp; (cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) &lt;= 18043)) : : +- Window [row_number() windowspecdefinition(bill_id#670, repay_time#692 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#5382], [bill_id#670], [repay_time#692 DESC NULLS LAST] : : +- *(9) Sort [bill_id#670 ASC NULLS FIRST, repay_time#692 DESC NULLS LAST], false, 0 : : +- Exchange hashpartitioning(bill_id#670, 200) : : +- *(8) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(8) Filter isnotnull(bill_id#670) : : +- *(8) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(bill_id)], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt; : +- *(17) Sort [loan_id#5672 ASC NULLS FIRST], false, 0 : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[first(write_off_date#4705, true)]) : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[partial_first(write_off_date#4705, true)]) : +- *(17) Project [loan_id#5672, write_off_date#4705] : +- *(17) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight : :- *(17) Project [loan_id#5672, write_off_date#4705, bill_id#4714] : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4718], LeftOuter, BuildRight : : :- *(17) Project [loan_id#5672, write_off_date#4705] : : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4708], LeftOuter, BuildRight : : : :- *(17) HashAggregate(keys=[loan_id#5672], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- Exchange hashpartitioning(loan_id#5672, 200) : : : : +- *(13) HashAggregate(keys=[loan_id#5672], functions=[partial_min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- *(13) Project [loan_id#5672, (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) AS is_write_off_bill#4594, date_add(cast(due_date#5686 as date), 91) AS write_off_date#4630] : : : : +- *(13) Filter (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) : : : : +- *(13) FileScan parquet [loan_id#5672,status#5676,due_date#5686,repay_time#5694L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;loan_id:string,status:string,due_date:string,repay_time:bigint&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : : +- *(14) Project [id#392 AS loan_id#4708] : : : +- *(14) FileScan parquet [id#392] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) : : +- *(15) Project [id#4717 AS bill_id#4714, loan_id#4718] : : +- *(15) FileScan parquet [id#4717,loan_id#4718] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,loan_id:string&gt; : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : +- *(16) FileScan parquet [bill_id#670] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string&gt; +- *(33) Sort [write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(32) Project [write_off_date#4776, total_write_off_principal#5986] +- Window [sum(write_off_principal#5982) windowspecdefinition(1, write_off_date#4776 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_write_off_principal#5986], [1], [write_off_date#4776 ASC NULLS FIRST] +- *(31) Sort [1 ASC NULLS FIRST, write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(1, 200) +- *(30) HashAggregate(keys=[write_off_date#4776], functions=[sum(write_off_principal#4779)]) +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(29) HashAggregate(keys=[write_off_date#4776], functions=[partial_sum(write_off_principal#4779)]) +- SortAggregate(key=[loan_id#609], functions=[first(write_off_date#4705, true), first(amount#398, true), sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- SortAggregate(key=[loan_id#609], functions=[partial_first(write_off_date#4705, true), partial_first(amount#398, true), partial_sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- *(28) Sort [loan_id#609 ASC NULLS FIRST], false, 0 +- *(28) Project [loan_id#609, write_off_date#4705, amount#398, repaid_principal#684, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repayment_date#4760] +- *(28) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight :- *(28) Project [loan_id#609, write_off_date#4705, amount#398, bill_id#4714] : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4718], LeftOuter, BuildRight : :- *(28) Project [loan_id#609, write_off_date#4705, amount#398] : : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4708], LeftOuter, BuildRight : : :- *(28) HashAggregate(keys=[loan_id#609], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : +- ReusedExchange [loan_id#609, min#6101], Exchange hashpartitioning(loan_id#5672, 200) : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(25) Project [id#392 AS loan_id#4708, amount#398] : : +- *(25) FileScan parquet [id#392,amount#398] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,amount:string&gt; : +- ReusedExchange [bill_id#4714, loan_id#4718], BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) +- *(27) Project [bill_id#670, repay_time#692, repaid_principal#684] +- *(27) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt;","pubDate":"Tue, 28 May 2019 08:19:03 GMT","guid":"https://shang.at/post/Spark学习笔记-广播变量/","category":"Spark"},{"title":"数据结构学习笔记二-算法","link":"https://shang.at/post/数据结构学习笔记二-算法/","description":"递归二分查找哈希算法堆排序深度和广度优先搜索字符串匹配贪心算法##分治算法 回溯算法动态规划拓扑排序最短路径并行算法","pubDate":"Thu, 16 May 2019 09:54:29 GMT","guid":"https://shang.at/post/数据结构学习笔记二-算法/","category":"数据结构与算法"},{"title":"Spark学习笔记-pivot透视图","link":"https://shang.at/post/Spark学习笔记-pivot透视图/","description":"df = spark.createDataFrame([ ('2018-01','项目1',100, 'xm'), ('2018-01','项目1',100, 'xl'), ('2018-01','项目1',100, 'xp'), ('2018-01','项目2',200, 'ch'), ('2018-01','项目3',300, 'xl'), ('2018-02','项目1',1000, 'xp'), ('2018-02','项目2',2000, 'xl'), ('2018-03','项目x',999, 'xm')], ['date','project','income', 'saler']) df.toPandas() date project income saler 0 2018-01 项目1 100 xm 1 2018-01 项目1 100 xl 2 2018-01 项目1 100 xp 3 2018-01 项目2 200 ch 4 2018-01 项目3 300 xl 5 2018-02 项目1 1000 xp 6 2018-02 项目2 2000 xl 7 2018-03 项目x 999 xm pivotdf_pivot = df.groupBy('date').pivot( 'project', ['项目1', '项目2', '项目3', '项目x']).agg( sum('income')).na.fill(0)df_pivot.toPandas() date 项目1 项目2 项目3 项目x 0 2018-03 0 0 0 999 1 2018-02 1000 2000 0 0 2 2018-01 300 200 300 0 df.groupBy('project').pivot( 'date').agg( sum('income')).na.fill(0).toPandas() project 2018-01 2018-02 2018-03 0 项目2 200 2000 0 1 项目x 0 0 999 2 项目1 300 1000 0 3 项目3 300 0 0 unpivotdf_pivot.selectExpr(\"date\", \"stack(4, '项目11', `项目1`, '项目22', `项目2`, '项目33', `项目3`, '项目xx', `项目x`) as (`project`,`income`)\")\\ .filter(\"income &gt; 0 \")\\ .orderBy([\"date\", \"project\"])\\ .toPandas() date project income 0 2018-01 项目11 300 1 2018-01 项目22 200 2 2018-01 项目33 300 3 2018-02 项目11 1000 4 2018-02 项目22 2000 5 2018-03 项目xx 999 stack(n, expr1, ..., exprk) 将k个[expr1, ..., exprk]拆解成n rows","pubDate":"Thu, 09 May 2019 02:52:07 GMT","guid":"https://shang.at/post/Spark学习笔记-pivot透视图/","category":"Spark"},{"title":"Spark学习笔记-tips","link":"https://shang.at/post/Spark学习笔记-tips/","description":"写spark dataframe的时候，最好用哪些字段就取哪些字段，否则spark会默认把所有字段都读进内存，如果进行cache操作，就会无故占用大量内存 没有被明确select的字段依然可以作为filter的条件 获取周的第一天日期和当前日期位于周的第几天，周的第一天定义不同 周日 周一 Spark Shuffle spill (Memory) and (Disk) on SPARK UI? What do they mean? https://community.hortonworks.com/questions/202809/spark-shuffle-spill-memory.html 窗口函数会引起重分区吗？分区数(200)是固定的吗？ test_df = kreditpintar.spark.range(0, end=100, numPartitions=5).toDF(&apos;input&apos;)test_df.rdd.getNumPartitions() # 5test_1_df = test_df.withColumn(&apos;id&apos;, row_number().over(Window.partitionBy(lit(1)).orderBy(&apos;input&apos;)))test_1_df.rdd.getNumPartitions() # 200test_2_df = test_df.withColumn(&apos;id&apos;, monotonically_increasing_id())test_2_df.rdd.getNumPartitions() # 5 通过withColumn(‘group’, lit(‘aaaabbb’))添加的新列，不能最为后续的join操作的condition expression？ groupBy 和 窗口函数的实现原理 哪一个效率更高 groupby 、窗口函数、distinct三种方式去重 哪个效率高 distinct&gt;groupby&gt;窗口函数 循环的去跑脚本，然后union每次循环的结果。 这样的使用 task可能会失败，需要优化 转化long列类型到时间戳，保留毫秒信息 a_df = spark.createDataFrame([[1556613225852]], ['a'])a_df.select((col('a')/1000.0).cast('timestamp')).toPandas()#CAST((a / 1000.0) AS TIMESTAMP)#0 2019-04-30 08:33:45.852 spark进行计算的过程中间检查数据没有问题，但是执行collect后出现数据不一致的情况(丢失数据和union后的数据重复)","pubDate":"Mon, 15 Apr 2019 05:50:00 GMT","guid":"https://shang.at/post/Spark学习笔记-tips/","category":"Spark"},{"title":"BI工具使用之Tableau一","link":"https://shang.at/post/BI工具使用之Tableau一/","description":"","pubDate":"Thu, 11 Apr 2019 07:08:39 GMT","guid":"https://shang.at/post/BI工具使用之Tableau一/","category":"BI"},{"title":"Spark学习笔记-DSL语法","link":"https://shang.at/post/Spark学习笔记-DSL语法/","description":"","pubDate":"Sun, 31 Mar 2019 02:20:06 GMT","guid":"https://shang.at/post/Spark学习笔记-DSL语法/","category":"Spark"},{"title":"数据结构与算法学习笔记-排序算法","link":"https://shang.at/post/数据结构与算法学习笔记-排序算法/","description":"O(n^2)冒泡排序(Bubble Sort) 算法描述 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否符合大小关系要求。如果不满足就互换位置。一次冒泡至少会让一个元素移动到它应该在的位置，重复n次，就完成了n个元素的排序工作。 算法实现 def bubble_sort(nums): \"\"\" 冒泡排序：从小到大 :param nums: :return: \"\"\" if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(i + 1, len(nums)): if nums[i] &gt; nums[j]: nums[i], nums[j] = nums[j], nums[i] return nums def bubble_sort1(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(len(nums) - 1 - i): if nums[j] &gt; nums[j + i]: nums[j], nums[j + 1] = nums[j + 1], nums[j] return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34] print(bubble_sort(nums)) 插入排序 算法描述 将一个元素插入一个已经有序的序列，使其依然有序。首先，将原始的序列分为两个子序列，有序的和无序的，然后，从无序的序列中依次拿出一个元素，插入到有序的序列的合适位置，并保持有序的序列依然有序，直到无序的序列中没有元素了。 算法实现 def insert_sort(nums): if len(nums) &lt;= 1: return nums for i in range(1, len(nums)): tmp = nums[i] j = i - 1 for j in range(i - 1, -1, -1): if tmp &lt; nums[j]: nums[j + 1] = nums[j] else: break nums[j + 1] = tmp return nums if __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(insert_sort(nums)) 选择排序 算法描述 选择排序是选择无序序列中的最小的元素放到有序序列的末尾，直到无序序列没有元素。 算法实现 def selection_sort(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): min_val = nums[i] min_j = i for j in range(i + 1, len(nums)): if min_val &gt; nums[j]: min_val = nums[j] min_j = j nums[i], nums[min_j] = nums[min_j], nums[i] return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(selection_sort(nums)) 希尔排序 算法描述 希尔排序是对插入排序的优化。希尔排序，通过将原始序列按照一定的步长划分为多个子序列 将原始的一维数组映射成二维数组， 然后按列进行插入排序，这样的话，可以让一个元素在一次比较中跨越较大的区间，随后算法在使用较小的步长，一直到步长为1(已知当对有序度较高数组进行排序时，插入排序的时间复杂度接近O(N)，因此可以大幅度提高插入排序的效率)。 常见的步长选择有 算法实现 def shell_sort(list): n = len(list) # 初始步长 gap = n // 2 while gap &gt; 0: print(gap) for i in range(gap, n): # 每个步长进行插入排序 temp = list[i] j = i # 插入排序 while j &gt;= gap and list[j - gap] &gt; temp: list[j] = list[j - gap] j -= gap print('inner=', list) list[j] = temp print(list) # 得到新的步长 gap = gap // 2 return listdef shell_sort1(collection): # Marcin Ciura's gap sequence gaps = [701, 301, 132, 57, 23, 10, 4, 1] for gap in gaps: i = gap while i &lt; len(collection): temp = collection[i] j = i while j &gt;= gap and collection[j - gap] &gt; temp: collection[j] = collection[j - gap] j -= gap collection[j] = temp i += 1 return collectionif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', shell_sort1(nums)) O(nlogn)归并排序 算法描述 将数组分为两部分，分别排序，最后将两部分排好序的数组合并成一个有序的数组。利用递归的方式，重复上述过程。 算法实现 def merge_sort(nums): print('before=', nums) length = len(nums) if length &gt; 1: midpoint = length // 2 left_half = merge_sort(nums[:midpoint]) right_half = merge_sort(nums[midpoint:]) i = 0 j = 0 k = 0 left_length = len(left_half) right_length = len(right_half) while i &lt; left_length and j &lt; right_length: if left_half[i] &lt; right_half[j]: nums[k] = left_half[i] i += 1 else: nums[k] = right_half[j] j += 1 k += 1 while i &lt; left_length: nums[k] = left_half[i] i += 1 k += 1 while j &lt; right_length: nums[k] = right_half[j] j += 1 k += 1 print('after=', nums) return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', merge_sort(nums)) 快速排序 算法描述 随机选择一个pivot节点，然后将数组中的数据分成大于pivot和小于pivot的两部分，然后递归地将大于pivot和小于pivot的部分再按照相同的思路处理，直到每个pivot两端的部分都只有最多一个元素 算法实现 def quick_sort(collection): length = len(collection) if length &lt;= 1: return collection else: pivot = collection[0] greater = [element for element in collection[1:] if element &gt; pivot] lesser = [element for element in collection[1:] if element &lt;= pivot] return quick_sort(lesser) + [pivot] + quick_sort(greater) O(n) 时间复杂度内求无序数组中的第 K 大元素 # 选择数组的最后一个元素，作为pivot，然后将数组的所有元素分为大于pivot和小于pivot的两部分，# 如果 len(lesser) == k - 1，则返回pivot# 如果 len(lesser) &gt;= k，则说明要查找的元素在小于pivot的部分，那么继续在lesser中查找# 否则的话，说明要查找的元素在大于pivot的部分，那么继续在greater中查找def find_k_max(nums, k): length = len(nums) if length &lt; k: return None pivot = nums[length - 1] greater = [element for element in nums[:length - 1] if element &gt; pivot] lesser = [element for element in nums[:length - 1] if element &lt;= pivot] if len(lesser) == k - 1: return pivot elif len(lesser) &gt;= k: return find_k_max(lesser, k) else: return find_k_max(greater, k - len(lesser) - 1) 堆排序 算法描述 算法实现 O(n)计数排序 算法描述 算法实现 基数排序 算法描述 算法实现 桶排序 算法描述 算法实现 总结 排序算法 时间复杂度 空间复杂度 稳定性 是否基于比较 冒泡排序 O($$n^2$$) O(n^2) O(1) 是 是 选择排序 O($$n^2$$) O(n^2) O(1) 否 是 插入排序 O($$n^2$$) O(n^2) O(1) 是 是 希尔排序 O($$nlog^2n$$) O(nlog^2n) O(1) 是 是 归并排序 O($$nlogn$$) O(nlogn) O(n) 是 是 快速排序 O($$nlogn$$) O(nlogn) O(1) 否 是 堆排序 O($$nlogn$$) O(nlogn) 是 计数排序 O($$n$$) O(n) 否 基数排序 O($$n$$) O(n) 否 桶排序 O($$n$$) O(n) 否","pubDate":"Fri, 29 Mar 2019 00:49:58 GMT","guid":"https://shang.at/post/数据结构与算法学习笔记-排序算法/","category":"数据结构与算法"}]}