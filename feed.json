{"title":"努力，奋斗","description":null,"language":"zh-CN","link":"https://shang.at","pubDate":"Sat, 04 Jul 2020 14:20:22 GMT","lastBuildDate":"Sun, 12 Jul 2020 22:53:39 GMT","generator":"hexo-generator-json-feed","webMaster":"王尚","items":[{"title":"操作系统-CentOS7时区设置","link":"https://shang.at/post/操作系统-CentOS7时区设置/","description":"简介：","pubDate":"Sat, 04 Jul 2020 14:20:22 GMT","guid":"https://shang.at/post/操作系统-CentOS7时区设置/","category":"centos7时区设置"},{"title":"Linux-ssh","link":"https://shang.at/post/Linux-ssh/","description":"简介：","pubDate":"Sat, 04 Jul 2020 14:13:25 GMT","guid":"https://shang.at/post/Linux-ssh/","category":"Linux"},{"title":"Python学习-deque","link":"https://shang.at/post/Python学习-deque/","description":"简介：","pubDate":"Thu, 02 Jul 2020 15:52:28 GMT","guid":"https://shang.at/post/Python学习-deque/","category":"Python"},{"title":"数据结构与算法-单调栈和窗口及其更新结构","link":"https://shang.at/post/数据结构与算法-单调栈和窗口及其更新结构/","description":"简介：","pubDate":"Thu, 02 Jul 2020 11:07:20 GMT","guid":"https://shang.at/post/数据结构与算法-单调栈和窗口及其更新结构/","category":"数据结构与算法"},{"title":"Python学习-标准库学习","link":"https://shang.at/post/Python学习-标准库学习/","description":"简介：","pubDate":"Wed, 01 Jul 2020 09:17:07 GMT","guid":"https://shang.at/post/Python学习-标准库学习/","category":"Python"},{"title":"Python学习-itertools","link":"https://shang.at/post/Python学习-itertools/","description":"简介：","pubDate":"Wed, 01 Jul 2020 09:16:51 GMT","guid":"https://shang.at/post/Python学习-itertools/","category":"Python"},{"title":"Spark应用-大数据集的处理","link":"https://shang.at/post/Spark应用-大数据集的处理/","description":"简介：","pubDate":"Wed, 01 Jul 2020 09:12:03 GMT","guid":"https://shang.at/post/Spark应用-大数据集的处理/","category":"Spark应用"},{"title":"Python学习-类的特殊方法","link":"https://shang.at/post/Python学习-类的特殊方法/","description":"简介：通过定义具有特殊名称的方法，类可以实现某些通过特殊语法调用的操作（例如算术运算或下标和切片）。这是Python运算符重载的方法，它允许类针对语言运算符定义自己的行为。例如，如果一个类定义了一个名为__getitem__()的方法，并且x是该类的实例，则x[i]大致等效于type(x).__getitem__(x, i)。除非另有说明，否则当未定义适当的方法（通常为AttributeError或TypeError）时，尝试执行操作会引发异常。 将特殊方法设置为None表示相应的操作不可用。例如，如果一个类将__iter__()设置为None，则该类不可迭代，因此在其实例上调用iter()会引发TypeError（而不会回到__getitem__()）。 默认情况下，我们自定义的类，它的这些特殊方法都有默认的实现。","pubDate":"Wed, 01 Jul 2020 02:22:24 GMT","guid":"https://shang.at/post/Python学习-类的特殊方法/","category":"Python"},{"title":"Java学习-JMH","link":"https://shang.at/post/Java学习-JMH/","description":"简介：本节学习Java准测试工具套件-JMH","pubDate":"Tue, 30 Jun 2020 15:13:41 GMT","guid":"https://shang.at/post/Java学习-JMH/","category":"JAVA学习"},{"title":"算法-动态规划","link":"https://shang.at/post/算法-动态规划/","description":"简介：动态规划是高级的算法思想，本节主要记录DP的分析思路","pubDate":"Mon, 29 Jun 2020 17:54:22 GMT","guid":"https://shang.at/post/算法-动态规划/","category":"数据结构与算法"},{"title":"Python学习-python3.6-dict有序且效率更高","link":"https://shang.at/post/Python学习-python3-6-dict有序且效率更高/","description":"https://www.cnblogs.com/xieqiankun/p/python_dict.html","pubDate":"Sun, 28 Jun 2020 23:22:42 GMT","guid":"https://shang.at/post/Python学习-python3-6-dict有序且效率更高/","category":"Python"},{"title":"Hadoop学习","link":"https://shang.at/post/Hadoop学习/","description":"简介：","pubDate":"Fri, 26 Jun 2020 16:34:48 GMT","guid":"https://shang.at/post/Hadoop学习/","category":"Hadoop学习"},{"title":"Hadoop学习-Yarn-Scheduler","link":"https://shang.at/post/Hadoop学习-Yarn-Scheduler/","description":"","pubDate":"Thu, 25 Jun 2020 06:24:45 GMT","guid":"https://shang.at/post/Hadoop学习-Yarn-Scheduler/","category":"Hadoop学习"},{"title":"Hadoop学习-Yarn-Container","link":"https://shang.at/post/Hadoop学习-Yarn-Container/","description":"","pubDate":"Thu, 25 Jun 2020 06:24:29 GMT","guid":"https://shang.at/post/Hadoop学习-Yarn-Container/","category":"Hadoop学习"},{"title":"Hadoop学习-如何实现一个在Yarn上的Application","link":"https://shang.at/post/Hadoop学习-如何实现一个在Yarn上的Application/","description":"","pubDate":"Thu, 25 Jun 2020 01:27:51 GMT","guid":"https://shang.at/post/Hadoop学习-如何实现一个在Yarn上的Application/","category":"Hadoop学习"},{"title":"Hadoop学习-Mapreduce编程模式","link":"https://shang.at/post/Hadoop学习-Mapreduce编程模式/","description":"","pubDate":"Thu, 25 Jun 2020 01:26:50 GMT","guid":"https://shang.at/post/Hadoop学习-Mapreduce编程模式/","category":"Hadoop学习"},{"title":"Hive学习-安装","link":"https://shang.at/post/Hive学习-安装/","description":"配置过程 mysql 安装 配置可以远程连接 jdbc driver hive使用的jdbc drive要与mysql的版本匹配 hive配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;!-- hive-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hostmachine:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;the URL of the MySQL database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive1234&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; &lt;value&gt;SchemaTable&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置数据存放在hdfs上的路径 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置metastore service 的节点 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node2:9083&lt;/value&gt; &lt;description&gt;IP address (or fully-qualified domain name) and port of the metastore host&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动过程 首先启动hdfs：start-dfs.sh 初始化metestore：schematool --dbType mysql --initSchema 启动metastore service：hive --service metastore 启动hive：hive https://www.jianshu.com/p/6108e0aed204","pubDate":"Wed, 24 Jun 2020 07:41:54 GMT","guid":"https://shang.at/post/Hive学习-安装/","category":"Hive学习"},{"title":"数据库-mysql-环境配置","link":"https://shang.at/post/数据库-mysql-环境配置/","description":"platform：MAC 安装1234567891011121314brew istall mysqlWe've installed your MySQL database without a root password. To secure it run: mysql_secure_installationMySQL is configured to only allow connections from localhost by defaultTo connect run: mysql -urootTo have launchd start mysql now and restart at login: brew services start mysqlOr, if you don't want/need a background service you can just run: mysql.server start 新安装的mysql，需要重置密码： The initial root account may or may not have a password. Choose whichever of the following procedures applies: If the root account exists with an initial random password that has been expired, connect to the server as root using that password, then choose a new password. This is the case if the data directory was initialized using mysqld —initialize, either manually or using an installer that does not give you the option of specifying a password during the install operation. Because the password exists, you must use it to connect to the server. But because the password is expired, you cannot use the account for any purpose other than to choose a new password, until you do choose one. If you do not know the initial random password, look in the server error log. Connect to the server as root using the password: 12shell&gt; mysql -u root -p Enter password: (enter the random root password here) Choose a new password to replace the random password: mysql&gt; ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘root-password’; If the root account exists but has no password, connect to the server as root using no password, then assign a password. This is the case if you initialized the data directory using mysqld —initialize-insecure. Connect to the server as root using no password: 1shell&gt; mysql -u root --skip-password Assign a password: 1mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'root-password'; After assigning the root account a password, you must supply that password whenever you connect to the server using the account. For example, to connect to the server using the mysql client, use this command: 12shell&gt; mysql -u root -p Enter password: (enter root password here) To shut down the server with mysqladmin, use this command: 12shell&gt; mysqladmin -u root -p shutdown Enter password: (enter root password here) 设置远程访问设置my.cnf使用brew 安装的mysql，my.cnf文件在/usr/local/etc/my.cnf，修改bind-address为0.0.0.0，然后重启brew services restart mysql 创建用户，赋予权限1234567891011121314# 登录mysqlmysql -u root -p 123456# 创建新用户create user 'hive' identified by 'hive1234';# 授权grant all privileges on *.* to 'hive'@'%' with grant option;# *.* 前边的*号指的是数据库，后面的*号指的是表，*.*的意思就是任意数据库下的任意表# 'root'@'%'，'root'用户名，'%'任意的主机名。# 这条配置信息就是说，允许任意节点以root身份登录，并且可以访问mysql里的任意库下的任意表# 刷新flush privileges; 至此，便可以在其他host上访问mysql服务了。 其他问题 jar记得更新 server 时区","pubDate":"Wed, 24 Jun 2020 06:16:33 GMT","guid":"https://shang.at/post/数据库-mysql-环境配置/","category":"数据库"},{"title":"Hadoop学习-Shell脚本学习","link":"https://shang.at/post/Hadoop学习-Shell脚本学习/","description":"","pubDate":"Wed, 24 Jun 2020 05:28:09 GMT","guid":"https://shang.at/post/Hadoop学习-Shell脚本学习/","category":"Hadoop学习"},{"title":"工具使用-iterm2","link":"https://shang.at/post/工具使用-iterm2/","description":"标签12345新建标签：command + t关闭标签：command + w切换标签：command + 数字 command + 左右方向键切换全屏：command + enter查找：command + f 分屏12345垂直分屏：command + d水平分屏：command + shift + d切换屏幕：command + option + 方向键 command + [ 或 command + ]查看历史命令：command + ;查看剪贴板历史：command + shift + h 其他1234567891011121314151617181920212223清除当前行：ctrl + u到行首：ctrl + a到行尾：ctrl + e前进后退：ctrl + f/b (相当于左右方向键)上一条命令：ctrl + p搜索命令历史：ctrl + r删除当前光标的字符：ctrl + d删除光标之前的字符：ctrl + h删除光标之前的单词：ctrl + w删除到文本末尾：ctrl + k交换光标处文本：ctrl + t清屏1：command + r清屏2：ctrl + l自带有哪些很实用的功能/快捷键⌘ + 数字在各 tab 标签直接来回切换选择即复制 + 鼠标中键粘贴，这个很实用⌘ + f 所查找的内容会被自动复制⌘ + d 横着分屏 / ⌘ + shift + d 竖着分屏⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏ctrl + u 清空当前行，无论光标在什么位置输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令⌘ + shift + h 会列出剪切板历史可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现 常用的一些快捷键1234567891011121314⌘ + 1 / 2 左右 tab 之间来回切换，这个在 前面 已经介绍过了⌘← / ⌘→ 到一行命令最左边/最右边 ，这个功能同 C+a / C+e⌥← / ⌥→ 按单词前移/后移，相当与 C+f / C+b，其实这个功能在Iterm中已经预定义好了，⌥f / ⌥b，看个人习惯了好像就这几个设置方法如下当然除了这些可以自定义的也不能忘了 linux 下那些好用的组合C+a / C+e 这个几乎在哪都可以使用C+p / !! 上一条命令C+k 从光标处删至命令行尾 (本来 C+u 是删至命令行首，但iterm中是删掉整行)C+w A+d 从光标处删至字首/尾C+h C+d 删掉光标前后的自负C+y 粘贴至光标后C+r 搜索命令历史，这个较常用 选中即复制iterm2 有 2 种好用的选中即复制模式。 一种是用鼠标，在 iterm2 中，选中某个路径或者某个词汇，那么，iterm2 就自动复制了。 另一种是无鼠标模式，command+f,弹出 iterm2 的查找模式，输入要查找并复制的内容的前几个字母，确认找到的是自己的内容之后，输入 tab，查找窗口将自动变化内容，并将其复制。如果输入的是 shift+tab，则自动将查找内容的左边选中并复制。 自动完成输入打头几个字母，然后输入 command+; iterm2 将自动列出之前输入过的类似命令。 剪切历史输入 command+shift+h，iterm2 将自动列出剪切板的历史记录。如果需要将剪切板的历史记录保存到磁盘，在 Preferences &gt; General &gt; Save copy/paste history to disk 中设置。","pubDate":"Tue, 23 Jun 2020 10:30:55 GMT","guid":"https://shang.at/post/工具使用-iterm2/","category":"工具使用"},{"title":"Hadoop学习-源码编译","link":"https://shang.at/post/Hadoop学习-源码编译/","description":"","pubDate":"Tue, 23 Jun 2020 10:18:16 GMT","guid":"https://shang.at/post/Hadoop学习-源码编译/","category":"Hadoop学习"},{"title":"Hadoop学习-集群搭建","link":"https://shang.at/post/Hadoop学习-集群搭建/","description":"配置12345678# hadoop.sh -&gt; /etc/profile.d/export HADOOP_PREFIX=/root/hadoopexport HADOOP_YARN_HOME=$&#123;HADOOP_PREFIX&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_PREFIX&#125;/etc/hadoopexport YARN_LOG_DIR=$&#123;HADOOP_YARN_HOME&#125;/logsexport YARN_IDENT_STRING=rootexport HADOOP_MAPRED_IDENT_STRING=rootexport PATH=$&#123;HADOOP_PREFIX&#125;/bin:$&#123;HADOOP_PREFIX&#125;/sbin:$&#123;PATH&#125; 12345678910111213&lt;!-- core-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/core-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node4:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- hdfs-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!-- config secondary namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node3:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- enable webhdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- disable permissions; only for development, of course --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122&lt;!-- yarn-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-common/yarn-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- enable log aggregation, this is false by default --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt; &lt;value&gt;node4&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 123456789101112&lt;!-- mapred-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node3:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动流程Step1. 第一次启动HDFS，需要格式化一下hdfs 1$HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster-name&gt; Step2. 启动 1234start-dfs.shstart-yarn.shyarn-daemon.sh start proxyservermr-jobhistory-daemon.sh start historyserver 停止流程1234stop-dfs.shstop-yarn.shyarn-daemon.sh stop proxyservermr-jobhistory-daemon.sh stop historyserver 访问Once the Hadoop cluster is up and running check the web-ui of the components as described below: Daemon Web Interface Notes NameNode http://nn_host:port/ Default HTTP port is 50070. SecondaryNameNode http://nn_host:port/ Default HTTP port is 50090. ResourceManager http://rm_host:port/ Default HTTP port is 8088. YarnWebProxy http://proxy_host:port/ no Default HTTP port. 需要自定义 MapReduce JobHistory Server http://jhs_host:port/ Default HTTP port is 19888.","pubDate":"Tue, 23 Jun 2020 10:17:38 GMT","guid":"https://shang.at/post/Hadoop学习-集群搭建/","category":"Hadoop学习"},{"title":"Hadoop学习-配置详解","link":"https://shang.at/post/Hadoop学习-配置详解/","description":"版本：2.7.3 core-default.xml parameter default value notes fs.defaultFS file:/// 定义namenode的URI，改成hdfs://host:port/ hadoop.tmp.dir /tmp/hadoop-${user.name} 定义其他临时目录的根目录 io.file.buffer.size 4096 读写文件操作时的缓存字节数，必须是硬件上的内存页大小的整数倍 hdfs-default.xmlnamenode parameter default value note dfs.namenode.http(s)-address 0.0.0.0:50070(50470) 配置dfs的web ui界面，不建议修改。可以通过http://namenode_hostname:50070访问 dfs.namenode.name.dir file://${hadoop.tmp.dir}/dfs/name 配置DFS namenode的fsimage文件存放在本次文件系统的路径。如果配置了使用逗号分隔的多个路径，那么namemode会在每个目录下面都冗余的存放一份。 dfs.namenode.edits.dir dfs.namenode.name.dir 配置DFS namenode的edits文件存放在本次文件系统的路径。如果配置了使用逗号分隔的多个路径，那么namemode会在每个目录下面都冗余的存放一份。 dfs.namenode.fs-limits.min-block-size 1048576 1m 最小块大小（以字节为单位），由Namenode在创建时强制执行。这样可以防止意外创建具有很小块大小（因此有很多块）的文件，这会降低性能。减少数据块的数量， dfs.namenode.handler.count 10 namenode端服务的线程数，测试时可以配置的小一些，减少内存占用 datanode parameter default value notes dfs.datanode.data.dir file://${hadoop.tmp.dir}/dfs/data 文件块的在local filesystem中的存放路径。如果提供的是逗号分隔的目录列表，那么数据将会存储在所有的目录中，(通常目录列表是在不同的设备上)。目录应该被相应的存储类型所标记(HDFS上有四种存储设备：SSD、DISK、ARCHIVE、RAM_DISK)，如果没有指定，默认是DISK。如果目录不存在，那么会自动创建(需要获取目录权限) dfs.datanode.handler.count 10 namenode端服务的线程数，测试时可以配置的小一些，减少内存占用 secondary namenode parameter default value notes dfs.namenode.secondary.http(s)-address 0.0.0.0:50090(50091) 配置secondary namenode的http server和端口 dfs.namenode.checkpoint.dir file://${hadoop.tmp.dir}/dfs/namesecondary dfs.namenode.checkpoint.edits.dir ${dfs.namenode.checkpoint.dir} dfs.namenode.checkpoint.period 3600 dfs.namenode.checkpoint.txns 1000000 dfs.namenode.checkpoint.check.period 60 dfs.namenode.checkpoint.max-retries 3 dfs.namenode.num.checkpoints.retained 2 dfs parameter default value notes dfs.permissions.enabled true 配置是否启用权限检查，默认是启用的。测试时可以设置为false。当开启状态时，dfs不会检测文件的权限检测。HDFS PermissionHDFS默认启动namenode的user为superuser，这个 dfs.blocksize 134217728 128m 单位字节，新文件的block 大小 dfs.hosts / dfs.hosts.exclude List of permitted/excluded DataNodes.If necessary, use these files to control the list of allowable datanodes. dfs.replication 3 块副本数 dfs.webhdfs.enabled true 启动namenode和datanode上的WebHHDFS(REST API) mapred-default.xmlMapReduce Applications Parameter Value Notes mapreduce.framework.name yarn Execution framework set to Hadoop YARN. mapreduce.map.memory.mb 1536 Larger resource limit for maps. mapreduce.map.java.opts -Xmx1024M Larger heap-size for child jvms of maps. mapreduce.reduce.memory.mb 3072 Larger resource limit for reduces. mapreduce.reduce.java.opts -Xmx2560M Larger heap-size for child jvms of reduces. mapreduce.task.io.sort.mb 100 Higher memory-limit while sorting data for efficiency. mapreduce.task.io.sort.factor 10 More streams merged at once while sorting files. mapreduce.reduce.shuffle.parallelcopies 5 Higher number of parallel copies run by reduces to fetch outputs from very large number of maps. MapReduce JobHistory Server Parameter Value Notes mapreduce.jobhistory.address 0.0.0.0:10020 MapReduce JobHistory Server IPC host:port mapreduce.jobhistory.webapp.address 0.0.0.0:19888 MapReduce JobHistory Server Web UI host:port yarn.app.mapreduce.am.staging-dir /tmp/hadoop-yarn/staging The staging dir used while submitting jobs. mapreduce.jobhistory.intermediate-done-dir ${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate Directory where history files are written by MapReduce jobs. mapreduce.jobhistory.done-dir ${yarn.app.mapreduce.am.staging-dir}/history/done Directory where history files are managed by the MR JobHistory Server. yarn-default.xmlResourceManager and NodeManager Parameter default Value Notes yarn.acl.enable false 是否开启ACLs yarn.admin.acl * ACL to set admins on the cluster. ACLs are of for comma-separated-usersspacecomma-separated-groups. Defaults to special value of * which means anyone. Special value of just space means no one has access. yarn.log-aggregation-enable false 是否启动日志聚合。日志聚合会收集每个container的日志并且在应用完成后将他们移动到HDFS中。具体目录由下面两个选项配置yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix。用户可以通过Application Timeline Server访问这些日志文件 ResourceManager Parameter default Value Notes yarn.resourcemanager.address ${yarn.resourcemanager.hostname}:8032 配置RM的URIfor clients to submit jobs. yarn.resourcemanager.scheduler.address ${yarn.resourcemanager.hostname}:8030 资源调度器URIfor ApplicationMasters to talk to Scheduler to obtain resources. yarn.resourcemanager.resource-tracker.address ${yarn.resourcemanager.hostname}:8031 for NodeManagers. yarn.resourcemanager.admin.address ${yarn.resourcemanager.hostname}:8033 for administrative commands. yarn.resourcemanager.webapp.address ${yarn.resourcemanager.hostname}:8088 RM web application yarn.resourcemanager.hostname 0.0.0.0 ResourceManager host.应该改成特定的hostname yarn.web-proxy.address 默认没有配置，会作为RM的一部分运行 The address for the web proxy as HOST:PORT, if this is not given then the proxy will run as part of the RM yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.&lt;br /&gt;scheduler.capacity.CapacityScheduler 指定RM使用的调度器：CapacityScheduler (recommended), FairScheduler (also recommended), or FifoScheduler yarn.scheduler.minimum-allocation-mb 1024 In MBs，Minimum limit of memory to allocate to each container request at the ResourceManager. yarn.scheduler.maximum-allocation-mb 8192 In MBs，Maximum limit of memory to allocate to each container request at the Resource Manager. yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path List of permitted/excluded NodeManagers.If necessary, use these files to control the list of allowable NodeManagers. NodeManager Parameter default Value Notes yarn.nodemanager.resource.memory-mb 8192 Resource i.e. available physical memory, in MB, for given NodeManager.Defines total available resources on the NodeManager to be made available to running containers yarn.nodemanager.vmem-pmem-ratio 2.1Maximum ratio by which virtual memory usage of tasks may exceed physical memory The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio. yarn.nodemanager.local-dirs ${hadoop.tmp.dir}/nm-local-dirComma-separated list of paths on the local filesystem where intermediate data is written. Multiple paths help spread disk i/o. yarn.nodemanager.log-dirs ${yarn.log.dir}/userlogsComma-separated list of paths on the local filesystem where logs are written. Multiple paths help spread disk i/o. yarn.nodemanager.log.retain-seconds 10800 Default time (in seconds) to retain log files on the NodeManager. Only applicable if log-aggregation is disabled. yarn.nodemanager.remote-app-log-dir /tmp/logs HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled. yarn.nodemanager.remote-app-log-dir-suffix logs Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled. yarn.nodemanager.aux-services mapreduce_shuffle Shuffle service that needs to be set for Map Reduce applications. History Server (Needs to be moved elsewhere) Parameter default Value Notes yarn.log-aggregation.retain-seconds -1 How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node. yarn.log-aggregation.retain-check-interval-seconds -1 Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.","pubDate":"Tue, 23 Jun 2020 09:12:52 GMT","guid":"https://shang.at/post/Hadoop学习-配置详解/","category":"Hadoop学习"},{"title":"Shell编程-常用命令","link":"https://shang.at/post/Shell编程-常用命令/","description":"文本编辑sedawkLinux上的定时器cron命令xargsexec远程控制sshscprsync软件管理rpm","pubDate":"Tue, 23 Jun 2020 08:53:17 GMT","guid":"https://shang.at/post/Shell编程-常用命令/","category":"Shell编程"},{"title":"操作系统-centos7修改hostname","link":"https://shang.at/post/操作系统-centos7修改hostname/","description":"在CentOS7中，有三种定义的主机名:静态的（static）、瞬态的（transient）、灵活的（pretty）。“静态”主机名也称为内核主机名，是系统在启动时从/etc/hostname自动初始化的主机名。“瞬态”主机名是在系统运行时临时分配的主机名，例如，通过DHCP或mDNS服务器分配。静态主机名和瞬态主机名都遵从作为互联网域名同样的字符限制规则。而另一方面，“灵活”主机名则允许使用自由形式（包括特殊/空白字符）的主机名，以展示给终端用户。 方法一1234567891011121314151617181920212223242526[root@Geeklp201 ~]# hostnamectl #查看一下当前主机名的情况 Static hostname: Geeklp201 Icon name: computer-vm Chassis: vm Machine ID: 77efa27de81d470883b5bb0ed04f468c Boot ID: fa62bd1c0f5e4e53a0691fb97971594f Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-693.el7.x86_64 Architecture: x86-64[root@Geeklp201 ~]# hostnamectl set-hostname geeklp --static[root@Geeklp201 ~]# hostnamectl status Static hostname: geeklp Pretty hostname: Geeklp201 Icon name: computer-vm Chassis: vm Machine ID: 77efa27de81d470883b5bb0ed04f468c Boot ID: fa62bd1c0f5e4e53a0691fb97971594f Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-693.el7.x86_64 Architecture: x86-64重启VM 方法二通过修改文件/etc/hostname来实现主机名的修改。把该文件内容替换成自己想要的主机名重启即可。","pubDate":"Tue, 23 Jun 2020 04:31:11 GMT","guid":"https://shang.at/post/操作系统-centos7修改hostname/","category":"centos7修改hostname"}]}