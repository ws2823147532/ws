{"meta":{"title":"努力，奋斗","subtitle":"记录学习","description":null,"author":"王尚","url":"https://shang.at","root":"/"},"pages":[{"title":"About","date":"2019-03-24T01:33:47.232Z","updated":"2019-03-24T01:33:47.232Z","comments":true,"path":"about/index.html","permalink":"https://shang.at/about/index.html","excerpt":"","text":"大数据工程师一枚 邮箱：2823147532@qq.com 此博客仅为个人学习数据结构和算法的一个笔记，就是想督促自己坚持学习。笔记内容主要来源于其他的博客和读完博客后的自己的一点思考"},{"title":"Tags","date":"2019-03-24T01:33:47.233Z","updated":"2019-03-24T01:33:47.233Z","comments":true,"path":"tags/index.html","permalink":"https://shang.at/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据结构与算法学习笔记-内存不足","slug":"数据结构与算法学习笔记-内存不足","date":"2020-05-31T23:35:34.000Z","updated":"2020-05-31T23:37:26.809Z","comments":true,"path":"post/数据结构与算法学习笔记-内存不足/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-内存不足/","excerpt":"","text":"中位数定义：数字排序之后，位于中间的那个数。比如将100亿个数字进行排序，排序之后，位于第50亿个位置的那个数 就是中位数。 ①内存够：内存够还慌什么啊，直接把100亿个全部排序了，你用冒泡都可以…然后找到中间那个就可以了。但是你以为面试官会给你内存？？ ②内存不够：题目说是整数，我们认为是带符号的int,所以4字节，占32位。 假设100亿个数字保存在一个大文件中，依次读一部分文件到内存(不超过内存的限制)，将每个数字用二进制表示，比较二进制的最高位(第32位，符号位，0是正，1是负)，如果数字的最高位为0，则将这个数字写入 file_0文件中；如果最高位为 1，则将该数字写入file_1文件中。 从而将100亿个数字分成了两个文件，假设 file_0文件中有 60亿 个数字，file_1文件中有 40亿 个数字。那么中位数就在 file_0 文件中，并且是 file_0 文件中所有数字排序之后的第 10亿 个数字。（file_1中的数都是负数，file_0中的数都是正数，也即这里一共只有40亿个负数，那么排序之后的第50亿个数一定位于file_0中） 现在，我们只需要处理 file_0 文件了（不需要再考虑file_1文件）。对于 file_0 文件，同样采取上面的措施处理：将file_0文件依次读一部分到内存(不超内存限制)，将每个数字用二进制表示，比较二进制的 次高位（第31位），如果数字的次高位为0，写入file_0_0文件中；如果次高位为1，写入file_0_1文件 中。 现假设 file_0_0文件中有30亿个数字，file_0_1中也有30亿个数字，则中位数就是：file_0_0文件中的数字从小到大排序之后的第10亿个数字。 抛弃file_0_1文件，继续对 file_0_0文件 根据 次次高位(第30位) 划分，假设此次划分的两个文件为：file_0_0_0中有5亿个数字，file_0_0_1中有25亿个数字，那么中位数就是 file_0_0_1文件中的所有数字排序之后的 第 5亿 个数。 按照上述思路，直到划分的文件可直接加载进内存时，就可以直接对数字进行快速排序，找出中位数了。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"外排-分组归并-桶排序","slug":"外排-分组归并-桶排序","permalink":"https://shang.at/tags/外排-分组归并-桶排序/"}]},{"title":"Java学习-Future","slug":"Java学习-Future","date":"2020-05-31T22:44:35.000Z","updated":"2020-05-31T22:45:01.561Z","comments":true,"path":"post/Java学习-Future/","link":"","permalink":"https://shang.at/post/Java学习-Future/","excerpt":"","text":"","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"并发编程-Future","slug":"并发编程-Future","permalink":"https://shang.at/tags/并发编程-Future/"}]},{"title":"Python学习-一些常见的操作","slug":"Python学习-一些常见的操作","date":"2020-05-29T09:15:13.000Z","updated":"2020-05-30T01:49:14.370Z","comments":true,"path":"post/Python学习-一些常见的操作/","link":"","permalink":"https://shang.at/post/Python学习-一些常见的操作/","excerpt":"","text":"如何初始化一个一维数组 n=10l = [0]*nl1 = [0 for _ in range(n)] 如何初始化一个二维数组 m, n = 10, 7l = [[0]*m]*n # 会有赋值问题：n个[0]*m 实际上都是同一个对象l1 = [[0 for _ in range(m)] for _ in range(n)] # 没有赋值问题 如何初始化一个二维数组并且设置右边界和下边界为1(根据实际情况处理) m, n = 10, 7dp = [[1 if i == m - 1 or j == n - 1 else 0 for i in range(m)] for j in range(n)] 正序遍历 n=10for i in range(n): print(i) 倒序遍历 n=10for i in range(n-1, -1, -1): print(i) 二维数组一维化 from itertools import chainb=[[1,2,3], [5,8], [7,8,9]]c=list(chain(*b))print(c)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------ab = [[1,2,3], [5,8], [7,8,9]]print([i for item in ab for i in item])[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------import operatorfrom functools import reducea = [[1,2,3], [4,6], [7,8,9,8]]print(reduce(operator.add, a))[1, 2, 3, 4, 6, 7, 8, 9, 8]### --------------------------------------------------------------------------------a = [[1,2,3], [5, 8], [7,8,9]]l=[]for m in range(0,3): for i in a[m]: l.append(i)print(l)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------a=[[1,2,3], [5,8], [7,8,9]]a= eval('['+str(a).replace(' ','').replace('[','').replace(']','')+']')print(a)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------def flatten(a): if not isinstance(a, (list, )): return [a] else: b = [] for item in a: b += flatten(item)if __name__ == '__main__': a = [[[1,2],3],[4,[5,6]],[7,8,9]] print(flatten(a))[1, 2, 3, 4, 5, 6, 7, 8, 9]### -------------------------------------------------------------------------------- 如何拷贝一个一维数组 x = [1,2,3,4]y = x[:] 如何拷贝一个二维数组 ### --------------------------------------------------------------------------------x=[[1,2,3], [4,6]]y = [row[:] for row in x]### --------------------------------------------------------------------------------x=[[1,2,3], [4,6]]from copy import copy, deepcopyy = deepcopy(x)### --------------------------------------------------------------------------------old_array = [[2, 3], [4, 5]]# python2.*new_array = map(list, old_array)# python3.*new_array = list(map(list, old_array))### --------------------------------------------------------------------------------arr = [[1,2],[3,4]]deepcopy1d2d = lambda lVals: [x if not isinstance(x, list) else x[:] for x in lVals]dst = deepcopy1d2d(arr)dst[1][1]=150print dstprint arr","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"常见操作技巧","slug":"常见操作技巧","permalink":"https://shang.at/tags/常见操作技巧/"}]},{"title":"Python学习-lru_cache","slug":"Python学习-lru-cache","date":"2020-05-28T01:48:49.000Z","updated":"2020-05-28T01:49:40.668Z","comments":true,"path":"post/Python学习-lru-cache/","link":"","permalink":"https://shang.at/post/Python学习-lru-cache/","excerpt":"","text":"","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"lru_cache","slug":"lru-cache","permalink":"https://shang.at/tags/lru-cache/"}]},{"title":"Java学习-Thread","slug":"Java学习-Thread","date":"2020-05-24T11:17:33.000Z","updated":"2020-05-25T10:04:55.397Z","comments":true,"path":"post/Java学习-Thread/","link":"","permalink":"https://shang.at/post/Java学习-Thread/","excerpt":"","text":"线程的基本概念进程：每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1–n个线程。（进程是资源分配的最小单位）线程：同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位） 线程的状态创建、就绪、运行、阻塞、终止 创建：新创建的一个线程对象 就绪：线程对象创建成功后，其他的线程调用该对象的start方法。该状态的线程位于可运行线程池内，变的可运行，等待获取CPU的使用权呢 运行：就绪状态的线程获取了CPU使用权，执行程序代码 阻塞：阻塞状态是线程因为某种原因放弃了CPU使用权，暂停运行。直到线程再次进入就绪状态，才有机会转到运行状态。阻塞的情况分为以下三种： 等待阻塞：运行的线程执行wait方法，JVM会把该线程放入等待池中(wait会释放持有的锁) 同步阻塞：运行的线程在获取对象同步锁的时，如果该同步锁被别的线程占用，则JVM会把改线程放入锁池中 其他阻塞：运行的线程执行sleep或join方法，或者发出了I/O(文件读写、网络请求)请求，JVM会把该线程置为阻塞状态。当sleep状态超时、join等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态(sleep不会释放持有的锁) 终止：线程执行完了或者因异常推出了run方法，该线程结束生命周期","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"并发编程-Thread","slug":"并发编程-Thread","permalink":"https://shang.at/tags/并发编程-Thread/"}]},{"title":"Java学习-并发编程","slug":"Java学习-并发编程","date":"2020-05-24T10:46:42.000Z","updated":"2020-05-24T11:18:03.790Z","comments":true,"path":"post/Java学习-并发编程/","link":"","permalink":"https://shang.at/post/Java学习-并发编程/","excerpt":"","text":"Java中的并发编程是指：如何协调多个线程来共同完成一件任务，其中最主要的就是如何处理共享数据 并发编程是为了充分利用计算机处理器资源，提高系统吞吐量，提高程序执行效率而逐步演化出来的一种编程模式，它主要解决了以下的一些问题： 充分发挥多处理器的强大能力 现在处理器的基本调度单位是线程，多线程程序可以充分发挥多处理器的能力 让异步编程更加简单 对于一个需要IO操作(耗时)的事件，可以让它在一个独立的线程中处理，而不会使主流程卡顿 让程序建模更加简单 可以将复杂的工作流分解为一组简单的工作流，每个工作流放到一个线程中独立执行，只在特定的同步位置进行交互 等 JAVA中的并发编程实际上就是面向线程编程，其中最主要的问题就是处理线程间数据共享的问题。因此诞生了很多同步技术： JAVA中的Thread","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"}]},{"title":"Java学习-JVM","slug":"Java学习-JVM","date":"2020-05-24T10:42:22.000Z","updated":"2020-05-24T10:42:57.262Z","comments":true,"path":"post/Java学习-JVM/","link":"","permalink":"https://shang.at/post/Java学习-JVM/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://shang.at/tags/JVM/"}]},{"title":"Java学习-wait&notify","slug":"Java学习-wait-notify","date":"2020-05-22T05:09:37.000Z","updated":"2020-05-24T10:46:59.543Z","comments":true,"path":"post/Java学习-wait-notify/","link":"","permalink":"https://shang.at/post/Java学习-wait-notify/","excerpt":"","text":"","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"}]},{"title":"Java学习-Timer","slug":"Java学习-Timer","date":"2020-05-22T02:31:16.000Z","updated":"2020-05-22T02:31:16.871Z","comments":true,"path":"post/Java学习-Timer/","link":"","permalink":"https://shang.at/post/Java学习-Timer/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Java学习-NIO","slug":"Java学习-NIO","date":"2020-05-19T18:46:53.000Z","updated":"2020-05-20T02:54:52.451Z","comments":true,"path":"post/Java学习-NIO/","link":"","permalink":"https://shang.at/post/Java学习-NIO/","excerpt":"","text":"Java NIO系列教程（六） 多路复用器Selector ServerSocketChannel：一个面向流的侦听套接字 通道。用于监听是否有新的连接到来，可以调用accept函数获取到来的连接(SocketChannel) SocketChannel：一个面向流的连接套接字 通道。用于从连接中读取数据，和向连接写入数据 Selector：选择器，可以管理一批注册的通道集合的信息和它们的就绪状态(OP_CONNECT|OP_ACCEPT|OP_READ|OP_WRITE) SelectionKey：选择键封装了特定的通道与特定的选择器的注册关系，一个key就代表了一个channel SelectableChannel可以被注册到Selector对象上，然后调用Selector.select()方法可以更新SelectionKeys(即处于某种就绪状态的通道集合)，然后我们就可以遍历这些通过处理相应的事件。比如连接就绪的通道，我们会把他们注册成读状态，等待它读就绪；连接读就绪，我们就可以从中读取数据；连接写就绪，我们就可以向连接写数据","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"NIO","slug":"NIO","permalink":"https://shang.at/tags/NIO/"}]},{"title":"Java学习-线程安全的集合类","slug":"Java学习-线程安全的集合类","date":"2020-05-17T15:16:15.000Z","updated":"2020-05-17T15:16:15.157Z","comments":true,"path":"post/Java学习-线程安全的集合类/","link":"","permalink":"https://shang.at/post/Java学习-线程安全的集合类/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Java学习-Iterator","slug":"Java学习-Iterator","date":"2020-05-17T07:27:13.000Z","updated":"2020-05-17T07:55:27.700Z","comments":true,"path":"post/Java学习-Iterator/","link":"","permalink":"https://shang.at/post/Java学习-Iterator/","excerpt":"","text":"在看Iterator之前，先看一个早期版本的迭代器java.util.Enumeration public interface Enumeration&lt;E&gt; &#123; boolean hasMoreElements(); E nextElement();&#125; * NOTE: The functionality of this interface is duplicated by the Iterator* interface. In addition, Iterator adds an optional remove operation, and* has shorter method names. New implementations should consider using* Iterator in preference to Enumeration. 现在来看Iterator： public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); default void remove() &#123; throw new UnsupportedOperationException(\"remove\"); &#125; default void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (hasNext()) action.accept(next()); &#125;&#125; 注意的点： Iterator在好的设计下可以在遍历的过程中对列表进行增加和删除和修改元素 Iterator在遍历的过程只能进行一遍，即遍历完的对象不能再次遍历， 因为大多数Iterator在实现的过程中都是维护了了cursor指针，这个指针一般只会增加，不会减少 同时大都没有充值cursor指针的接口 关键是看Iterator的设计如何 例如下面的ListIterator public interface ListIterator&lt;E&gt; extends Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); boolean hasPrevious(); E previous(); int nextIndex(); int previousIndex(); void remove(); void set(E e); void add(E e);&#125; ListIterator在原来的Iterator的基础上扩展了，使之可以往前遍历，同时可以修改和增加元素","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"JAVA-Iterator","slug":"JAVA-Iterator","permalink":"https://shang.at/tags/JAVA-Iterator/"}]},{"title":"Java学习-函数式编程","slug":"Java学习-函数式编程","date":"2020-05-17T00:38:33.000Z","updated":"2020-05-17T03:28:07.220Z","comments":true,"path":"post/Java学习-函数式编程/","link":"","permalink":"https://shang.at/post/Java学习-函数式编程/","excerpt":"","text":"Java8之后加入了一种全新的方式来实现方法(功能)作为参数传递的机制：lambda表达式 像python语言，天生就支持将function作为参数传递给函数. 可以想象，既然是一种实现方法作为参数传递的机制，java是一种面向对象的编程语言，也就是说在java中除了原始数据类型之外，都是对象： &gt; @FunctionalInterface&gt; interface CharBinaryOperator &#123;&gt; String applyAsChar(char left, char right);&gt; &#125;&gt; &gt; CharBinaryOperator charBinaryOperator = (char a, char b) -&gt; &#123;&gt; System.out.println(a);&gt; System.out.println(b);&gt; return String.valueOf(a + b);&gt; &#125;;&gt; charBinaryOperator instanceof CharBinaryOperator charBinaryOperator是CharBinaryOperator的一个实例 CharBinaryOperator.class instanceof Class CharBinaryOperator.class是Class的一个实例 Class.class instanceof Class Class.class同时也是Class的一个实例 在java中，传递的参数要么是原始数据类型，要么是对象(类型也是对象，所以能够传递)，不能是其他的类型。在JDK8之前，要想将一个功能传递到函数内部(这一般会被称为函数回调，是大多数异步编程的常用套路：到达某个时间节点或满足某中情况触发一个操作)，那么就只能显示的先定义一个接口，然后创建一个实现了这个接口的类，然后再实例化这个类得到一个对象，最后将这个对象作为参数传入函数，函数内部调用对象实现的方法，如： CharBinaryOperator charBinaryOperator1 = new CharBinaryOperator() &#123; @Override public String applyAsChar(char a, char b) &#123; System.out.println(a); System.out.println(b); return String.valueOf(a + b); &#125;&#125;;charBinaryOperator.applyAsChar('a', 'b'); 在JDK8及之后，我们不需要再显示的做这一系列的事情(当然你这么做也不会有问题)。 在JDK8及之后，所有满足条件的interface都会被解释函数式接口，即都可以通过lambda表达式的形式代替上述流程 什么样的interface才算满足条件呢？ 只声明了一个未实现的函数的interface就可以。在JDK8及以后，interface中定义的函数也可以有默认的实现 在声明接口的时候，可以使用java.lang.FunctionalInterface注解，表示该interface是一个函数式接口(当然可以不加，compiler会自动判断) 如果在有多个未实现的函数的interface上加这个注解的时候，编译阶段就会报错：Multiple non-overriding abstract methods found in interface OOXX lambda表达式只不过是为了实现这个机制的一种解决方案，可以提高开发效率，同时隐藏了interface的定义细节，compilier完全是按照参数列表来推断当前的lambda表达式是和哪一个interface绑定的(compilier直接找到接口的定义，不是推断的)。如果没有预定义的，那么就会在编译期间报错，所以在java中lambda表达式的使用是有一定的限制的。 同时，在使用JDK预定义的操作时，在内部是调用了接口内定义的那个具体的函数的，所以对于开发者来说也是透明的，如列表的forEach()函数 &gt; default void forEach(Consumer&lt;? super T&gt; action) &#123;&gt; Objects.requireNonNull(action);&gt; for (T t : this) &#123;&gt; action.accept(t);&gt; &#125;&gt; &#125;&gt; // 我们在使用的时候是这样的&gt; List&lt;Integer&gt; integers = new ArrayList&lt;&gt;();&gt; integers.add(1);&gt; integers.forEach(i -&gt; &#123;&gt; System.out.println(i);&gt; &#125;);&gt; 本质上，还是要先有interface的定义(在JDK中已经预定义了大部分的interface，所以我们才不用自己手动定义，我上面的例子中就是一个没有被预定义的例子)，运行结果也是创建了一个实现了指定接口的对象，然后将对象作为参数传递给函数 如果我们完全脱离了JDK预定义的操作，那么我们就需要自己定义innterface，并且在我们使用该接口的地方显示的声明方法的使用，但是在外层传递方法参数的调用，我们仍可以使用简洁明了的lambda表达式，无论怎么说，lambda表达式的这种机制极大的方便了开发人员 Lambda 表达式和匿名类之间的区别 this 关键字。对于匿名类 this 关键字解析为匿名类，而对于 Lambda 表达式，this 关键字解析为包含写入 Lambda 的类。 JDK中预定义的interface在java.util.function可以看到全部的预定义的interface，以下四种是最有代表性的 Function R apply(T) Function&lt;V, R&gt; compose(Function&lt;? super V, ? extends T&gt; before) Function&lt;T, V&gt; andThen(Function&lt;? super R, ? extends V&gt; after) Function&lt;T, T&gt; identity() Consumer void accept(T) Consumer&lt; T&gt; addThen(Consumer&lt;? super T&gt;) Predicate boolean test() Predicate&lt; T&gt; add(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; negate() Predicate&lt; T&gt; or(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; isEqual(Object) Supplier T get() Lambda 表达式的例子1 线程初始化线程可以初始化如下： // Old waynew Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"Hello world\"); &#125;&#125;).start();// New waynew Thread( () -&gt; System.out.println(\"Hello world\")).start(); 2 事件处理事件处理可以用 Java 8 使用 Lambda 表达式来完成。以下代码显示了将 ActionListener 添加到 UI 组件的新旧方式： // Old waybutton.addActionListener(new ActionListener() &#123; @Override public void actionPerformed(ActionEvent e) &#123; System.out.println(\"Hello world\"); &#125;&#125;);// New waybutton.addActionListener( (e) -&gt; &#123; System.out.println(\"Hello world\");&#125;); 3 遍例输出（方法引用）输出给定数组的所有元素的简单代码。请注意，还有一种使用 Lambda 表达式的方式。 // old wayList&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);for (Integer n : list) &#123; System.out.println(n);&#125;// 使用 -&gt; 的 Lambda 表达式list.forEach(n -&gt; System.out.println(n));// 使用 :: 的 Lambda 表达式list.forEach(System.out::println); 6.4 逻辑操作输出通过逻辑判断的数据。 package com.wuxianjiezh.demo.lambda;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class Main &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); System.out.print(\"输出所有数字：\"); evaluate(list, (n) -&gt; true); System.out.print(\"不输出：\"); evaluate(list, (n) -&gt; false); System.out.print(\"输出偶数：\"); evaluate(list, (n) -&gt; n % 2 == 0); System.out.print(\"输出奇数：\"); evaluate(list, (n) -&gt; n % 2 == 1); System.out.print(\"输出大于 5 的数字：\"); evaluate(list, (n) -&gt; n &gt; 5); &#125; public static void evaluate(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) &#123; for (Integer n : list) &#123; if (predicate.test(n)) &#123; System.out.print(n + \" \"); &#125; &#125; System.out.println(); &#125;&#125; 运行结果： 输出所有数字：1 2 3 4 5 6 7 不输出：输出偶数：2 4 6 输出奇数：1 3 5 7 输出大于 5 的数字：6 7 6.4 Stream API 示例java.util.stream.Stream接口 和 Lambda 表达式一样，都是 Java 8 新引入的。所有 Stream 的操作必须以 Lambda 表达式为参数。Stream 接口中带有大量有用的方法，比如 map() 的作用就是将 input Stream 的每个元素，映射成output Stream 的另外一个元素。 下面的例子，我们将 Lambda 表达式 x -&gt; x*x 传递给 map() 方法，将其应用于流的所有元素。之后，我们使用 forEach 打印列表的所有元素。 // old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);for(Integer n : list) &#123; int x = n * n; System.out.println(x);&#125;// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);list.stream().map((x) -&gt; x*x).forEach(System.out::println); 下面的示例中，我们给定一个列表，然后求列表中每个元素的平方和。这个例子中，我们使用了 reduce() 方法，这个方法的主要作用是把 Stream 元素组合起来。 // old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = 0;for(Integer n : list) &#123; int x = n * n; sum = sum + x;&#125;System.out.println(sum);// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = list.stream().map(x -&gt; x*x).reduce((x,y) -&gt; x + y).get();System.out.println(sum);","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"}]},{"title":"Java学习-集合类","slug":"Java学习-集合类","date":"2020-05-16T23:27:00.000Z","updated":"2020-05-17T09:06:31.786Z","comments":true,"path":"post/Java学习-集合类/","link":"","permalink":"https://shang.at/post/Java学习-集合类/","excerpt":"","text":"[TOC] 总览 注：这里只列举了单一线线程使用的集合对象(Vector除外) JVAV中列表类集合，按照数据的存储方式可以分为两大类：基于数组和基于链表。两种方式各有好处，需要根据实际业务场景做出选择。 数组 优点 支持随机访问，给定下标的访问是O(1)的时间复杂度 缺点 内存必须是连续的，否则会申请空间失败 查找、插入、扩容、删除都是O(n)的时间复杂度 有容量的限制，增加节点时，可能会因为数组大小不够导致扩容，扩容的时间复杂度是O(n)的 使用注意 最好能够预估数据的最大容量，可以预先设计capacity，尽量避免扩容操作 但是在特别的使用场景下，基于数组的实现效率会更好，比如下面要说的ArrayDeque 具体实现 ArrayList Vector Stack ArrayDeque 链表 优点 内存不用是连续的 插入、删除都是O(1)的时间复杂度 没有容量的限制，按理说限制就是JVAV堆的大小限制 缺点 查找是O(n)的时间复杂度 使用 单独使用链表的时候，还挺少的，毕竟一个没有附加特性的链表结构，仅仅只能够做到新增和删除的时间复杂度为O(1)，但是查询却需要O(n)，并且还需要额外的空间存储链表结构。数组可以通过预估容量的方式尽量减少扩容的操作，对比发现，使用基于数组的集合性价比更高 具体实现 LinkList LinkList在定位低index个元素的时候，有个优化的点可以学习 Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 总结：可以发现，基于数组和基于链表的集合实现方式，想要从他们中查询到具体的元素，时间复杂度都是O(n)的，这是因为，这里仅仅考虑了数据的存储方式，并没有额外的信息给出来，所以是没有办法加速查询的。要想实现加速查询，那么就必须在这基础上增加新的特性： 数组 有序性：可以借助有序性使用二分查找，把查找时间复杂度降到O(log n) 链表 建立树结构： 二叉搜索树：前序遍历就是正向排序，可以把查询的时间复杂度降到O(log n)，但是要维护二叉搜索，尽量保证他是平衡的(但是这个的时间复杂度是O(1)的) 堆：查找最大(最小)值是O(1)的时间复杂度 升维：比如跳表，就是在有序的链表上建立多级索引来实现加速查询的，可以把查找时间复杂度降到O(log n)，但是在新增和删除节点时需要维护多级索引(但是这个的时间复杂度是O(1)的) List 接口信息如下： Method Return Comment Insert add(E) boolean 向队列加入元素，如果空间不足，会触发扩容 Insert add(int, E) void 向指定位置插入元素，可能会抛IndexOutOfBoundsException Remove remove(Object) boolean 移除指定的元素，没有的话返回false，有的话返回true Remove remove(int) E 移除指定index的元素，可能会抛IndexOutOfBoundsException Examine get(int) E 返回指定index的元素，可能会抛IndexOutOfBoundsException Update set(int, E) E 更新指定index的元素，可能会抛IndexOutOfBoundsException 在List的源码中发现多处这样的代码： // 只返回第一个遇到的o，当o为null的时候，o.equals会报错public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 详看扩容操作 public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;// DEFAULT_CAPACITY=10 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // newCapacity = int(1.5*oldCapacity) int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; // 整形溢出：Integer.MAX_VALUE + 8&lt;0 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Queue 设计了一套支持队列操作的接口，如下： Method Return Comment Insert add(E) boolean 向队列添加一个元素，如果没有空间会抛出IllegalStateException Insert offer(E) boolean 向队列添加一个元素，如果没有空间会返回false Remove remove() E 移除并返回头结点，如果队列为空的话，会抛NoSuchElementException Remove poll() E 移除并返回头结点，如果队列为空的话，会返回null Examine element() E 返回头结点，如果队列为空的话，会抛NoSuchElementException Examine peek() E 返回头结点，如果队列为空的话，会返回null Deque Stack 总结：通常使用ArrayDeque来作为先进先出的Queue，后进先出的Stack","categories":[{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"}],"tags":[{"name":"JAVA集合类-列表","slug":"JAVA集合类-列表","permalink":"https://shang.at/tags/JAVA集合类-列表/"}]},{"title":"分布式服务框架-IPC&RPC","slug":"分布式服务框架-IPC-RPC","date":"2020-05-14T03:38:18.000Z","updated":"2020-05-15T01:43:48.472Z","comments":true,"path":"post/分布式服务框架-IPC-RPC/","link":"","permalink":"https://shang.at/post/分布式服务框架-IPC-RPC/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"}],"tags":[{"name":"IPC&RPC","slug":"IPC-RPC","permalink":"https://shang.at/tags/IPC-RPC/"}]},{"title":"Java学习-代理","slug":"Java学习-代理","date":"2020-05-14T03:34:00.000Z","updated":"2020-05-15T01:44:12.636Z","comments":true,"path":"post/Java学习-代理/","link":"","permalink":"https://shang.at/post/Java学习-代理/","excerpt":"","text":"","categories":[{"name":"JAVA","slug":"JAVA","permalink":"https://shang.at/categories/JAVA/"}],"tags":[{"name":"代理","slug":"代理","permalink":"https://shang.at/tags/代理/"}]},{"title":"Mysql学习-事务和隔离级别","slug":"Mysql学习-事务和隔离级别","date":"2020-05-12T17:31:26.000Z","updated":"2020-05-13T03:30:51.247Z","comments":true,"path":"post/Mysql学习-事务和隔离级别/","link":"","permalink":"https://shang.at/post/Mysql学习-事务和隔离级别/","excerpt":"","text":"MYSQL事务和隔离级别一、事务事务是由一组SQL语句组成的逻辑处理单元，是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。事务具有以下4个属性，通常简称为事务的ACID属性: 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 二、并发一致性问题1、更新丢失(Lost Update)T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。 如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读一句话：事务B读取到了事务A已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果A事务回滚Rollback，B读取的数据无效，不符合一致性要求。 解决办法: 把数据库的事务隔离级别调整到 READ_COMMITTED T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 3、不可重复读(Non-Repeatable Reads) 在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 一句话：一个事务范围内两个相同的查询却返回了不同数据。 同时操作，事务1分别读取事务2操作时和提交后的数据，读取的记录内容不一致。不可重复读是指在同一个事务内，两个相同的查询返回了不同的结果。 解决办法: 如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。把数据库的事务隔离级别调整到REPEATABLE_READ T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4、幻读一个事务T1按相同的查询条件重新读取以前检索过的数据，却发现其他事务T2插入了满足其查询条件的新数据，这种现象就称为“幻读”。（和可重复读类似，但是事务 T2 的数据操作仅仅是插入和删除，不是修改数据，读取的记录数量前后不一致） 一句话：事务A 读取到了事务B提交的新增数据，不符合隔离性。 解决办法: 如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。把数据库的事务隔离级别调整到 SERIALIZABLE_READ。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 三、事务隔离级别“脏读”、”不可重复读”和”幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。 MYSQL常看当前数据库的事务隔离级别：show variables like &#39;tx_isolation&#39;; 1、读未提交 (Read Uncommitted)最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。 2、读已提交 (Read Committed)被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。 3、可重复读(Repeatable Read)所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。(引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。) MYSQL默认是REPEATABLE-READ。 4、串行化(Serializable)所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。 5、总结 读未提交: 一个事务还没提交时，它做的变更就能被别的事务看到。 读提交: 一个事务提交之后，它做的变更才会被其他事务看到。 可重复读 : 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化: 顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 四个级别逐渐增强，每个级别解决一个问题，事务级别越高，性能越差，大多数环境(Read committed 就可以用了) 隔离级别 读数据一致性 脏读 不可重复读 幻读 未提交读 最低级别 √ √ √ 提交读 语句级 × √ √ 可重复读 事务级 × × √ 可串行化 最高级别,事务级 × × × 参考","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://shang.at/categories/Mysql/"}],"tags":[{"name":"事务和隔离级别","slug":"事务和隔离级别","permalink":"https://shang.at/tags/事务和隔离级别/"}]},{"title":"Mysql学习-第三范式","slug":"Mysql学习-第三范式","date":"2020-05-12T17:19:15.000Z","updated":"2020-05-13T02:56:08.500Z","comments":true,"path":"post/Mysql学习-第三范式/","link":"","permalink":"https://shang.at/post/Mysql学习-第三范式/","excerpt":"","text":"第一范式(1NF) 数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性 符合1NF的关系中的每个属性都不可再分 有两点要求： schema定义：每个属性不可再分，即字段的含义要明确，同一个字段不应该有多于1个的含义 图中的这种schema在RDBMS中是不可能存在的，也就是无法创建的。可以改成如下的schema: 存储的数据：同一列中不能有多个值 图中的一个字段里面存了多个值，这种情况在RDBMS中是可以存在的，但是该字段是可再分的，应该。可以将数据分成多条存储，如下图 第二范式(2NF) 满足第一范式 没有部分依赖 在同一个表中，不能存在某些字段依赖一些键，而另一些字段依赖另外一些键 员工表的一个候选键是{id，mobile，deptNo}，而deptName依赖于deptNo，同样 name 依赖于 id，因此不是 2NF的。为了满足第二范式的条件，需要将这个表拆分成employee、dept、employee_dept、employee_mobile四个表 不满足2NF的表，可能存在的问题：修改异常、新增异常、删除异常 第三范式(3NF) 满足第二范式 没有传递依赖 在同一个表中，不要存在字段A依赖字段B，同时字段B依赖字段C，推导出来字段A间接依赖字段C的关系。 员工表的province、city、district依赖于zip，而zip依赖于id，换句话说，province、city、district传递依赖于id，违反了 3NF 规则。为了满足第三范式的条件，可以将这个表拆分成employee和zip两个表 但是这种关系也不是一定不能存在，视具体的业务而定吧 示例假设有一个名为employee的员工表，它有九个属性：id(员工编号)、name(员工名称)、mobile(电话)、zip(邮编)、province(省份)、city(城市)、district(区县)、deptNo(所属部门编号)、deptName(所属部门名称)、表总数据如下： idnamemobilezipprovincecitydistrictdeptNodeptName101张三 1391000000113910000002100001北京北京海淀区D1部门1101张三1391000000113910000002100001北京北京海淀区D2部门2102李四13910000003200001上海上海静安区D3部门3103王五13910000004510001广东省广州白云区D4部门4103王五13910000004510001广东省广州白云区D5部门 5 将上表改成满足第1范式，如下： idnamemobilezipprovincecitydistrictdeptNodeptName101张三13910000001100001北京北京海淀区D1部门1101张三13910000002100001北京北京海淀区D1部门1101张三13910000001100001北京北京海淀区D2部门2101张三13910000002100001北京北京海淀区D2部门2102李四13910000003200001上海上海静安区D3部门3103王五13910000004510001广东省广州白云区D4部门4103王五13910000004510001广东省广州白云区D5部门5 仍存在的问题 修改异常：上表中张三、王五都有多条记录，因为他隶属于两个部门。如果我们要修改王五的地址，必修修改两行记录。假如一个部门得到了王五的新地址并进行了更新，而另一个部门没有，那么此时王五在表中会存在两个不同的地址，导致了数据不一致 新增异常：假如一个新员工假如公司，他正处于入职培训阶段，还没有被正式分配到某个部门，如果deptNo字段不允许为空，我们就无法向employee表中新增该员工的数据。 删除异常：假设公司撤销了D3部门，那么在删除deptNo为D3的行时，会将李四的信息也一并删除。因为他隶属于D3这一部门。 为了解决上面的问题，我们可以将上述表设计成满足3NF 在关系数据库模型设计中，一般需要满足第三范式的要求。如果一个表具有良好的主外键设计，就应该是满足3NF的表。规范化带来的好处是通过减少数据冗余提高更新数据的效率，同时保证数据完整性。然而，我们在实际应用中也要防止过度规范化的问题。规范化程度越高，划分的表就越多，在查询数据时越有可能使用表连接操作。而如果连接的表过多，会影响查询性能。关键的问题是要依据业务需求，仔细权衡数据查询和数据更新关系，指定最合适的规范化程度。不要为了遵循严格的规范化规则而修改业务需求。 参考","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://shang.at/categories/Mysql/"}],"tags":[{"name":"第三范式","slug":"第三范式","permalink":"https://shang.at/tags/第三范式/"}]},{"title":"Python学习-函数参数传递","slug":"Python学习-函数参数传递","date":"2020-04-19T04:00:49.000Z","updated":"2020-04-19T04:26:43.749Z","comments":true,"path":"post/Python学习-函数参数传递/","link":"","permalink":"https://shang.at/post/Python学习-函数参数传递/","excerpt":"","text":"在Python(估计也适用于其他的语言)中，函数参数的传递分为两类 值传递和引用传递，实际上这两类传递类型都是属于变量传值，即： 值传递：将实际参数值复制一份传递到函数内，这样在函数内对参数进行修改，就不会影响到原参数 引用传递：将实际参数的地址直接传递到函数内，那么在函数内对参数所进行的修改，将可能会影响到原参数 要注意的是，在函数内修改参数，实际上又分为两种情况(仅说引用传递)： 1、对参数(a)重新进行赋值操作(a=new_obj)，此时，实际上修改的已经不是传递给函数的最初的参数(a)了，它已经指向了其他的内存地址，这时再修改a，实际上就和之前的对象没有任何关系了 2、直接对a进行修改，比如说a.name=’sdd’，这时，原始的对象就会发生变化","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python学习","slug":"python学习","permalink":"https://shang.at/tags/python学习/"}]},{"title":"Python学习-OrderedDict","slug":"Python学习-OrderedDict","date":"2020-04-16T08:50:57.000Z","updated":"2020-04-16T08:51:22.122Z","comments":true,"path":"post/Python学习-OrderedDict/","link":"","permalink":"https://shang.at/post/Python学习-OrderedDict/","excerpt":"","text":"from collections import OrderedDict # 记录插入顺序的dict，操作方式和dict一样。# 是基于dict和双端队列实现，可以用来实现LRUcache","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python源码学习","slug":"python源码学习","permalink":"https://shang.at/tags/python源码学习/"}]},{"title":"Python学习-bisect","slug":"Python学习-bisect","date":"2020-04-16T08:47:50.000Z","updated":"2020-04-16T08:49:46.149Z","comments":true,"path":"post/Python学习-bisect/","link":"","permalink":"https://shang.at/post/Python学习-bisect/","excerpt":"","text":"这个模块对有序列表提供了支持，使得他们可以在插入新数据仍然保持有序。对于长列表，如果其包含元素的比较操作十分昂贵的话，这可以是对更常见方法的改进 \"\"\"Bisection algorithms.\"\"\"def insort_right(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the right of the rightmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 a.insert(lo, x)insort = insort_right # backward compatibilitydef bisect_right(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt;= x, and all e in a[i:] have e &gt; x. So if x already appears in the list, a.insert(x) will insert just after the rightmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 return lobisect = bisect_right # backward compatibilitydef insort_left(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the left of the leftmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid a.insert(lo, x)def bisect_left(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt; x, and all e in a[i:] have e &gt;= x. So if x already appears in the list, a.insert(x) will insert just before the leftmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid return lo# Overwrite above definitions with a fast C implementationtry: from _bisect import *except ImportError: pass","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python源码学习","slug":"python源码学习","permalink":"https://shang.at/tags/python源码学习/"}]},{"title":"数据结构与算法学习笔记-查找算法","slug":"数据结构与算法学习笔记-查找算法","date":"2020-04-10T00:58:45.000Z","updated":"2020-04-10T01:00:39.177Z","comments":true,"path":"post/数据结构与算法学习笔记-查找算法/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-查找算法/","excerpt":"","text":"总结 查找算法 时间复杂度 二分查找 O($$logn$$) O(logn)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"查找算法","slug":"查找算法","permalink":"https://shang.at/tags/查找算法/"}]},{"title":"Spark应用之import spark.implicits._","slug":"Spark应用之import-spark-implicits","date":"2020-03-25T09:10:54.000Z","updated":"2020-03-27T06:38:15.397Z","comments":true,"path":"post/Spark应用之import-spark-implicits/","link":"","permalink":"https://shang.at/post/Spark应用之import-spark-implicits/","excerpt":"","text":"在初期使用spark的时候，大家都会遇见一个很奇怪的写法import spark.implicits._ 这里面包含了四个关键字：import、spark、implicits、_ import和_实际上是Scala中包引入的写法，表示引入指定包内的所有成员 本文主要想记录一下另外两个关键字：spark、implicits 关键字一：spark关键字二：implicits","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/tags/Spark应用/"}]},{"title":"hadoop源码学习一","slug":"hadoop源码学习一","date":"2019-07-10T03:03:43.000Z","updated":"2020-05-14T03:39:39.124Z","comments":true,"path":"post/hadoop源码学习一/","link":"","permalink":"https://shang.at/post/hadoop源码学习一/","excerpt":"","text":"先导知识 JAVA-代理 IPC/RPC","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://shang.at/categories/Hadoop/"}],"tags":[{"name":"Hadoop-IPC","slug":"Hadoop-IPC","permalink":"https://shang.at/tags/Hadoop-IPC/"}]},{"title":"Pandas-学习","slug":"Pandas-学习","date":"2019-06-11T01:53:35.000Z","updated":"2019-08-03T02:17:49.584Z","comments":true,"path":"post/Pandas-学习/","link":"","permalink":"https://shang.at/post/Pandas-学习/","excerpt":"","text":"","categories":[{"name":"Pandas","slug":"Pandas","permalink":"https://shang.at/categories/Pandas/"}],"tags":[]},{"title":"Python学习-时间处理","slug":"Python学习-时间处理","date":"2019-06-06T08:37:46.000Z","updated":"2019-08-03T02:17:49.584Z","comments":true,"path":"post/Python学习-时间处理/","link":"","permalink":"https://shang.at/post/Python学习-时间处理/","excerpt":"","text":"关于时间戳的几个概念时间戳，根据1970年1月1日00:00:00开始按秒计算的偏移量。时间元组（struct_time），包含9个元素。 time.struct_time(tm_year=2017, tm_mon=10, tm_mday=1, tm_hour=14, tm_min=21, tm_sec=57, tm_wday=6, tm_yday=274, tm_isdst=0) 时间格式字符串，字符串形式的时间。time模块与时间戳和时间相关的重要函数 time.time() # 生成当前的时间戳，格式为10位整数的浮点数。time.strftime() # 根据时间元组生成时间格式化字符串。time.strptime() # 根据时间格式化字符串生成时间元组。time.strptime()与time.strftime()为互操作。time.localtime() # 根据时间戳生成当前时区的时间元组。time.mktime() # 根据时间元组生成时间戳。 示例 import time##生成当前时间的时间戳，只有一个参数即时间戳的位数，默认为10位，输入位数即生成相应位数的时间戳，比如可以生成常用的13位时间戳def now_to_timestamp(digits = 10): time_stamp = time.time() digits = 10 ** (digits -10) time_stamp = int(round(time_stamp*digits)) return time_stamp##将时间戳规范为10位时间戳def timestamp_to_timestamp10(time_stamp): time_stamp = int (time_stamp* (10 ** (10-len(str(time_stamp))))) return time_stamp##将当前时间转换为时间字符串，默认为2017-10-01 13:37:04格式def now_to_date(format_string=\"%Y-%m-%d %H:%M:%S\"): time_stamp = int(time.time()) time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将10位时间戳转换为时间字符串，默认为2017-10-01 13:37:04格式def timestamp_to_date(time_stamp, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将时间字符串转换为10位时间戳，时间字符串默认为2017-10-01 13:37:04格式def date_to_timestamp(date, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.strptime(date, format_string) time_stamp = int(time.mktime(time_array)) return time_stamp##不同时间格式字符串的转换def date_style_transfomation(date, format_string1=\"%Y-%m-%d %H:%M:%S\",format_string2=\"%Y-%m-%d %H-%M-%S\"): time_array = time.strptime(date, format_string1) str_date = time.strftime(format_string2, time_array) return str_dateprint(now_to_date())print(timestamp_to_date(1506816572))print(date_to_timestamp('2017-10-01 08:09:32'))print(timestamp_to_timestamp10(1506816572546))print(date_style_transfomation('2017-10-01 08:09:32')) 结果为 15068362240002017-10-01 13:37:042017-10-01 08:09:3215068165721506816572","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python中的时间处理","slug":"python中的时间处理","permalink":"https://shang.at/tags/python中的时间处理/"}]},{"title":"Spark学习笔记-Configuration","slug":"Spark学习笔记-Configuration","date":"2019-06-03T09:32:33.000Z","updated":"2019-08-03T02:17:49.585Z","comments":true,"path":"post/Spark学习笔记-Configuration/","link":"","permalink":"https://shang.at/post/Spark学习笔记-Configuration/","excerpt":"","text":"submit 参数 运行时可配置参数：在代码中使用spark.conf.set(‘’， ‘’)的方式设置。运行时设置的参数不会在WebUI中显示","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Configuration","slug":"Configuration","permalink":"https://shang.at/tags/Configuration/"}]},{"title":"Python学习-队列","slug":"Python学习-队列","date":"2019-06-03T02:10:48.000Z","updated":"2020-05-11T02:35:59.597Z","comments":true,"path":"post/Python学习-队列/","link":"","permalink":"https://shang.at/post/Python学习-队列/","excerpt":"","text":"队列from queue import Queue #LILO队列q = Queue() #创建队列对象q.put(0) #在队列尾部插入元素q.put(1)q.put(2)print('LILO队列',q.queue) #查看队列中的所有元素print(q.get()) #返回并删除队列头部元素print(q.queue)from queue import LifoQueue #LIFO队列lifoQueue = LifoQueue()lifoQueue.put(1)lifoQueue.put(2)lifoQueue.put(3)print('LIFO队列',lifoQueue.queue)lifoQueue.get() #返回并删除队列尾部元素lifoQueue.get()print(lifoQueue.queue)from queue import PriorityQueue #优先队列priorityQueue = PriorityQueue() #创建优先队列对象priorityQueue.put(3) #插入元素priorityQueue.put(78) #插入元素priorityQueue.put(100) #插入元素print(priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.put(1) #插入元素priorityQueue.put(2) #插入元素print('优先级队列:',priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue)priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('全部被删除后:',priorityQueue.queue) #查看优先级队列中的所有元素from collections import deque #双端队列dequeQueue = deque(['Eric','John','Smith'])print(dequeQueue)dequeQueue.append('Tom') #在右侧插入新元素dequeQueue.appendleft('Terry') #在左侧插入新元素print(dequeQueue)dequeQueue.rotate(2) #循环右移2次print('循环右移2次后的队列',dequeQueue)dequeQueue.popleft() #返回并删除队列最左端元素print('删除最左端元素后的队列：',dequeQueue)dequeQueue.pop() #返回并删除队列最右端元素print('删除最右端元素后的队列：',dequeQueue)","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"}]},{"title":"Spark学习笔记-广播变量","slug":"Spark学习笔记-广播变量","date":"2019-05-28T08:19:03.000Z","updated":"2019-08-03T02:17:49.589Z","comments":true,"path":"post/Spark学习笔记-广播变量/","link":"","permalink":"https://shang.at/post/Spark学习笔记-广播变量/","excerpt":"","text":"Shared Variables通常，当在远程集群节点上执行传递给Spark操作（例如mapor reduce）的函数时，它将在函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上的变量的更新不会传播回驱动程序。支持跨任务的通用，读写共享变量效率低下。但是，Spark确实为两种常见的使用模式提供了两种有限类型的共享变量：广播变量和累加器。 Broadcast广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。 Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 广播变量是v通过调用从变量创建的SparkContext.broadcast(v)。广播变量是一个包装器v，可以通过调用该value 方法来访问它的值。下面的代码显示了这个： &gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 创建广播变量后，应该使用它来代替v群集上运行的任何函数中的值，这样v就不会多次传送到节点。此外，在v广播之后不应修改对象 ，以确保所有节点获得相同的广播变量值（例如，如果稍后将变量发送到新节点）。 Performance Tuning == Physical Plan ==InMemoryTableScan [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075] +- InMemoryRelation [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075], true, 10000, StorageLevel(disk, 1 replicas) +- *(34) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, ((coalesce(nanvl(total_disburse_amount#5203, null), 0.0) - cast(coalesce(total_repay_principal_amount#5974, 0) as double)) - coalesce(nanvl(total_write_off_principal#5986, null), 0.0)) AS total_outstanding_amount_ex_dp90#6075] +- SortMergeJoin [bill_create_date#4955], [write_off_date#4776], LeftOuter :- *(23) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203, total_repay_principal_amount#5974] : +- SortMergeJoin [bill_create_date#4955], [repay_date#5789], LeftOuter : :- *(6) Sort [bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(bill_create_date#4955, 200) : : +- *(5) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203] : : +- Window [sum(disburse_amount#5197) windowspecdefinition(1, bill_create_date#4955 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_disburse_amount#5203], [1], [bill_create_date#4955 ASC NULLS FIRST] : : +- *(4) Sort [1 ASC NULLS FIRST, bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(1, 200) : : +- *(3) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[sum(cast(principal#615 as double))]) : : +- Exchange hashpartitioning(bill_create_date#4955, week_last_day#5015, month_last_day#5075, 200) : : +- *(2) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[partial_sum(cast(principal#615 as double))]) : : +- *(2) Project [principal#615, cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) AS bill_create_date#4955, next_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), Sun) AS week_last_day#5015, last_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date)) AS month_last_day#5075] : : +- *(2) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : :- *(2) Project [id#392] : : : +- *(2) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : +- *(2) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : : +- *(1) Project [loan_id#609, principal#615, create_time#632L] : : +- *(1) Filter ((cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) &lt;= 18043) &amp;&amp; isnotnull(loan_id#609)) : : +- *(1) FileScan parquet [loan_id#609,principal#615,create_time#632L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id)], ReadSchema: struct&lt;loan_id:string,principal:string,create_time:bigint&gt; : +- *(22) Sort [repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(21) Project [repay_date#5789, total_repay_principal_amount#5974] : +- Window [sum(repay_principal_amount#5970) windowspecdefinition(1, repay_date#5789 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_repay_principal_amount#5974], [1], [repay_date#5789 ASC NULLS FIRST] : +- *(20) Sort [1 ASC NULLS FIRST, repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(1, 200) : +- *(19) HashAggregate(keys=[repay_date#5789], functions=[sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(18) HashAggregate(keys=[repay_date#5789], functions=[partial_sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- *(18) Project [repaid_principal#684, write_off_date#4776, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repay_date#5789] : +- SortMergeJoin [loan_id#609], [loan_id#5672], LeftOuter : :- *(12) Sort [loan_id#609 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(loan_id#609, 200) : : +- *(11) Project [loan_id#609, repaid_principal#684, repay_time#692] : : +- *(11) BroadcastHashJoin [id#608], [bill_id#670], Inner, BuildRight : : :- *(11) Project [id#608, loan_id#609] : : : +- *(11) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : : :- *(11) Project [id#392] : : : : +- *(11) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : : +- *(11) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true])) : : : +- *(7) Project [id#608, loan_id#609] : : : +- *(7) Filter (isnotnull(loan_id#609) &amp;&amp; isnotnull(id#608)) : : : +- *(7) FileScan parquet [id#608,loan_id#609] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id), IsNotNull(id)], ReadSchema: struct&lt;id:string,loan_id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(10) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(10) Filter ((isnotnull(rn#5382) &amp;&amp; (rn#5382 = 1)) &amp;&amp; (cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) &lt;= 18043)) : : +- Window [row_number() windowspecdefinition(bill_id#670, repay_time#692 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#5382], [bill_id#670], [repay_time#692 DESC NULLS LAST] : : +- *(9) Sort [bill_id#670 ASC NULLS FIRST, repay_time#692 DESC NULLS LAST], false, 0 : : +- Exchange hashpartitioning(bill_id#670, 200) : : +- *(8) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(8) Filter isnotnull(bill_id#670) : : +- *(8) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(bill_id)], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt; : +- *(17) Sort [loan_id#5672 ASC NULLS FIRST], false, 0 : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[first(write_off_date#4705, true)]) : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[partial_first(write_off_date#4705, true)]) : +- *(17) Project [loan_id#5672, write_off_date#4705] : +- *(17) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight : :- *(17) Project [loan_id#5672, write_off_date#4705, bill_id#4714] : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4718], LeftOuter, BuildRight : : :- *(17) Project [loan_id#5672, write_off_date#4705] : : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4708], LeftOuter, BuildRight : : : :- *(17) HashAggregate(keys=[loan_id#5672], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- Exchange hashpartitioning(loan_id#5672, 200) : : : : +- *(13) HashAggregate(keys=[loan_id#5672], functions=[partial_min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- *(13) Project [loan_id#5672, (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) AS is_write_off_bill#4594, date_add(cast(due_date#5686 as date), 91) AS write_off_date#4630] : : : : +- *(13) Filter (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) : : : : +- *(13) FileScan parquet [loan_id#5672,status#5676,due_date#5686,repay_time#5694L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;loan_id:string,status:string,due_date:string,repay_time:bigint&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : : +- *(14) Project [id#392 AS loan_id#4708] : : : +- *(14) FileScan parquet [id#392] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) : : +- *(15) Project [id#4717 AS bill_id#4714, loan_id#4718] : : +- *(15) FileScan parquet [id#4717,loan_id#4718] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,loan_id:string&gt; : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : +- *(16) FileScan parquet [bill_id#670] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string&gt; +- *(33) Sort [write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(32) Project [write_off_date#4776, total_write_off_principal#5986] +- Window [sum(write_off_principal#5982) windowspecdefinition(1, write_off_date#4776 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_write_off_principal#5986], [1], [write_off_date#4776 ASC NULLS FIRST] +- *(31) Sort [1 ASC NULLS FIRST, write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(1, 200) +- *(30) HashAggregate(keys=[write_off_date#4776], functions=[sum(write_off_principal#4779)]) +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(29) HashAggregate(keys=[write_off_date#4776], functions=[partial_sum(write_off_principal#4779)]) +- SortAggregate(key=[loan_id#609], functions=[first(write_off_date#4705, true), first(amount#398, true), sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- SortAggregate(key=[loan_id#609], functions=[partial_first(write_off_date#4705, true), partial_first(amount#398, true), partial_sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- *(28) Sort [loan_id#609 ASC NULLS FIRST], false, 0 +- *(28) Project [loan_id#609, write_off_date#4705, amount#398, repaid_principal#684, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repayment_date#4760] +- *(28) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight :- *(28) Project [loan_id#609, write_off_date#4705, amount#398, bill_id#4714] : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4718], LeftOuter, BuildRight : :- *(28) Project [loan_id#609, write_off_date#4705, amount#398] : : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4708], LeftOuter, BuildRight : : :- *(28) HashAggregate(keys=[loan_id#609], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : +- ReusedExchange [loan_id#609, min#6101], Exchange hashpartitioning(loan_id#5672, 200) : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(25) Project [id#392 AS loan_id#4708, amount#398] : : +- *(25) FileScan parquet [id#392,amount#398] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,amount:string&gt; : +- ReusedExchange [bill_id#4714, loan_id#4718], BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) +- *(27) Project [bill_id#670, repay_time#692, repaid_principal#684] +- *(27) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt;","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"}]},{"title":"数据结构学习笔记二-算法","slug":"数据结构学习笔记二-算法","date":"2019-05-16T09:54:29.000Z","updated":"2019-05-16T14:39:58.762Z","comments":true,"path":"post/数据结构学习笔记二-算法/","link":"","permalink":"https://shang.at/post/数据结构学习笔记二-算法/","excerpt":"","text":"递归二分查找哈希算法堆排序深度和广度优先搜索字符串匹配贪心算法##分治算法 回溯算法动态规划拓扑排序最短路径并行算法","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://shang.at/tags/算法/"}]},{"title":"Spark学习笔记-pivot透视图","slug":"Spark学习笔记-pivot透视图","date":"2019-05-09T02:52:07.000Z","updated":"2019-08-03T02:17:49.587Z","comments":true,"path":"post/Spark学习笔记-pivot透视图/","link":"","permalink":"https://shang.at/post/Spark学习笔记-pivot透视图/","excerpt":"","text":"df = spark.createDataFrame([ ('2018-01','项目1',100, 'xm'), ('2018-01','项目1',100, 'xl'), ('2018-01','项目1',100, 'xp'), ('2018-01','项目2',200, 'ch'), ('2018-01','项目3',300, 'xl'), ('2018-02','项目1',1000, 'xp'), ('2018-02','项目2',2000, 'xl'), ('2018-03','项目x',999, 'xm')], ['date','project','income', 'saler']) df.toPandas() date project income saler 0 2018-01 项目1 100 xm 1 2018-01 项目1 100 xl 2 2018-01 项目1 100 xp 3 2018-01 项目2 200 ch 4 2018-01 项目3 300 xl 5 2018-02 项目1 1000 xp 6 2018-02 项目2 2000 xl 7 2018-03 项目x 999 xm pivotdf_pivot = df.groupBy('date').pivot( 'project', ['项目1', '项目2', '项目3', '项目x']).agg( sum('income')).na.fill(0)df_pivot.toPandas() date 项目1 项目2 项目3 项目x 0 2018-03 0 0 0 999 1 2018-02 1000 2000 0 0 2 2018-01 300 200 300 0 df.groupBy('project').pivot( 'date').agg( sum('income')).na.fill(0).toPandas() project 2018-01 2018-02 2018-03 0 项目2 200 2000 0 1 项目x 0 0 999 2 项目1 300 1000 0 3 项目3 300 0 0 unpivotdf_pivot.selectExpr(\"date\", \"stack(4, '项目11', `项目1`, '项目22', `项目2`, '项目33', `项目3`, '项目xx', `项目x`) as (`project`,`income`)\")\\ .filter(\"income &gt; 0 \")\\ .orderBy([\"date\", \"project\"])\\ .toPandas() date project income 0 2018-01 项目11 300 1 2018-01 项目22 200 2 2018-01 项目33 300 3 2018-02 项目11 1000 4 2018-02 项目22 2000 5 2018-03 项目xx 999 stack(n, expr1, ..., exprk) 将k个[expr1, ..., exprk]拆解成n rows","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"}]},{"title":"Spark学习笔记-tips","slug":"Spark学习笔记-tips","date":"2019-04-15T05:50:00.000Z","updated":"2019-08-03T02:17:49.589Z","comments":true,"path":"post/Spark学习笔记-tips/","link":"","permalink":"https://shang.at/post/Spark学习笔记-tips/","excerpt":"","text":"写spark dataframe的时候，最好用哪些字段就取哪些字段，否则spark会默认把所有字段都读进内存，如果进行cache操作，就会无故占用大量内存 没有被明确select的字段依然可以作为filter的条件 获取周的第一天日期和当前日期位于周的第几天，周的第一天定义不同 周日 周一 Spark Shuffle spill (Memory) and (Disk) on SPARK UI? What do they mean? https://community.hortonworks.com/questions/202809/spark-shuffle-spill-memory.html 窗口函数会引起重分区吗？分区数(200)是固定的吗？ test_df = kreditpintar.spark.range(0, end=100, numPartitions=5).toDF(&apos;input&apos;)test_df.rdd.getNumPartitions() # 5test_1_df = test_df.withColumn(&apos;id&apos;, row_number().over(Window.partitionBy(lit(1)).orderBy(&apos;input&apos;)))test_1_df.rdd.getNumPartitions() # 200test_2_df = test_df.withColumn(&apos;id&apos;, monotonically_increasing_id())test_2_df.rdd.getNumPartitions() # 5 通过withColumn(‘group’, lit(‘aaaabbb’))添加的新列，不能最为后续的join操作的condition expression？ groupBy 和 窗口函数的实现原理 哪一个效率更高 groupby 、窗口函数、distinct三种方式去重 哪个效率高 distinct&gt;groupby&gt;窗口函数 循环的去跑脚本，然后union每次循环的结果。 这样的使用 task可能会失败，需要优化 转化long列类型到时间戳，保留毫秒信息 a_df = spark.createDataFrame([[1556613225852]], ['a'])a_df.select((col('a')/1000.0).cast('timestamp')).toPandas()#CAST((a / 1000.0) AS TIMESTAMP)#0 2019-04-30 08:33:45.852 spark进行计算的过程中间检查数据没有问题，但是执行collect后出现数据不一致的情况(丢失数据和union后的数据重复)","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"}]},{"title":"BI工具使用之Tableau一","slug":"BI工具使用之Tableau一","date":"2019-04-11T07:08:39.000Z","updated":"2019-05-12T00:22:00.389Z","comments":true,"path":"post/BI工具使用之Tableau一/","link":"","permalink":"https://shang.at/post/BI工具使用之Tableau一/","excerpt":"","text":"","categories":[{"name":"BI","slug":"BI","permalink":"https://shang.at/categories/BI/"}],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://shang.at/tags/Tableau/"}]},{"title":"Spark学习笔记-DSL语法","slug":"Spark学习笔记-DSL语法","date":"2019-03-31T02:20:06.000Z","updated":"2019-03-31T02:21:41.762Z","comments":true,"path":"post/Spark学习笔记-DSL语法/","link":"","permalink":"https://shang.at/post/Spark学习笔记-DSL语法/","excerpt":"","text":"","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"sparkSql-DSL语法","slug":"sparkSql-DSL语法","permalink":"https://shang.at/tags/sparkSql-DSL语法/"}]},{"title":"数据结构与算法学习笔记-排序算法","slug":"数据结构与算法学习笔记-排序算法","date":"2019-03-29T00:49:58.000Z","updated":"2020-05-23T01:52:56.336Z","comments":true,"path":"post/数据结构与算法学习笔记-排序算法/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-排序算法/","excerpt":"","text":"[TOC] O(n^2)冒泡排序(Bubble Sort) 算法描述 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否符合大小关系要求。如果不满足就互换位置。一次冒泡至少会让一个元素移动到它应该在的位置，重复n次，就完成了n个元素的排序工作。 算法实现 def bubble_sort(nums): \"\"\" 冒泡排序：从小到大 :param nums: :return: \"\"\" if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(i + 1, len(nums)): if nums[i] &gt; nums[j]: nums[i], nums[j] = nums[j], nums[i] return nums def bubble_sort1(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(len(nums) - 1 - i): if nums[j] &gt; nums[j + i]: nums[j], nums[j + 1] = nums[j + 1], nums[j] return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34] print(bubble_sort(nums)) 插入排序 算法描述 将一个元素插入一个已经有序的序列，使其依然有序。首先，将原始的序列分为两个子序列，有序的和无序的，然后，从无序的序列中依次拿出一个元素，插入到有序的序列的合适位置，并保持有序的序列依然有序，直到无序的序列中没有元素了。 算法实现 def insert_sort(nums): if len(nums) &lt;= 1: return nums for i in range(1, len(nums)): tmp = nums[i] j = i - 1 for j in range(i - 1, -1, -1): if tmp &lt; nums[j]: nums[j + 1] = nums[j] else: break nums[j + 1] = tmp return nums if __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(insert_sort(nums)) 选择排序 算法描述 选择排序是选择无序序列中的最小的元素放到有序序列的末尾，直到无序序列没有元素。 算法实现 def selection_sort(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): min_val = nums[i] min_j = i for j in range(i + 1, len(nums)): if min_val &gt; nums[j]: min_val = nums[j] min_j = j nums[i], nums[min_j] = nums[min_j], nums[i] return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(selection_sort(nums)) 希尔排序 算法描述 希尔排序是对插入排序的优化。希尔排序，通过将原始序列按照一定的步长划分为多个子序列 将原始的一维数组映射成二维数组， 然后按列进行插入排序，这样的话，可以让一个元素在一次比较中跨越较大的区间，随后算法在使用较小的步长，一直到步长为1(已知当对有序度较高数组进行排序时，插入排序的时间复杂度接近O(N)，因此可以大幅度提高插入排序的效率)。 常见的步长选择有 算法实现 def shell_sort(list): n = len(list) # 初始步长 gap = n // 2 while gap &gt; 0: print(gap) for i in range(gap, n): # 每个步长进行插入排序 temp = list[i] j = i # 插入排序 while j &gt;= gap and list[j - gap] &gt; temp: list[j] = list[j - gap] j -= gap print('inner=', list) list[j] = temp print(list) # 得到新的步长 gap = gap // 2 return listdef shell_sort1(collection): # Marcin Ciura's gap sequence gaps = [701, 301, 132, 57, 23, 10, 4, 1] for gap in gaps: i = gap while i &lt; len(collection): temp = collection[i] j = i while j &gt;= gap and collection[j - gap] &gt; temp: collection[j] = collection[j - gap] j -= gap collection[j] = temp i += 1 return collectionif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', shell_sort1(nums)) O(nlogn)归并排序 算法描述 将数组分为两部分，分别排序，最后将两部分排好序的数组合并成一个有序的数组。利用递归的方式，重复上述过程。 算法实现 def merge_sort(nums): print('before=', nums) length = len(nums) if length &gt; 1: midpoint = length // 2 left_half = merge_sort(nums[:midpoint]) right_half = merge_sort(nums[midpoint:]) i = 0 j = 0 k = 0 left_length = len(left_half) right_length = len(right_half) while i &lt; left_length and j &lt; right_length: if left_half[i] &lt; right_half[j]: nums[k] = left_half[i] i += 1 else: nums[k] = right_half[j] j += 1 k += 1 while i &lt; left_length: nums[k] = left_half[i] i += 1 k += 1 while j &lt; right_length: nums[k] = right_half[j] j += 1 k += 1 print('after=', nums) return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', merge_sort(nums)) 快速排序 算法描述 随机选择一个pivot节点，然后将数组中的数据分成大于pivot和小于pivot的两部分，然后递归地将大于pivot和小于pivot的部分再按照相同的思路处理，直到每个pivot两端的部分都只有最多一个元素 算法实现 def quick_sort(collection): length = len(collection) if length &lt;= 1: return collection else: pivot = collection[0] greater = [element for element in collection[1:] if element &gt; pivot] lesser = [element for element in collection[1:] if element &lt;= pivot] return quick_sort(lesser) + [pivot] + quick_sort(greater) O(n) 时间复杂度内求无序数组中的第 K 大元素 # 选择数组的最后一个元素，作为pivot，然后将数组的所有元素分为大于pivot和小于pivot的两部分，# 如果 len(lesser) == k - 1，则返回pivot# 如果 len(lesser) &gt;= k，则说明要查找的元素在小于pivot的部分，那么继续在lesser中查找# 否则的话，说明要查找的元素在大于pivot的部分，那么继续在greater中查找def find_k_max(nums, k): length = len(nums) if length &lt; k: return None pivot = nums[length - 1] greater = [element for element in nums[:length - 1] if element &gt; pivot] lesser = [element for element in nums[:length - 1] if element &lt;= pivot] if len(lesser) == k - 1: return pivot elif len(lesser) &gt;= k: return find_k_max(lesser, k) else: return find_k_max(greater, k - len(lesser) - 1) 堆排序 算法描述 算法实现 O(n)计数排序 算法描述 算法实现 基数排序 算法描述 算法实现 桶排序 算法描述 算法实现 总结","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://shang.at/tags/排序算法/"}]},{"title":"数据分析-reduce函数引发的","slug":"数据分析-reduce函数引发的","date":"2019-03-28T05:35:25.000Z","updated":"2019-04-10T15:36:43.244Z","comments":true,"path":"post/数据分析-reduce函数引发的/","link":"","permalink":"https://shang.at/post/数据分析-reduce函数引发的/","excerpt":"","text":"reduce in python# _functools.reducedef reduce(function, sequence, initial=None): \"\"\" reduce(function, sequence[, initial]) -&gt; value Apply a function of two arguments cumulatively to the items of a sequence, from left to right, so as to reduce the sequence to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). If initial is present, it is placed before the items of the sequence in the calculation, and serves as a default when the sequence is empty. :param function:给定的一个func，func具有两个参数，参数1是临时聚合值，参数2是序列中下一个待聚合的值 :param sequence:待处理的可迭代的序列 :param initial:聚合数据的初始值 \"\"\" pass 工作原理：reduce函数对给定的序列遍历调用func函数，每次调用返回一个临时聚合值，直到整个序列遍历结束。如果设置了初始值，那么在第一次执行func函数的时候，会将func的参数1设置为初始值。 例子： &gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])15&gt;&gt;&gt; reduce(lambda x, y: x+y, [1, 2, 3, 4, 5], 100)115&gt;&gt;&gt; reduce(lambda x, y: str(x)+str(y), [1, 2, 3, 4, 5], '')'12345' reduce函数不仅可以完成这种聚合的功能，还可以完成更加复杂的操作， reduce&amp;foldLeft&amp;foldRight&amp;reduce in scala### hive的UDAFspark的UDAF","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"reduce","slug":"reduce","permalink":"https://shang.at/tags/reduce/"},{"name":"数据分析技巧","slug":"数据分析技巧","permalink":"https://shang.at/tags/数据分析技巧/"},{"name":"有初始值的聚合操作","slug":"有初始值的聚合操作","permalink":"https://shang.at/tags/有初始值的聚合操作/"}]},{"title":"信贷数据统计的相关指标","slug":"信贷数据统计的相关指标","date":"2019-03-22T11:19:47.000Z","updated":"2019-03-28T14:24:24.845Z","comments":true,"path":"post/信贷数据统计的相关指标/","link":"","permalink":"https://shang.at/post/信贷数据统计的相关指标/","excerpt":"","text":"贷款类型 等额本息贷款 根据固定的还款时间，计算出应还的总利息，再加上本金，然后每个月平均等额的还款。 等额本金贷款 等额本金相对来说要简单一些，每月所还的本金是相同的，利息由每个月的剩余本金计算得出。 固定点数贷款 按照定义，我们在首次还款时先按固定的点数还一部分贷款，然后再按较低的利率还完剩余的贷款。 双利率贷款 前x个月以较低的r1利率还款，后m-x个月以较高的r2利率还款（假设还款总月数为m） 相关指标 同比增长 环比增长","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"分析指标","slug":"分析指标","permalink":"https://shang.at/tags/分析指标/"}]},{"title":"airflow安装","slug":"airflow安装","date":"2019-03-19T11:24:02.000Z","updated":"2019-03-24T01:33:47.230Z","comments":true,"path":"post/airflow安装/","link":"","permalink":"https://shang.at/post/airflow安装/","excerpt":"","text":"# airflow needs a home, ~/airflow is the default,# but you can lay foundation somewhere else if you prefer# (optional)export AIRFLOW_HOME=~/airflow# install from pypi using pippip install apache-airflow# initialize the databaseairflow initdb# start the web server, default port is 8080airflow webserver -p 8080# start the schedulerairflow scheduler# visit localhost:8080 in the browser and enable the example dag in the home page USE Mysqlvim $AIRFLOW_HOME/airflow.cfg sql_alchemy_conn = mysql+pymysql://root:123456@localhost:3306/airflow 需要pip install pymysql 启动失败ERROR [airflow.models.DagBag] Failed to import: /anaconda3/lib/python3.7/site-packages/airflow/example_dags/example_http_operator.pyTraceback (most recent call last): File &quot;/anaconda3/lib/python3.7/site-packages/airflow/models.py&quot;, line 374, in process_file m = imp.load_source(mod_name, filepath) File &quot;/anaconda3/lib/python3.7/imp.py&quot;, line 171, in load_source module = _load(spec) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 696, in _load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;/anaconda3/lib/python3.7/site-packages/airflow/example_dags/example_http_operator.py&quot;, line 27, in &lt;module&gt; from airflow.operators.http_operator import SimpleHttpOperator File &quot;/anaconda3/lib/python3.7/site-packages/airflow/operators/http_operator.py&quot;, line 21, in &lt;module&gt; from airflow.hooks.http_hook import HttpHook File &quot;/anaconda3/lib/python3.7/site-packages/airflow/hooks/http_hook.py&quot;, line 23, in &lt;module&gt; import tenacity File &quot;/anaconda3/lib/python3.7/site-packages/tenacity/__init__.py&quot;, line 352 from tenacity.async import AsyncRetrying ^SyntaxError: invalid syntax 修复方式：修改from tenacity.async import AsyncRetrying为from tenacity.async_a import AsyncRetrying，同时tenacity包下的async文件名为async_a","categories":[],"tags":[]},{"title":"Spark学习笔记-抽样方法和自增ID","slug":"Spark学习笔记-抽样方法和自增ID","date":"2019-03-19T08:33:24.000Z","updated":"2019-03-24T01:33:47.229Z","comments":true,"path":"post/Spark学习笔记-抽样方法和自增ID/","link":"","permalink":"https://shang.at/post/Spark学习笔记-抽样方法和自增ID/","excerpt":"","text":"抽样方法sample(withReplacement=None, fraction=None, seed=None) Returns a sampled subset of this DataFrame. withReplacement – Sample with replacement or not (default False). true时会将抽样的数据放回数据集，导致抽样数据有重复的 false时不会放回 fraction – Fraction of rows to generate, range [0.0, 1.0]. 表示子集占数据集的占比 seed – Seed for sampling (default a random seed). fraction并不能保证完全按照占比抽样数据 自增IDmonotonically_increasing_id() 每个分区分别排序生成一个64位的整数，但不是连续的。会将分区值放到高31位，然后将每条记录的序列放到低33位。限制：分区数不能大于10亿，每个分区的数据量不能大于80亿。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"}]},{"title":"Spark学习笔记-SparkSQL内置函数","slug":"Spark学习笔记-SparkSQL内置函数","date":"2019-03-19T01:27:01.000Z","updated":"2019-04-10T15:36:43.243Z","comments":true,"path":"post/Spark学习笔记-SparkSQL内置函数/","link":"","permalink":"https://shang.at/post/Spark学习笔记-SparkSQL内置函数/","excerpt":"","text":"学习SparkSQL中的一些内置函数 日期函数 获取默认时区 spark.conf.get('spark.sql.session.timeZone')&gt;&gt; 'Asia/Shanghai' 获取当前时间 获取当前日期：current_date() spark.sql(\"\"\" select current_date()\"\"\").toPandas()&gt;&gt; 2019-03-19 获取当前时间：current_timestamp()/now() spark.sql(\"\"\" select current_timestamp()\"\"\").toPandas()&gt;&gt; 2019-03-19 13:54:22.236 从日期中截取字段 截取年月日、时分秒:year,month,day/dayofmonth,hour,minute,second dayofweek ,dayofyear 1 = Sunday, 2 = Monday, ..., 7 = Saturday weekofyear Extract the week number of a given date as integer. trunc截取某部分的日期，其他部分默认为01 Returns date truncated to the unit specified by the format.Parameters: format – ‘year’, ‘yyyy’, ‘yy’ or ‘month’, ‘mon’, ‘mm’ date_trunc [“YEAR”, “YYYY”, “YY”, “MON”, “MONTH”, “MM”, “DAY”, “DD”, “HOUR”, “MINUTE”, “SECOND”, “WEEK”, “QUARTER”] Returns timestamp truncated to the unit specified by the format.Parameters: format – ‘year’, ‘yyyy’, ‘yy’, ‘month’, ‘mon’, ‘mm’, ‘day’, ‘dd’, ‘hour’, ‘minute’, ‘second’, ‘week’, ‘quarter’ date_format将时间转化为某种格式的字符串 Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.A pattern could be for instance dd.MM.yyyy and could return a string like ‘18.03.1993’. All pattern letters of the Java class java.text.SimpleDateFormat can be used. 日期时间转换 unix_timestamp返回当前时间的unix时间戳 Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.if timestamp is None, then it returns current timestamp. from_unixtime将时间戳换算成当前时间，to_unix_timestamp将时间转化为时间戳 Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. to_date/date将字符串转化为日期格式，to_timestamp（Since: 2.2.0） Converts a Column of pyspark.sql.types.StringType or pyspark.sql.types.TimestampType into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to SimpleDateFormats. By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted (equivalent to col.cast(&quot;date&quot;)).Converts a Column of pyspark.sql.types.StringType or pyspark.sql.types.TimestampType into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to SimpleDateFormats. By default, it follows casting rules to pyspark.sql.types.TimestampType if the format is omitted (equivalent to col.cast(&quot;timestamp&quot;)). quarter 将1年4等分(range 1 to 4) Extract the quarter of a given date as integer. 日期、时间计算 months_between两个日期之间的月数 add_months返回日期后n个月后的日期 last_day(date),next_day(start_date, day_of_week) date_add,date_sub(减) datediff（两个日期间的天数） utc 在集群中对于时间戳的转换，如果不指定时区，默认会采用集群配置的时区，集群默认时区可以通过如下方式获取：spark.conf.get(‘spark.sql.session.timeZone’)。一般而言，这个值应该是集群统一设置，独立提交job的时候，不需要设置。 to_utc_timestamp(timestamp, tz) 将timestamp按照给定的tz解释，返回utc timestamp from_utc_timestamp(timestamp, tz) 将timestamp按照utc解释，返回给定tz的timestamp 对于有时区相关的数据统计时，需要注意。比如：集群默认时区设置为UTC，一般将数据存到集群中的时候会将时间戳转为utc timestamp以便后续的操作。此时如果有一个需求是统计北京时间的当天的数据，那么第一个想到的方式是使用current_date()获取当前日期，然后将数据中的时间戳使用to_date(from_utc_timestamp(from_unixtime(ts), ‘Asia/Beijing’))，然后进行比较。但是current_date()获取的日期，是根据集群默认时区得来的，因此会有时区的不同导致的数据统计错误，因此，这种情况不能直接使用current_date()，正确的使用方式是：to_date(from_utc_timestamp(current_timestamp(), ‘Asia/Beijing’))，然后在进行比较。 表关联 Join(other, on=None, how=None) on：a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join. how：str, default inner. Must be one of: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti inner:内连，返回joinDF1和joinDF2合并的rows，如果joinDF2中有多条记录对应于joinDF1的同一条记录，那么返回的row number会大于joinDF1的row number outer,full,full_outer：全连 left, left_outer：左连 right，right_outer:右连 left_semi：过滤出joinDF1中和joinDF2共有的部分，只返回joinDF1中的rows left_anti：过滤出joinDF1中joinDF2没有的部分，只返回joinDF1中的rows crossJoin(other) 返回两个DF的笛卡尔积 Parses the expression expr 将字符串表示的表达式，翻译成DSL expr(\"length(name)\")expr(\"array_contains(user_id_set, user_id)\")","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"sparkSql内置函数","slug":"sparkSql内置函数","permalink":"https://shang.at/tags/sparkSql内置函数/"}]},{"title":"数据分析小知识点","slug":"数据分析小知识点","date":"2019-03-19T01:25:56.000Z","updated":"2019-08-03T02:17:49.591Z","comments":true,"path":"post/数据分析小知识点/","link":"","permalink":"https://shang.at/post/数据分析小知识点/","excerpt":"","text":"总结一下在数据分析中需要注意的一些tips，持续更新 Tip1 时区在进行跨境业务处理的时候，时区的控制是十分必要的。平时对于国内的业务，部署在国内的服务器，使用的时区一般都是北京时间(北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间)，在数据库中一般存储相对于unix epoch (1970-01-01 00:00:00 UTC)的毫秒时间戳，做某个地区的数据统计时，需要将时间戳转换成当地的时间(即加一个时区的属性) https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431937554888869fb52b812243dda6103214cd61d0c2000 Tip 2 Excel函数 去重计数：SUMPRODUCT(1/COUNTIF(A2:A20,A2:A20)) VLOOKUP(要查找的值,查找返回,返回查找到的第几列,是否精确查找[1]) 概念落地页，也称：着陆页、引导页，是指访问者在其他地方看到发出的某个具有明确主题的特定营销活动——通过Email、社交媒体或广告发布的诱人优惠信息等，点击后被链接到你网站上的第一个页面 PRD：产品需求文档，产品需求文档是将商业需求文档（BRD）和市场需求文档（MRD）用更加专业的语言进行描述","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"数据分析Tips","slug":"数据分析Tips","permalink":"https://shang.at/tags/数据分析Tips/"}]},{"title":"Spark学习笔记-窗口函数","slug":"Spark学习笔记-窗口函数","date":"2019-03-12T02:15:03.000Z","updated":"2019-03-24T01:33:47.229Z","comments":true,"path":"post/Spark学习笔记-窗口函数/","link":"","permalink":"https://shang.at/post/Spark学习笔记-窗口函数/","excerpt":"","text":"from pyspark.sql import SparkSessionfrom pyspark.sql.functions import *from pyspark.sql import Windowfrom pyspark.sql.types import StructType, StringType, StructField, IntegerTypeschema = StructType([ StructField('shop_id', StringType()), StructField('date', StringType()), StructField('amount', IntegerType())]) spark = SparkSession \\ .builder \\ .master('local[*]') \\ .enableHiveSupport() \\ .getOrCreate() data = [ &#123;'shop_id': '10006', 'date': '201501120030', 'amount': 2313&#125;, &#123;'shop_id': '10006', 'date': '201501120100', 'amount': 23112&#125;, &#123;'shop_id': '10006', 'date': '201501120130', 'amount': 23112&#125;, &#123;'shop_id': '10006', 'date': '201501120200', 'amount': 24234&#125;, &#123;'shop_id': '10006', 'date': '201501120230', 'amount': 132&#125;, &#123;'shop_id': '10006', 'date': '201501120300', 'amount': 31232&#125;, &#123;'shop_id': '10006', 'date': '201501120330', 'amount': 221313&#125;, &#123;'shop_id': '10006', 'date': '201501120400', 'amount': 2134&#125;, &#123;'shop_id': '10006', 'date': '201501120430', 'amount': 2231&#125;, &#123;'shop_id': '10006', 'date': '201501120500', 'amount': 2234&#125;, &#123;'shop_id': '10006', 'date': '201501120530', 'amount': 2234&#125;, &#123;'shop_id': '10006', 'date': '201501120600', 'amount': 231635&#125;, &#123;'shop_id': '10006', 'date': '201501120630', 'amount': 2536&#125;, &#123;'shop_id': '10006', 'date': '201501120700', 'amount': 425432&#125;, &#123;'shop_id': '10006', 'date': '201501120730', 'amount': 36362&#125;, &#123;'shop_id': '10006', 'date': '201501120800', 'amount': 5645622&#125;, &#123;'shop_id': '10006', 'date': '201501120830', 'amount': 34532&#125;, &#123;'shop_id': '10006', 'date': '201501120900', 'amount': 366642&#125;, &#123;'shop_id': '10006', 'date': '201501120930', 'amount': 74632&#125;, &#123;'shop_id': '10006', 'date': '201501121000', 'amount': 63562&#125;, &#123;'shop_id': '10006', 'date': '201501121030', 'amount': 26353&#125;, &#123;'shop_id': '10006', 'date': '201501121100', 'amount': 2353&#125;, &#123;'shop_id': '10006', 'date': '201501121130', 'amount': 26352&#125;, &#123;'shop_id': '10006', 'date': '201501121200', 'amount': 254352&#125;, &#123;'shop_id': '10006', 'date': '201501121230', 'amount': 534236&#125;, &#123;'shop_id': '10006', 'date': '201501121300', 'amount': 35432&#125;, &#123;'shop_id': '10006', 'date': '201501121330', 'amount': 353462&#125;, &#123;'shop_id': '10006', 'date': '201501121400', 'amount': 64562&#125;, &#123;'shop_id': '10006', 'date': '201501121430', 'amount': 652562&#125;, &#123;'shop_id': '10006', 'date': '201501121500', 'amount': 2456&#125;, &#123;'shop_id': '10006', 'date': '201501121530', 'amount': 6422&#125;, &#123;'shop_id': '10006', 'date': '201501121600', 'amount': 422&#125;, &#123;'shop_id': '10006', 'date': '201501121630', 'amount': 27843&#125;, &#123;'shop_id': '10006', 'date': '201501121700', 'amount': 2362&#125;, &#123;'shop_id': '10006', 'date': '201501121730', 'amount': 24683&#125;, &#123;'shop_id': '10006', 'date': '201501121800', 'amount': 4532&#125;, &#123;'shop_id': '10006', 'date': '201501121830', 'amount': 5342&#125;, &#123;'shop_id': '10006', 'date': '201501121900', 'amount': 65642&#125;, &#123;'shop_id': '10006', 'date': '201501121930', 'amount': 2534&#125;, &#123;'shop_id': '10006', 'date': '201501122000', 'amount': 25376&#125;, &#123;'shop_id': '10006', 'date': '201501122030', 'amount': 242443&#125;, &#123;'shop_id': '10006', 'date': '201501122100', 'amount': 2344562&#125;, &#123;'shop_id': '10006', 'date': '201501122130', 'amount': 5462&#125;, &#123;'shop_id': '10006', 'date': '201501122200', 'amount': 2535&#125;, &#123;'shop_id': '10006', 'date': '201501122230', 'amount': 242546&#125;, &#123;'shop_id': '10006', 'date': '201501122300', 'amount': 6542&#125;, &#123;'shop_id': '10006', 'date': '201501122330', 'amount': 2546&#125;, &#123;'shop_id': '10006', 'date': '201501130000', 'amount': 45245&#125;, ] df = spark.createDataFrame(data, schema) df.printSchema() root |-- shop_id: string (nullable = true) |-- date: string (nullable = true) |-- amount: integer (nullable = true) 关于子窗口： 子窗口需要指定一个边界，有以下两种方式： ROWS between CURRENT ROW | UNBOUNDED PRECEDING | [num] PRECEDING AND UNBOUNDED FOLLOWING | [num] FOLLOWING| CURRENT ROW RANGE between [num] PRECEDING AND [num] FOLLOWING 窗口的含义 ROWS是物理窗口，从行数上控制窗口的尺寸的；RANGE是逻辑窗口，从列值上控制窗口的尺寸 通常会结合order by子句使用，如果在order by子句后面没有指定窗口子句，则默认为：rows between unbounded preceding and current row spark中关于Window函数的学习在spark中涉及Window函数的主要有以下两个类和一个Column的方法pyspark.sql.column.Column#over 在窗口上应用某一种分析函数pyspark.sql.window.Window 创建WindowSpec的工具类 pyspark.sql.window.Window.unboundedPreceding pyspark.sql.window.Window.unboundedFollowing pyspark.sql.window.Window.currentRow pyspark.sql.window.Window#partitionBy pyspark.sql.window.Window#orderBy pyspark.sql.window.Window#rowsBetween(start, end) pyspark.sql.window.Window#rangeBetween(start, end)pyspark.sql.window.WindowSpec 窗口的规范pyspark.sql.window.Window#rowsBetween(start, end)定义窗口的边界，[start, end]，在边界处是闭区间start和end都是相对于当前row的相对位置，例如：- 0：当前row- -1：当前行的前1row- 5：当前行的后5row- (-1, 5)：窗口的范围为，当前row+当前行的前1row+当前行的后5row = 7rows df.show(50) +-------+------------+-------+|shop_id| date| amount|+-------+------------+-------+| 10006|201501120030| 2313|| 10006|201501120100| 23112|| 10006|201501120130| 2342|| 10006|201501120200| 24234|| 10006|201501120230| 132|| 10006|201501120300| 31232|| 10006|201501120330| 221313|| 10006|201501120400| 2134|| 10006|201501120430| 2231|| 10006|201501120500| 2234|| 10006|201501120530| 2234|| 10006|201501120600| 231635|| 10006|201501120630| 2536|| 10006|201501120700| 425432|| 10006|201501120730| 36362|| 10006|201501120800|5645622|| 10006|201501120830| 34532|| 10006|201501120900| 366642|| 10006|201501120930| 74632|| 10006|201501121000| 63562|| 10006|201501121030| 26353|| 10006|201501121100| 2353|| 10006|201501121130| 26352|| 10006|201501121200| 254352|| 10006|201501121230| 534236|| 10006|201501121300| 35432|| 10006|201501121330| 353462|| 10006|201501121400| 64562|| 10006|201501121430| 652562|| 10006|201501121500| 2456|| 10006|201501121530| 6422|| 10006|201501121600| 422|| 10006|201501121630| 27843|| 10006|201501121700| 2362|| 10006|201501121730| 24683|| 10006|201501121800| 4532|| 10006|201501121830| 5342|| 10006|201501121900| 65642|| 10006|201501121930| 2534|| 10006|201501122000| 25376|| 10006|201501122030| 242443|| 10006|201501122100|2344562|| 10006|201501122130| 5462|| 10006|201501122200| 2535|| 10006|201501122230| 242546|| 10006|201501122300| 6542|| 10006|201501122330| 2546|| 10006|201501130000| 45245|+-------+------------+-------+ 1.统计截止到当前时间段的店铺累计销售金额 df.withColumn( 't_amount', sum('amount').over(Window.partitionBy('shop_id').orderBy(asc('date')))).select( 'shop_id', 'date', 't_amount').show(50, truncate=False) +-------+------------+--------+|shop_id|date |t_amount|+-------+------------+--------+|10006 |201501120030|2313 ||10006 |201501120100|25425 ||10006 |201501120130|27767 ||10006 |201501120200|52001 ||10006 |201501120230|52133 ||10006 |201501120300|83365 ||10006 |201501120330|304678 ||10006 |201501120400|306812 ||10006 |201501120430|309043 ||10006 |201501120500|311277 ||10006 |201501120530|313511 ||10006 |201501120600|545146 ||10006 |201501120630|547682 ||10006 |201501120700|973114 ||10006 |201501120730|1009476 ||10006 |201501120800|6655098 ||10006 |201501120830|6689630 ||10006 |201501120900|7056272 ||10006 |201501120930|7130904 ||10006 |201501121000|7194466 ||10006 |201501121030|7220819 ||10006 |201501121100|7223172 ||10006 |201501121130|7249524 ||10006 |201501121200|7503876 ||10006 |201501121230|8038112 ||10006 |201501121300|8073544 ||10006 |201501121330|8427006 ||10006 |201501121400|8491568 ||10006 |201501121430|9144130 ||10006 |201501121500|9146586 ||10006 |201501121530|9153008 ||10006 |201501121600|9153430 ||10006 |201501121630|9181273 ||10006 |201501121700|9183635 ||10006 |201501121730|9208318 ||10006 |201501121800|9212850 ||10006 |201501121830|9218192 ||10006 |201501121900|9283834 ||10006 |201501121930|9286368 ||10006 |201501122000|9311744 ||10006 |201501122030|9554187 ||10006 |201501122100|11898749||10006 |201501122130|11904211||10006 |201501122200|11906746||10006 |201501122230|12149292||10006 |201501122300|12155834||10006 |201501122330|12158380||10006 |201501130000|12203625|+-------+------------+--------+ 分析：根据shop_id分组，根据date正序排列，由于orderBy后面没有追加rowsBetween()，则默认的rowsBetween为：[Window.unboundedPreceding，Window.currentRow]。即会统计根据date排序后，从第一行计算到当前行，从而达到了统计截止到当前时间段的店铺累计销售金额的效果 2.统计每个时间段的销售占比 df.withColumn( 't_amount', col('amount')/sum('amount').over(Window.partitionBy('shop_id'))).select( 'shop_id', 'date', 'amount','t_amount').show(50, truncate=False) +-------+------------+-------+---------------------+|shop_id|date |amount |t_amount |+-------+------------+-------+---------------------+|10006 |201501120030|2313 |1.8953384752481333E-4||10006 |201501120100|23112 |0.0018938635036720647||10006 |201501120130|2342 |1.919101906195905E-4 ||10006 |201501120200|24234 |0.0019858033985803397||10006 |201501120230|132 |1.0816458224502965E-5||10006 |201501120300|31232 |0.002559239570209671 ||10006 |201501120330|221313 |0.01813502135635928 ||10006 |201501120400|2134 |1.748660746294646E-4 ||10006 |201501120430|2231 |1.8281453256716753E-4||10006 |201501120500|2234 |1.8306036116317898E-4||10006 |201501120530|2234 |1.8306036116317898E-4||10006 |201501120600|231635 |0.018980835612369275 ||10006 |201501120630|2536 |2.0780710649499637E-4||10006 |201501120700|425432 |0.03486111708611171 ||10006 |201501120730|36362 |0.0029796064693892186||10006 |201501120800|5645622|0.46261844329041574 ||10006 |201501120830|34532 |0.0028296510258222453||10006 |201501120900|366642 |0.030043696032941034 ||10006 |201501120930|74632 |0.006115559925841707 ||10006 |201501121000|63562 |0.005208452406559526 ||10006 |201501121030|26353 |0.0021594403302297475||10006 |201501121100|2353 |1.9281156213829908E-4||10006 |201501121130|26352 |0.00215935838736441 ||10006 |201501121200|254352 |0.02084233168423317 ||10006 |201501121230|534236 |0.04377682860625429 ||10006 |201501121300|35432 |0.0029033996046256747||10006 |201501121330|353462 |0.028963689067797477 ||10006 |201501121400|64562 |0.00529039527189667 ||10006 |201501121430|652562 |0.05347280009013715 ||10006 |201501121500|2456 |2.0125167726802487E-4||10006 |201501121530|6422 |5.262370811951367E-4 ||10006 |201501121600|422 |3.457988917227463E-5 ||10006 |201501121630|27843 |0.002281535199582091 ||10006 |201501121700|2362 |1.9354904792633337E-4||10006 |201501121730|24683 |0.0020225957451167173||10006 |201501121800|4532 |3.7136506570793515E-4||10006 |201501121830|5342 |4.377387866310215E-4 ||10006 |201501121900|65642 |0.0053788935664607854||10006 |201501121930|2534 |2.0764322076432208E-4||10006 |201501122000|25376 |0.002079382150795358 ||10006 |201501122030|242443 |0.019866474100933125 ||10006 |201501122100|2344562|0.19212012824058425 ||10006 |201501122130|5462 |4.4757193047147874E-4||10006 |201501122200|2535 |2.0772516362965921E-4||10006 |201501122230|242546 |0.01987491421606285 ||10006 |201501122300|6542 |5.360702250355939E-4 ||10006 |201501122330|2546 |2.086265351483678E-4 ||10006 |201501130000|45245 |0.0037075049421790656|+-------+------------+-------+---------------------+ 分析：根据shop_id分组，不排序，窗口大小默认就是整个分组。 3.找出2点的销售金额及前半小时的销售金额和后1个小时的销售金额 df.withColumn( 'pre_half_hour', lag('date', 1).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'pre_half_hour_amount', lag('amount', 1).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'follow_one_hour', lead('date', 2).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'follow_one_hour_amount', lead('amount', 2).over(Window.partitionBy('shop_id').orderBy(asc('date')))).filter( col('date') == '201501120200').select( 'shop_id', 'date', 'amount','pre_half_hour', 'pre_half_hour_amount', 'follow_one_hour', 'follow_one_hour_amount').show(truncate=False) +-------+------------+------+-------------+--------------------+---------------+----------------------+|shop_id|date |amount|pre_half_hour|pre_half_hour_amount|follow_one_hour|follow_one_hour_amount|+-------+------------+------+-------------+--------------------+---------------+----------------------+|10006 |201501120200|24234 |201501120130 |2342 |201501120300 |31232 |+-------+------------+------+-------------+--------------------+---------------+----------------------+ 分析：pyspark.sql.functions.lag(col, count=1, default=none)是取前N行的值pyspark.sql.functions.lead(col, count=1, default=none)是取后N行的值。 4.按照销售金额进行排名，金额最大的排最前（limit可以取topn的数） df.withColumn( 'rn', dense_rank().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) +-------+------------+-------+---+|shop_id|date |amount |rn |+-------+------------+-------+---+|10006 |201501120800|5645622|1 ||10006 |201501122100|2344562|2 ||10006 |201501121430|652562 |3 ||10006 |201501121230|534236 |4 ||10006 |201501120700|425432 |5 ||10006 |201501120900|366642 |6 ||10006 |201501121330|353462 |7 ||10006 |201501121200|254352 |8 ||10006 |201501122230|242546 |9 ||10006 |201501122030|242443 |10 ||10006 |201501120600|231635 |11 ||10006 |201501120330|221313 |12 ||10006 |201501120930|74632 |13 ||10006 |201501121900|65642 |14 ||10006 |201501121400|64562 |15 ||10006 |201501121000|63562 |16 ||10006 |201501130000|45245 |17 ||10006 |201501120730|36362 |18 ||10006 |201501121300|35432 |19 ||10006 |201501120830|34532 |20 ||10006 |201501120300|31232 |21 ||10006 |201501121630|27843 |22 ||10006 |201501121030|26353 |23 ||10006 |201501121130|26352 |24 ||10006 |201501122000|25376 |25 ||10006 |201501121730|24683 |26 ||10006 |201501120200|24234 |27 ||10006 |201501120100|23112 |28 ||10006 |201501120130|23112 |28 ||10006 |201501122300|6542 |29 ||10006 |201501121530|6422 |30 ||10006 |201501122130|5462 |31 ||10006 |201501121830|5342 |32 ||10006 |201501121800|4532 |33 ||10006 |201501122330|2546 |34 ||10006 |201501120630|2536 |35 ||10006 |201501122200|2535 |36 ||10006 |201501121930|2534 |37 ||10006 |201501121500|2456 |38 ||10006 |201501121700|2362 |39 ||10006 |201501121100|2353 |40 ||10006 |201501120030|2313 |41 ||10006 |201501120500|2234 |42 ||10006 |201501120530|2234 |42 ||10006 |201501120430|2231 |43 ||10006 |201501120400|2134 |44 ||10006 |201501121600|422 |45 ||10006 |201501120230|132 |46 |+-------+------------+-------+---+ df.withColumn( 'rn', rank().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) +-------+------------+-------+---+|shop_id|date |amount |rn |+-------+------------+-------+---+|10006 |201501120800|5645622|1 ||10006 |201501122100|2344562|2 ||10006 |201501121430|652562 |3 ||10006 |201501121230|534236 |4 ||10006 |201501120700|425432 |5 ||10006 |201501120900|366642 |6 ||10006 |201501121330|353462 |7 ||10006 |201501121200|254352 |8 ||10006 |201501122230|242546 |9 ||10006 |201501122030|242443 |10 ||10006 |201501120600|231635 |11 ||10006 |201501120330|221313 |12 ||10006 |201501120930|74632 |13 ||10006 |201501121900|65642 |14 ||10006 |201501121400|64562 |15 ||10006 |201501121000|63562 |16 ||10006 |201501130000|45245 |17 ||10006 |201501120730|36362 |18 ||10006 |201501121300|35432 |19 ||10006 |201501120830|34532 |20 ||10006 |201501120300|31232 |21 ||10006 |201501121630|27843 |22 ||10006 |201501121030|26353 |23 ||10006 |201501121130|26352 |24 ||10006 |201501122000|25376 |25 ||10006 |201501121730|24683 |26 ||10006 |201501120200|24234 |27 ||10006 |201501120100|23112 |28 ||10006 |201501120130|23112 |28 ||10006 |201501122300|6542 |30 ||10006 |201501121530|6422 |31 ||10006 |201501122130|5462 |32 ||10006 |201501121830|5342 |33 ||10006 |201501121800|4532 |34 ||10006 |201501122330|2546 |35 ||10006 |201501120630|2536 |36 ||10006 |201501122200|2535 |37 ||10006 |201501121930|2534 |38 ||10006 |201501121500|2456 |39 ||10006 |201501121700|2362 |40 ||10006 |201501121100|2353 |41 ||10006 |201501120030|2313 |42 ||10006 |201501120500|2234 |43 ||10006 |201501120530|2234 |43 ||10006 |201501120430|2231 |45 ||10006 |201501120400|2134 |46 ||10006 |201501121600|422 |47 ||10006 |201501120230|132 |48 |+-------+------------+-------+---+ df.withColumn( 'rn', row_number().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) +-------+------------+-------+---+|shop_id|date |amount |rn |+-------+------------+-------+---+|10006 |201501120800|5645622|1 ||10006 |201501122100|2344562|2 ||10006 |201501121430|652562 |3 ||10006 |201501121230|534236 |4 ||10006 |201501120700|425432 |5 ||10006 |201501120900|366642 |6 ||10006 |201501121330|353462 |7 ||10006 |201501121200|254352 |8 ||10006 |201501122230|242546 |9 ||10006 |201501122030|242443 |10 ||10006 |201501120600|231635 |11 ||10006 |201501120330|221313 |12 ||10006 |201501120930|74632 |13 ||10006 |201501121900|65642 |14 ||10006 |201501121400|64562 |15 ||10006 |201501121000|63562 |16 ||10006 |201501130000|45245 |17 ||10006 |201501120730|36362 |18 ||10006 |201501121300|35432 |19 ||10006 |201501120830|34532 |20 ||10006 |201501120300|31232 |21 ||10006 |201501121630|27843 |22 ||10006 |201501121030|26353 |23 ||10006 |201501121130|26352 |24 ||10006 |201501122000|25376 |25 ||10006 |201501121730|24683 |26 ||10006 |201501120200|24234 |27 ||10006 |201501120100|23112 |28 ||10006 |201501120130|23112 |29 ||10006 |201501122300|6542 |30 ||10006 |201501121530|6422 |31 ||10006 |201501122130|5462 |32 ||10006 |201501121830|5342 |33 ||10006 |201501121800|4532 |34 ||10006 |201501122330|2546 |35 ||10006 |201501120630|2536 |36 ||10006 |201501122200|2535 |37 ||10006 |201501121930|2534 |38 ||10006 |201501121500|2456 |39 ||10006 |201501121700|2362 |40 ||10006 |201501121100|2353 |41 ||10006 |201501120030|2313 |42 ||10006 |201501120500|2234 |43 ||10006 |201501120530|2234 |44 ||10006 |201501120430|2231 |45 ||10006 |201501120400|2134 |46 ||10006 |201501121600|422 |47 ||10006 |201501120230|132 |48 |+-------+------------+-------+---+ 分析：dense_rank和rank都是排名函数，区别在于dense_rank是连续排名，rank遇到排名并列时，下一列排名跳空。row_number是加行号，次序是连续的，不会存在重复的行号 5.按销售金额排序，取出前20%的时间段和相应金额 df.withColumn( 'tile', ntile(5).over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'tile' ).show(50, truncate=False) +-------+------------+-------+----+|shop_id|date |amount |tile|+-------+------------+-------+----+|10006 |201501120800|5645622|1 ||10006 |201501122100|2344562|1 ||10006 |201501121430|652562 |1 ||10006 |201501121230|534236 |1 ||10006 |201501120700|425432 |1 ||10006 |201501120900|366642 |1 ||10006 |201501121330|353462 |1 ||10006 |201501121200|254352 |1 ||10006 |201501122230|242546 |1 ||10006 |201501122030|242443 |1 ||10006 |201501120600|231635 |2 ||10006 |201501120330|221313 |2 ||10006 |201501120930|74632 |2 ||10006 |201501121900|65642 |2 ||10006 |201501121400|64562 |2 ||10006 |201501121000|63562 |2 ||10006 |201501130000|45245 |2 ||10006 |201501120730|36362 |2 ||10006 |201501121300|35432 |2 ||10006 |201501120830|34532 |2 ||10006 |201501120300|31232 |3 ||10006 |201501121630|27843 |3 ||10006 |201501121030|26353 |3 ||10006 |201501121130|26352 |3 ||10006 |201501122000|25376 |3 ||10006 |201501121730|24683 |3 ||10006 |201501120200|24234 |3 ||10006 |201501120100|23112 |3 ||10006 |201501120130|23112 |3 ||10006 |201501122300|6542 |3 ||10006 |201501121530|6422 |4 ||10006 |201501122130|5462 |4 ||10006 |201501121830|5342 |4 ||10006 |201501121800|4532 |4 ||10006 |201501122330|2546 |4 ||10006 |201501120630|2536 |4 ||10006 |201501122200|2535 |4 ||10006 |201501121930|2534 |4 ||10006 |201501121500|2456 |4 ||10006 |201501121700|2362 |5 ||10006 |201501121100|2353 |5 ||10006 |201501120030|2313 |5 ||10006 |201501120500|2234 |5 ||10006 |201501120530|2234 |5 ||10006 |201501120430|2231 |5 ||10006 |201501120400|2134 |5 ||10006 |201501121600|422 |5 ||10006 |201501120230|132 |5 |+-------+------------+-------+----+ 分析： NTILE就是把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号 设置n=5，那么ntile就会把排好序的数据均分成n个组，ntile函数会返回每条数据所在组的组编号，从而可以达到取前百分比的数据 思考：在使用row_number函数的时候，并没有指定rowsBetween，那么默认应该是默认的rows between unbounded preceding and current row。但是，结果却是把组内的所有元素都进行了标号 rowsBetween应该是针对于具有聚合性质的函数起作用","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"}]},{"title":"Spark学习笔记-union方法","slug":"Spark学习笔记-union方法","date":"2019-03-12T01:53:56.000Z","updated":"2019-03-24T01:33:47.225Z","comments":true,"path":"post/Spark学习笔记-union方法/","link":"","permalink":"https://shang.at/post/Spark学习笔记-union方法/","excerpt":"","text":"方法简介pyspark.sql.dataframe.DataFrame#union(other) union两个df，效果相当于union all(pyspark.sql.dataframe.DataFrame#unionAll在2.0以后就被Deprecated了)。 union方法的特点是： schema会使用前面df的schema， 只有两个有相同数量列的df才能进行union， union的时候会根据列的顺序进行union，与属性名无关 案例测试from pyspark.sql import SparkSessionfrom pyspark.sql.functions import *spark = SparkSession \\ .builder \\ .master('local[*]') \\ .enableHiveSupport() \\ .getOrCreate() df1 = spark.createDataFrame([&#123;'name':'cc', 'age':24&#125;, &#123;'name':'aa', 'age':25&#125;])df1.printSchema() root |-- age: long (nullable = true) |-- name: string (nullable = true)df2 = spark.createDataFrame([&#123;'name1':'bb', 'age1':2&#125;, &#123;'name1':'dd', 'age1':3&#125;])df2.printSchema() root |-- age1: long (nullable = true) |-- name1: string (nullable = true)df1.union(df2) DataFrame[age: bigint, name: string]union_df = df1.union(df2)union_df.printSchema() root |-- age: long (nullable = true) |-- name: string (nullable = true)union_df.show() +---+----+ |age|name| +---+----+ | 24| cc| | 25| aa| | 2| bb| | 3| dd| +---+----+select_union_df = df1.select('name').union(df2.select('age1'))select_union_df.printSchema() root |-- name: string (nullable = true)select_union_df.show() +----+ |name| +----+ | cc| | aa| | 2| | 3| +----+select_union_df = df1.select('name', 'age').union(df2.select('age1')) pyspark.sql.utils.AnalysisException: \"Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 1 columns;;\\n'Union\\n:- Project [name#1, age#0L]\\n: +- LogicalRDD [age#0L, name#1], false\\n+- Project [age1#4L]\\n +- LogicalRDD [age1#4L, name1#5], false\\n\"select_union_df = df1.select('age').union(df2.select('name1'))select_union_df.show() +---+ |age| +---+ | 24| | 25| | bb| | dd| +---+select_union_df.printSchema() root |-- age: string (nullable = true)select_union_df = df1.select('name', 'age').union(df2.select('age1', 'name1'))select_union_df.printSchema() root |-- name: string (nullable = true) |-- age: string (nullable = true)select_union_df.show() +----+---+ |name|age| +----+---+ | cc| 24| | aa| 25| | 2| bb| | 3| dd| +----+---+ 其他案例要想实现sql中union的效果，需要结合distinct()来使用: df1 = spark.createDataFrame([&#123;'name':'cc', 'age':24&#125;, &#123;'name':'aa', 'age':25&#125;])df2 = spark.createDataFrame([&#123;'name1':'cc', 'age1':24&#125;, &#123;'name1':'dd', 'age1':3&#125;])df1.union(df2).show() +---+----+ |age|name| +---+----+ | 24| cc| | 25| aa| | 24| cc| | 3| dd| +---+----+df1.union(df2).distinct().show() +---+----+ |age|name| +---+----+ | 24| cc| | 3| dd| | 25| aa| +---+----+","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"}]},{"title":"Spark学习笔记一","slug":"Spark学习笔记一","date":"2019-03-10T10:45:03.000Z","updated":"2019-03-24T01:33:47.230Z","comments":true,"path":"post/Spark学习笔记一/","link":"","permalink":"https://shang.at/post/Spark学习笔记一/","excerpt":"","text":"版本：pyspark 2.4.0 主要包 pyspark pyspark.sql module pyspark.streaming module pyspark.ml package pyspark.mllib package pyspark pyspark.SparkConf(loadDefaults=True, _jvm=None, _jconf=None) 是spark应用的配置类，默认loadDefaults=True，会自动加载java系统参数中的spark.*的参数，_jconf是一个已经存在的sparkConf句柄 主要api： setMaster() 设置应用的提交类型：local|local[n]|local[*] or 不填，本地测试时可以填local系列，提交到集群运行时可以不用填，提交任务的时候会根据集群的配置，自动选择提交的类型：standalone或者yarn模式 setAppName() 设置应用的名称 pyspark.SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=) spark应用上下文，是spark应用的主要入口。代表了与spark cluster的链接，可以用来在集群中创建RDD和广播变量。 主要api讲解： addFile(self, path, recursive=False) 为spark job添加一个可下载文件，spark的每一个node都会下载一份，可以是local file、hdfs file、http file、https file或ftp file。可以使用SparkFiles通过文件名来读取设置的文件， 注意，每个应用中，每个文件名只能设置一次。recursive设置为True时，传递的path可以是目录，但是目前只支持hdfs file的场景 import osfrom tempfile import gettempdirfrom pyspark import SparkConffrom pyspark import SparkContextif __name__ == '__main__': from pyspark import SparkFiles conf = SparkConf() conf.setMaster(\"local\").setAppName(\"My app\") sc = SparkContext(conf=conf) path = os.path.join(gettempdir(), \"test.txt\") print(path) with open(path, \"w\") as testFile: _ = testFile.write(\"100\") sc.addFile(path) def func(iterator): with open(SparkFiles.get('test.txt')) as testFile: fileVal = int(testFile.readline()) print(fileVal) return [x * fileVal for x in iterator] result = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect() print(result) accumulator(value, accum_param=None) 创建一个累加器。一个全局共享的可以进行累加的变量，只能在worker上进行update操作，在driver上获取结果值得操作。值类型默认是int和float类型，也可以使用accum_param参数设置为自定义的数据类型 # 代码后加 broadcast(value) 在集群中广播一个只读的值，返回一个Broadcast对象，以便在分布式方法中调用。被广播的变量只会被发送到集群的各个node上一次","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"pyspark官方文档学习","slug":"pyspark官方文档学习","permalink":"https://shang.at/tags/pyspark官方文档学习/"}]},{"title":"数据仓库学习笔记二-建模流程","slug":"数据仓库学习笔记二-建模流程","date":"2019-03-03T11:36:50.000Z","updated":"2019-03-03T12:26:38.927Z","comments":true,"path":"post/数据仓库学习笔记二-建模流程/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记二-建模流程/","excerpt":"","text":"数据建模的基本流程在建模的不同阶段，将数据模型分为三个层次，每层的作用各不相同。 概念模型:确定系统的核心以及划清系统范围和 边界 逻辑模型:梳理业务规则以及对概念模型的求精 物理模型:从性能、访问、开发等多方面考虑， 做系统的实现 概念模型概念建模小贴士1 注重全局的理解而非细节 在概念模型阶段，即需要对整体架构做思考 概念模型通常是自上而下的模式，通过会议等模式反复沟通，澄清需求 在此阶段，应粗略地估算出整个项目需要的时间以及项目计划草案 根据计划粗略地估算出项目的费用 是数据模型工程师与客户沟通的破冰之旅，使他们在此期间达成共识并奠定未来良好的沟通基础以及私人关系 出品的概念模型可以帮助划定系统边界以及避免方向性的错误 商业主导，相比技术专家而言，更需要商业专家 是未来逻辑模型的沟通基础，以及逐步求精的依据 概念模型交付品通常具备如下特点: 与客户一致的商业语言 尽量一页纸描述清楚整个模型 通常用实体关系型图表示，但不需添加实体的属性 允许多对多的关系存在 逻辑模型逻辑建模小提示1 应更精确估算出整个项目需要的时间以及项目计划草案 并且根据计划更精确地估算出项目的费用 当实体数量超过100时，需要定义术语表 规范化 先规范化再逆规范化，不可一步到位 不可缺少约束的定义 使用CASE工具做逻辑模型 多对多关系需要解决 需要同级评审(Peer Review) 确定可信赖数据源，关键属性需用真实数据验证 应用成熟的建模模式(Pattern) 一定程度的抽象化，决定了未来模型的弹性 高质量的模型定义 重要关联关系需要强制建立 与概念模型保持一致 注意模型的版本管理 非常非常注意细节 数据库专家深度介入 占据整个数据建模80%以上时间 不要忽视属性的长度定义和约束定义 不要忽视属性的默认值(Default Value) 使用控制数据范围的域(Domain) 逻辑建模交付品的特点 要像一本书，而非一页纸 所有实体属性均需添加 实体间关系要清晰描述 使用术语表 遵循命名规范 采用CASE工具创建项目文件 对各个实体必须有清晰描述 对关键属性必须有清晰描述 物理模型物理建模小贴士1 使用CASE工具由逻辑模型自动生成 应用术语表自动转换生成字段名称 对表空间、索引、视图、物化视图、主键、外键等都有命名规则 逆规范化在逻辑层完成，而非本层 数据库DBA深度介入，需要DBA的评审(Peer Review) 和数据库的DDL保持一致 注意版本管理 注意开发、测试、生产三个不同版本的模型管理 注意性能 估算数据规模 考虑数据归档 充分考虑未来使用数据库的优点和缺点 物理建模交付品的特点 自动生成基础库表结构，之后适度手动调整 与未来要使用的数据库类型息息相关 生成数据字典并发布 可直接用于生成DDL DDL中注意注释的生成 如何进行高质量数据建模什么样的模型算是高质量数据模型? 对真实世界的抽象正确而完整 用建模语言表达清晰而准确 框架稳定且灵活，满足当下的需求并能够一定程度容纳未来的变化 根据需求尽可能减少数据冗余 充分考虑潜在的性能问题 从企业全局的视角出发构筑模型","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据仓库学习笔记二-重要意义","slug":"数据仓库学习笔记二-重要意义","date":"2019-03-03T10:55:58.000Z","updated":"2019-03-03T11:36:09.787Z","comments":true,"path":"post/数据仓库学习笔记二-重要意义/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记二-重要意义/","excerpt":"","text":"数据时代的演化DIKWdata数据 + information信息 + knowledge知识 + wisdom智慧 描述数据的数据被称为元数据(metadata) 信息（information）= 元数据（metadata）+数据（data） 什么是数据模型数据模型实际上就是为了装载数据，用元数据搭建起来的框子 数据模型是将数据元素以标准化的模式组织起来, 用来模拟现实世界的信息框架蓝图。 数据模型的要求: 直观地模拟世界 容易为人所理解 便于计算机实现 数据模型是整个数据应用的基石，牵一发而动全身。数据模型的小小改动将会导致上层数据应用的大幅度变化 建设高质量数据模型的意义低质量数据模型的十宗罪 没有准确的捕获到需求 数据模型不完整 各层模型与其扮演角色不匹配 数据结构不合理 抽象化不够，造成模型不灵活 没有或者不遵循命名规范 缺少数据模型的定义和描述 数据模型可读性差 元数据与数据不匹配 数据模型与企业标准不一致 低质量数据模型的影响 大量修改和重做 重复建设 知识丢失 下游开发困难 高成本 数据质量低下 新业务无法展开 意义","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据仓库学习笔记一","slug":"数据仓库学习笔记一","date":"2019-03-03T10:24:23.000Z","updated":"2019-03-03T11:12:25.458Z","comments":true,"path":"post/数据仓库学习笔记一/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记一/","excerpt":"","text":"学习路线： 高质量数据建模基础 经典数据仓库架构 EDW建模 维度建模 数据仓库生命周期","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据结构学习笔记二-复杂度分析","slug":"数据结构学习笔记二-复杂度分析","date":"2019-03-03T10:22:47.000Z","updated":"2020-04-10T00:41:06.625Z","comments":true,"path":"post/数据结构学习笔记二-复杂度分析/","link":"","permalink":"https://shang.at/post/数据结构学习笔记二-复杂度分析/","excerpt":"","text":"时间复杂度 通常使用大O时间复杂度表示法：它表示了代码执行时间随数据规模增长的变化趋势， 亦可称为 渐进式时间复杂度 几种常见时间复杂度实例分析 各种时间复杂度执行时间比较 故可以看出来O(logn)的算法要比O(n)的算法效率上快上不少 常见排序算法的时间复杂度 空间复杂度 亦可称为 渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系 空间复杂度基本就只有O(1)、O(n)、O(n^2) 时间复杂度进阶实际上我们前面说的大O表示法只是一个初级的判断方式，此外还有最好情况时间复杂度、最坏情况时间复杂度、平均时间复杂度、均摊时间复杂度。 在大部分的时间大O表示法已经能够分辨出各种算法的时间复杂度了。只有在特殊情况下（同一块代码(或分析出的相同的大O等级)在不同的情况下，时间复杂度有量级的差距），才需要使用最好、最坏、平均三种方法来区分。均摊的情况就更加特殊了，比如向数组加入一个元素，当数组长度不足时，扩充数组再添加元素。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"复杂度分析","slug":"复杂度分析","permalink":"https://shang.at/tags/复杂度分析/"}]},{"title":"数据结构学习笔记一","slug":"数据结构学习笔记一","date":"2019-03-03T09:34:30.000Z","updated":"2020-04-10T01:31:23.477Z","comments":true,"path":"post/数据结构学习笔记一/","link":"","permalink":"https://shang.at/post/数据结构学习笔记一/","excerpt":"","text":"基础知识数组为什么从0开始编号 数组的随机访问的寻址公式： a[i]_address = base_address + i * data_type_size 常见数据类型的占用内存大小 类型 字节大小 位数 表示范围 最大存储量 byte 1byte 8bits -$$2^7$$~$$2^7$$-1 $$2^8$$-1=255 int 4bytes 4*8bits -$$2^{31}$$~$$2^{31}$$-1 short 2bytes 2*8bits -$$2^{15}$$~$$2^{15}$$-1 long 8bytes 8*8bits -$$2^{63}$$~$$2^{63}$$-1 float 4bytes 4*8bits double 8bytes 8*8bits char 2bytes 2*bits 二进制-十进制计算机内部的二进制表示法：原码、反码、补码 数组链表栈队列跳表散列表关键点 散列思想 散列表是利用了数组可以根据下标随机访问的特性，而产生的一种高性能的数据结构(查找的时间复杂度为O(1))。散列表其实就是数组的一种拓展，是由数组演化而来。 其关键的概念有 键-key、 将键映射到数组下标的方法-散列函数(哈希函数)、 散列函数计算来的值-散列值(哈希值) 散列函数 散列冲突 开放寻址 链表法 进阶：链表使用红黑树或者跳表实现 具体实现|打造一个工业级散列表 散列函数 装载因子 散列冲突解决方法 二叉树红黑树递归树堆图Trie数AC自动机位图布隆过滤器B+数索引","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"}]},{"title":"Python数据科学学习笔记一","slug":"Python数据科学学习笔记一","date":"2019-03-03T09:19:14.000Z","updated":"2019-08-03T11:51:48.061Z","comments":true,"path":"post/Python数据科学学习笔记一/","link":"","permalink":"https://shang.at/post/Python数据科学学习笔记一/","excerpt":"","text":"描述性统计和探索型数据分析变量类型 名义变量 等级变量 连续变量 描述名义变量的分布两个统计量 频数 百分比 可视化 柱状图","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"描述性统计和探索型数据分析","slug":"描述性统计和探索型数据分析","permalink":"https://shang.at/tags/描述性统计和探索型数据分析/"}]},{"title":"记一次线上JVM问题调试","slug":"记一次线上JVM问题调试","date":"2019-02-27T03:21:32.000Z","updated":"2019-03-01T00:40:58.622Z","comments":true,"path":"post/记一次线上JVM问题调试/","link":"","permalink":"https://shang.at/post/记一次线上JVM问题调试/","excerpt":"发现java进程 使用linux命令：ps ps -ef | grep prcessName[root@YZSJHL81-35 ~]# ps -ef | grep Bootstraproot 118271 1 99 00:10 ? 3-16:08:07 Bootstrap startroot 197682 197016 0 17:08 pts/0 00:00:00 grep Bootstrap 使用java自带的检测工具 jps [root@YZSJHL81-35 ~]# jps118271 Bootstrap197101 Jps","text":"发现java进程 使用linux命令：ps ps -ef | grep prcessName[root@YZSJHL81-35 ~]# ps -ef | grep Bootstraproot 118271 1 99 00:10 ? 3-16:08:07 Bootstrap startroot 197682 197016 0 17:08 pts/0 00:00:00 grep Bootstrap 使用java自带的检测工具 jps [root@YZSJHL81-35 ~]# jps118271 Bootstrap197101 Jps 检测java进程启动的时间 ps -p 118271 -o etimeps -p 118271 -o etime= 检测进程打开的文件数 lsof -n -p 118271 | wc -l 检测系统的进程 top top - 17:11:21 up 132 days, 2:14, 1 user, load average: 0.12, 0.21, 0.18Tasks: 298 total, 1 running, 297 sleeping, 0 stopped, 0 zombieCpu(s): 1.0%us, 0.6%sy, 0.0%ni, 98.0%id, 0.3%wa, 0.0%hi, 0.1%si, 0.0%stMem: 99009860k total, 90554384k used, 8455476k free, 333672k buffersSwap: 0k total, 0k used, 0k free, 81820024k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMANDq-退出1-显示所有的cpu及其状态 查看指定进程下所有线程的状态 top -Hp pid 查看JVM的GC状态jstat -gcutil -t pid 1000 100 ---查看pid进程的GC状态每隔1000ms一次，一共100次 ​ 查看JVM栈信息printf '%x\\n' tid -- 打印指定线程id的十六进制表示jstack pid | grep 'tid十六进制' -C20 -color -- 查看进程的栈信息jmap -histo:live pid --查看进程所有存活的实例jmap -dumap:format=b,file=dump.hprof pid --导出JVM的dump文件","categories":[{"name":"JVM","slug":"JVM","permalink":"https://shang.at/categories/JVM/"}],"tags":[{"name":"JVM问题调试","slug":"JVM问题调试","permalink":"https://shang.at/tags/JVM问题调试/"}]},{"title":"辨析 Sass 中的 Map 和 List","slug":"demo","date":"2015-10-21T02:34:12.000Z","updated":"2019-03-01T00:39:39.899Z","comments":true,"path":"post/demo/","link":"","permalink":"https://shang.at/post/demo/","excerpt":"如果你使用过 Sass 3.3 之前的版本，那么你一定对那段时光颇有感触，那时候没有现如今这么好的条件，那时候的 Map 还只能用多重列表（lists of list）来模拟。多重列表可以实现复杂数据的嵌套定义，但却不是以键值对的形式实现的，所有当我们需要获取其中特定的某一项时就会比较麻烦。Map 这种数据类型天生就是基于键值对的形式，非常便于组织数据。 自从可以使用 Map 之后，开发者们开始毫无顾忌地定义 Map 存储数据，比如断点宽度、颜色值、栅格布局等等响应式排版的细节，都被一股脑的塞进了 Map 中。 那么，有了 Map 之后，我们还有必要使用 List 吗？可能某些人会觉得为了保持向后兼容应该继续使用多重列表模拟 Map，因为可能有些开发者仍然在使用老版本的 Sass 编译器，但实际上，这是多此一举了，Sass 的版本通常由 package.json 或者其他同类型的项目配置文件所控制，往往只需一条命令（gem update sass）即可更新 Sass 的版本，因此基本上无需考虑对老版本的兼容问题。","text":"如果你使用过 Sass 3.3 之前的版本，那么你一定对那段时光颇有感触，那时候没有现如今这么好的条件，那时候的 Map 还只能用多重列表（lists of list）来模拟。多重列表可以实现复杂数据的嵌套定义，但却不是以键值对的形式实现的，所有当我们需要获取其中特定的某一项时就会比较麻烦。Map 这种数据类型天生就是基于键值对的形式，非常便于组织数据。 自从可以使用 Map 之后，开发者们开始毫无顾忌地定义 Map 存储数据，比如断点宽度、颜色值、栅格布局等等响应式排版的细节，都被一股脑的塞进了 Map 中。 那么，有了 Map 之后，我们还有必要使用 List 吗？可能某些人会觉得为了保持向后兼容应该继续使用多重列表模拟 Map，因为可能有些开发者仍然在使用老版本的 Sass 编译器，但实际上，这是多此一举了，Sass 的版本通常由 package.json 或者其他同类型的项目配置文件所控制，往往只需一条命令（gem update sass）即可更新 Sass 的版本，因此基本上无需考虑对老版本的兼容问题。 使用多重列表替代 Map 的优势之一就是减少代码量。下面让我们来比较一下多种列表和 Map 的语法结构以及遍历方式。 测试表格 Variable Description site Sitewide information. page Page specific information and custom variables set in front-matter. config Site configuration theme Theme configuration. Inherits from site configuration. _ (single underscore) Lodash library path Path of current page url Full URL of current page env Environment variables 语法比较 测试标题 在下面的示例中，我创建了一个用于控制响应式布局的数据，该数据一共有四个断点，每一个断点都包含了 min-width、max-width、font-size 和 line-height 四个样式。 Map 语法下面就是使用 Map 存储的数据，具体来说，该 Map 中首先存储了四个用于标识断点的 Key，相对应的是保存具体属性值得 Value。虽然这种形式可读性更高，但是总体代码量却高达 26 行 450 个字符。 $breakpoint-map: ( small: ( min-width: null, max-width: 479px, base-font: 16px, vertical-rhythm: 1.3 ), medium: ( min-width: 480px, max-width: 959px, base-font: 18px, vertical-rhythm: 1.414 ), large: ( min-width: 960px, max-width: 1099px, base-font: 18px, vertical-rhythm: 1.5 ), xlarge: ( min-width: 1100px, max-width: null, base-font: 21px, vertical-rhythm: 1.618 )); 多重列表语法下面的多重列表存储了和上面 Map 同样的数据，在多重列表中没有 Key-Value 的对应关系，这意味着要想找到特定的值，必须使用遍历或 nth() 的方式来实现了。从另一个角度来看，多种列表又比 Map 的代码量小得多，总共只有六行 180 个字符。 $breakpoint-list: ( (small, null, 479px, 16px, 1.3), (medium, 480px, 959px, 18px, 1.414), (large, 960px, 1099px, 18px, 1.5), (xlarge, 1100px, null, 21px, 1.618)); 遍历比较 测试标题 从上面简单地比较中可以粗略的看出，多种列表的代码量明显少于 Map。但是，如果我们需要遍历这些值得话，复杂度又是怎样的呢？ 遍历 Map我们可以使用如下的代码遍历 Map： @each $label, $map in $breakpoint-map &#123;&#125; 这里的变量 $label 和 $map 会随着对 $breakpoint-map 的遍历被动态地赋值，$label 将会被赋值为 $breakpoint-map 的 Key，而 $map 会被赋值为 $breakpoint-map 的 Value。为了在遍历过程中获取特定值，我们就需要使用 Sass 原生的 map-get() 函数，使用该函数需要传入两个参数：Map 的名字和求取的 Key，最后返回该 Map 中匹配该 Key 的 Value。 具体的做法就是使用 @each 遍历 Map，然后使用 map-get() 获取特定值，最终只需要六行代码 220 个字符即可完成整个遍历： @each $label, $map in $breakpoint-map &#123; $min-width: map-get($map, min-width); $max-width: map-get($map, max-width); $base-font: map-get($map, base-font); $vertical-rhythm: map-get($map, vertical-rhythm);&#125; 遍历多重列表遍历多重列表不必像遍历 Map 一样动态获取到 Map 后再使用 map-get() 函数取特定值，直接遍历一遍即可获得特定值。 因为多种列表内层的每一个列表结构相同，都有按照相同顺序排列的五个值，所以我们可以持续遍历每个值并赋值给特定的变量。无需调用 map-get()，直接引用这些变量即可进行赋值等裸机操作。最终遍历多重列表只使用了两行代码 100 个字符： @each $label, $min-width, $max-width, $base-font, $vertical-rhythm in $breakpoint-list &#123;&#125; 慎用多重列表 测试标题 经过上述的比对，看起来多重列表各方面都在碾压 Map，实则不然，Sass 中添加 Map 有一条非常重要的原因就是：Key-Value 的映射关系。 遗漏键值如果要使用多重列表，那么就必须保证自己非常熟悉多重列表内部的每一项所代表的意义。下面我们举个例子，来看看遗漏了某些值的情况： $breakpoint-list: ( (small, null, 479px, 16px, 1.3), (medium, 480px, 959px, 18px, 1.414), (large, 960px, 1099px, 18px, 1.5), (xlarge, 1100px, 21px, 1.618));p &#123; @each $label, $min-width, $max-width, $base-font, $vertical-rhythm in $breakpoint-list &#123; @if $min-width &#123; @include breakpoint( $min-width ) &#123; font-size: $base-font; line-height: $vertical-rhythm; &#125; &#125; @else &#123; font-size: $base-font; line-height: $vertical-rhythm; &#125; &#125;&#125; 当我们尝试运行这段代码时，结果肯定是错误地，因为在 $breakpoint-list 的最后一行，xlarge 被赋值给了 $label，1100px 被赋值给了 $min-width，21px 被赋值给了 $max-width, 1.618 被赋值给了 $base-font，最终导致 $vertical-rhythm 没有被赋值，结果就是 font-size 的属性值是错的，line-height 的属性值是空的。此外，Sass 还不会对此抛出错误，导致我们无从知晓错误所在。 如果我们使用 Map 来代替这里的多重列表，那么使用 map-get() 函数即使遇见空值也能正确获得想要的结果。这就是值得我们慎重思考的地方：多种列表虽然简单快速，但是丧失了 Map 中的容错能力和快速取值能力。 查找特定列表在多重列表中查找特定列表简直就是一种折磨。如果使用 Map，那么配合 map-get() 函数可以快速定位到特定子 Map： $medium-map: map-get($maps, medium); 但如果要获取多种列表 medium 列表，麻烦可就大了： @function get-list($label) &#123; @each $list in $breakpoint-list &#123; @if nth($list, 1) == $label &#123; @return $list; &#125; &#125; @return null;&#125;$medium-list: get-list(medium); 这段代码的逻辑就是遍历整个多重列表，知道找到第一个匹配项，然后返回，如果一直没有找到匹配项，就一直遍历到末尾，然后返回 null。这实际上就是手工实现了 map-get() 的逻辑。 缺少原生的 Map 函数Sass 提供了诸多的原生函数用于处理 Map 数据类型，但是多重列表是没法调用这些函数的，比如，使用 map-merge() 可以合并两个 Map，如果两个 Map 有相同的值，则取第二个 Map 的值为最终值。当然你也可以在多重列表中使用 join() 或 append() 来增加新列表，从而模拟出 map-merge() 的效果。 另一个实用的 Map 函数就是 map-has-key()，对于依赖 map-get() 的自定义函数来说，map-has-key() 可以用来验证特定的 Key 是否存在。但在列表中是完全没有相似的方法。 总结 Test Title 相比起列表来说，Key-Value 模型的 Map 显然更有力量，原生的 Sass Map 函数更是提供了强力的数据查找和验证工具。 虽然多重列表代码量少，但并不能像 Map 一样进行错误检查或验证参数。在大多数时候，相比较多重列表而言，我相信 Map 是更好的选择。如果是为了更少的代码量和其他简单地调用，那么我偶尔会用用多重列表，但是从项目的宏观控制和数据存储方面显然更优秀。","categories":[],"tags":[{"name":"css","slug":"css","permalink":"https://shang.at/tags/css/"}]}],"categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"},{"name":"JAVA源码","slug":"JAVA源码","permalink":"https://shang.at/categories/JAVA源码/"},{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"},{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"},{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"},{"name":"JAVA","slug":"JAVA","permalink":"https://shang.at/categories/JAVA/"},{"name":"Mysql","slug":"Mysql","permalink":"https://shang.at/categories/Mysql/"},{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://shang.at/categories/Hadoop/"},{"name":"Pandas","slug":"Pandas","permalink":"https://shang.at/categories/Pandas/"},{"name":"BI","slug":"BI","permalink":"https://shang.at/categories/BI/"},{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"},{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"},{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"},{"name":"JVM","slug":"JVM","permalink":"https://shang.at/categories/JVM/"}],"tags":[{"name":"外排-分组归并-桶排序","slug":"外排-分组归并-桶排序","permalink":"https://shang.at/tags/外排-分组归并-桶排序/"},{"name":"并发编程-Future","slug":"并发编程-Future","permalink":"https://shang.at/tags/并发编程-Future/"},{"name":"常见操作技巧","slug":"常见操作技巧","permalink":"https://shang.at/tags/常见操作技巧/"},{"name":"lru_cache","slug":"lru-cache","permalink":"https://shang.at/tags/lru-cache/"},{"name":"并发编程-Thread","slug":"并发编程-Thread","permalink":"https://shang.at/tags/并发编程-Thread/"},{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"},{"name":"JVM","slug":"JVM","permalink":"https://shang.at/tags/JVM/"},{"name":"NIO","slug":"NIO","permalink":"https://shang.at/tags/NIO/"},{"name":"JAVA-Iterator","slug":"JAVA-Iterator","permalink":"https://shang.at/tags/JAVA-Iterator/"},{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"},{"name":"JAVA集合类-列表","slug":"JAVA集合类-列表","permalink":"https://shang.at/tags/JAVA集合类-列表/"},{"name":"IPC&RPC","slug":"IPC-RPC","permalink":"https://shang.at/tags/IPC-RPC/"},{"name":"代理","slug":"代理","permalink":"https://shang.at/tags/代理/"},{"name":"事务和隔离级别","slug":"事务和隔离级别","permalink":"https://shang.at/tags/事务和隔离级别/"},{"name":"第三范式","slug":"第三范式","permalink":"https://shang.at/tags/第三范式/"},{"name":"python学习","slug":"python学习","permalink":"https://shang.at/tags/python学习/"},{"name":"python源码学习","slug":"python源码学习","permalink":"https://shang.at/tags/python源码学习/"},{"name":"查找算法","slug":"查找算法","permalink":"https://shang.at/tags/查找算法/"},{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/tags/Spark应用/"},{"name":"Hadoop-IPC","slug":"Hadoop-IPC","permalink":"https://shang.at/tags/Hadoop-IPC/"},{"name":"python中的时间处理","slug":"python中的时间处理","permalink":"https://shang.at/tags/python中的时间处理/"},{"name":"Configuration","slug":"Configuration","permalink":"https://shang.at/tags/Configuration/"},{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"},{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/tags/Spark学习/"},{"name":"算法","slug":"算法","permalink":"https://shang.at/tags/算法/"},{"name":"Tableau","slug":"Tableau","permalink":"https://shang.at/tags/Tableau/"},{"name":"sparkSql-DSL语法","slug":"sparkSql-DSL语法","permalink":"https://shang.at/tags/sparkSql-DSL语法/"},{"name":"排序算法","slug":"排序算法","permalink":"https://shang.at/tags/排序算法/"},{"name":"reduce","slug":"reduce","permalink":"https://shang.at/tags/reduce/"},{"name":"数据分析技巧","slug":"数据分析技巧","permalink":"https://shang.at/tags/数据分析技巧/"},{"name":"有初始值的聚合操作","slug":"有初始值的聚合操作","permalink":"https://shang.at/tags/有初始值的聚合操作/"},{"name":"分析指标","slug":"分析指标","permalink":"https://shang.at/tags/分析指标/"},{"name":"sparkSql内置函数","slug":"sparkSql内置函数","permalink":"https://shang.at/tags/sparkSql内置函数/"},{"name":"数据分析Tips","slug":"数据分析Tips","permalink":"https://shang.at/tags/数据分析Tips/"},{"name":"pyspark官方文档学习","slug":"pyspark官方文档学习","permalink":"https://shang.at/tags/pyspark官方文档学习/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"https://shang.at/tags/复杂度分析/"},{"name":"描述性统计和探索型数据分析","slug":"描述性统计和探索型数据分析","permalink":"https://shang.at/tags/描述性统计和探索型数据分析/"},{"name":"JVM问题调试","slug":"JVM问题调试","permalink":"https://shang.at/tags/JVM问题调试/"},{"name":"css","slug":"css","permalink":"https://shang.at/tags/css/"}]}