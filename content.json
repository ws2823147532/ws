{"meta":{"title":"努力，奋斗","subtitle":"记录学习","description":null,"author":"王尚","url":"https://shang.at","root":"/"},"pages":[{"title":"About","date":"2019-03-24T01:33:47.232Z","updated":"2019-03-24T01:33:47.232Z","comments":true,"path":"about/index.html","permalink":"https://shang.at/about/index.html","excerpt":"","text":"大数据工程师一枚 邮箱：2823147532@qq.com 此博客仅为个人学习数据结构和算法的一个笔记，就是想督促自己坚持学习。笔记内容主要来源于其他的博客和读完博客后的自己的一点思考"},{"title":"Categories","date":"2020-06-29T17:14:30.000Z","updated":"2020-06-29T17:15:11.496Z","comments":true,"path":"categories/index.html","permalink":"https://shang.at/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-06-29T17:14:30.000Z","updated":"2020-06-29T17:16:06.172Z","comments":true,"path":"tags/index.html","permalink":"https://shang.at/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据结构与算法-左神学习笔记-暴力递归到动态规划1-4","slug":"数据结构与算法-左神学习笔记-暴力规划到动态规划1-4","date":"2020-07-23T04:19:14.000Z","updated":"2020-07-23T04:22:14.421Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-暴力规划到动态规划1-4/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-暴力规划到动态规划1-4/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-动态规划","slug":"数据结构与算法-左神学习笔记-动态规划","date":"2020-07-23T04:18:52.000Z","updated":"2020-07-23T04:22:22.446Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-动态规划/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-动态规划/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-暴力递归","slug":"数据结构与算法-左神学习笔记-暴力递归","date":"2020-07-23T04:18:43.000Z","updated":"2020-07-23T04:22:11.929Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-暴力递归/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-暴力递归/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-并查集&图","slug":"数据结构与算法-左神学习笔记-并查集-图","date":"2020-07-23T04:18:31.000Z","updated":"2020-07-23T04:22:19.722Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-并查集-图/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-并查集-图/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-贪心算法","slug":"数据结构与算法-左神学习笔记-贪心算法","date":"2020-07-23T04:18:15.000Z","updated":"2020-07-23T04:22:35.728Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-贪心算法/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-贪心算法/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-二叉树的递归套路","slug":"数据结构与算法-左神学习笔记-二叉树的递归套路","date":"2020-07-23T04:18:05.000Z","updated":"2020-07-23T04:22:25.021Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-二叉树的递归套路/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-二叉树的递归套路/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-链表相关面试题","slug":"数据结构与算法-左神学习笔记-链表相关面试题","date":"2020-07-23T04:17:43.000Z","updated":"2020-07-23T04:22:32.905Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-链表相关面试题/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-链表相关面试题/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-Trie&桶排序&排序总结","slug":"数据结构与算法-左神学习笔记-Trie-桶排序-排序总结","date":"2020-07-23T04:17:29.000Z","updated":"2020-07-23T04:22:39.989Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-Trie-桶排序-排序总结/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-Trie-桶排序-排序总结/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-比较器&堆","slug":"数据结构与算法-左神学习笔记-比较器-堆","date":"2020-07-23T04:17:04.000Z","updated":"2020-07-23T04:22:17.032Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-比较器-堆/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-比较器-堆/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-归并&随机快排","slug":"数据结构与算法-左神学习笔记-归并-随机快排","date":"2020-07-23T04:16:44.000Z","updated":"2020-07-23T04:22:28.632Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-归并-随机快排/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-归并-随机快排/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-链表&栈&队列&递归&哈希表","slug":"数据结构与算法-左神学习笔记-链表-栈-队列-递归-哈希表","date":"2020-07-23T04:16:10.000Z","updated":"2020-07-23T04:16:19.428Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-链表-栈-队列-递归-哈希表/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-链表-栈-队列-递归-哈希表/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"数据结构与算法-左神学习笔记-复杂度&对数器&二分法&位运算","slug":"数据结构与算法-左神学习笔记-复杂度-对数器-二分法-位运算","date":"2020-07-23T04:15:02.000Z","updated":"2020-07-23T04:15:26.265Z","comments":true,"path":"post/数据结构与算法-左神学习笔记-复杂度-对数器-二分法-位运算/","link":"","permalink":"https://shang.at/post/数据结构与算法-左神学习笔记-复杂度-对数器-二分法-位运算/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"大数据-aws","slug":"大数据-aws","date":"2020-07-23T01:08:29.000Z","updated":"2020-07-23T01:13:53.160Z","comments":true,"path":"post/大数据-aws/","link":"","permalink":"https://shang.at/post/大数据-aws/","excerpt":"简介：","text":"简介： AWS常见命令aws s3 [commend] help aws s3 ls [—recursive] [—human-readable] [—summarize] aws s3 rm s3_path [—recursive] aws s3 cp s3_path target_path [—recursive]","categories":[{"name":"大数据","slug":"大数据","permalink":"https://shang.at/categories/大数据/"}],"tags":[{"name":"aws","slug":"aws","permalink":"https://shang.at/tags/aws/"}]},{"title":"Python学习-文件操作","slug":"Python学习-文件操作","date":"2020-07-22T03:24:24.000Z","updated":"2020-07-22T06:20:32.279Z","comments":true,"path":"post/Python学习-文件操作/","link":"","permalink":"https://shang.at/post/Python学习-文件操作/","excerpt":"简介：","text":"简介： 打开文件：open(filename, &#39;r|w|x|a|b|t|+&#39;, buffering=4096,) 检测路径是否存在：os.path.exists(path) 创建目录：os.mkdir(path) 递归创建路径：os.makedirs(path) 删除目录：os.rmdir(path) 递归删除目录：shutil.rmtree(path) 获取当前的工作目录：os.getcwd() 判断给定的路径是否为目录：os.path.isdir(source) 获取给定文件全路径的一级目录：os.path.dirname(source) 获取给定文件全路径的文件名：os.path.basename(source) 修改工作目录：os.chdir(source_dir) 拼接路径：os.path.join(path1, path2) 执行shell命令：os.system(cmd) 循环遍历指定目录下 的(直属的)所有文件(夹)：os.listdir(path) 递归遍历指定目录下的所有文件(夹)：os.walk(path)，返回的是一个generator 12345#conding=utf8 import os for path,dir_list,file_list in os.walk(path): for file_name in file_list: print(os.path.join(path, file_name))","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"文件操作","slug":"文件操作","permalink":"https://shang.at/tags/文件操作/"}]},{"title":"操作系统-网络的配置","slug":"操作系统-网络的配置","date":"2020-07-15T14:24:58.000Z","updated":"2020-07-15T14:39:06.169Z","comments":true,"path":"post/操作系统-网络的配置/","link":"","permalink":"https://shang.at/post/操作系统-网络的配置/","excerpt":"简介：本例中以CentOS 7举例说明如何设置Linux开机自动获取IP地址和设置固定IP地址。","text":"简介：本例中以CentOS 7举例说明如何设置Linux开机自动获取IP地址和设置固定IP地址。 自动获取动态IP地址1.输入“ip addr”并按回车键确定，发现无法获取IP(CentOS 7默认没有ifconfig命令)，记录下网卡名称（本例中为ens33）。 2.输入“cd /etc/sysconfig/network-scripts/”按回车键确定，继续输入“ls”按回车键查看文件。 3.输入“vi ifcfg-ens33”并按回车键确定（网卡名称可能不同）。亦可在第二步直接输入“cd /etc/sysconfig/network-scripts/ifcfg-ens33”直接编辑文件。 4.查看最后一项（蓝色框内），发现为“ONBOOT=no”。 5.按“i”键进入编辑状态，将最后一行“no”修改为“yes”，然后按“ESC”键退出编辑状态，并输入“:x”保存退出。 6.输入“service network restart”重启服务,亦可输入“systemctl restart netwrok”。 7.再次输入“ip addr”查看，现已可自动获取IP地址。 设置静态IP地址8.输入“cd /etc/sysconfig/network-scripts/”按回车键确定，继续输入“ls”按回车键查看文件，确定网卡名称。 9.输入“vi ifcfg-ens33”并按回车键确定（网卡名称可能不同）。如确知网卡名称可直接输入“cd /etc/sysconfig/network-scripts/ifcfg-ens33”编辑文件。 10.按“i”进入编辑状态，设置为“BOOTPROTO=’static’”（如设置为none则禁止DHCP，static则启用静态IP地址，设置为dhcp则为开启DHCP服务），并修改其他部分为您的设置， 本例中为192.168.1.200/24，GW:192.168.1.1，可根据您的需要配置IPV6部分。 注意：NM_CONTROLLED=no和ONBOOT=yes可根据您的需求进行设置。 11.确认无误后按“ESC”退出编辑状态，并输入“:x”保存退出，输入“service network restart”重启服务后输入“ifconfig”查看网络配置。 12.如需设置DNS(需在第9步设置NM_CONTROLLED=no)则输入“vi /etc/resolv.conf”并按回车键执行命令（如已在第9步配置DNS则可省略此步骤）。 13.在此文件里面输入DNS服务器地址（本例中以广东电信为例，亦可输入路由器DNS地址）并保存退出。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"}],"tags":[{"name":"网络的配置","slug":"网络的配置","permalink":"https://shang.at/tags/网络的配置/"}]},{"title":"工具使用-UML","slug":"工具使用-UML","date":"2020-07-15T03:36:56.000Z","updated":"2020-07-15T04:13:54.847Z","comments":true,"path":"post/工具使用-UML/","link":"","permalink":"https://shang.at/post/工具使用-UML/","excerpt":"简介：统一建模语言（Unified Modeling Language，缩写UML），是非专利的第三代建模和规约语言。UML是一种开放的方法，用于说明、可视化、构建和编写一个正在开发的、面向对象的、软件密集系统的制品的开放方法。","text":"简介：统一建模语言（Unified Modeling Language，缩写UML），是非专利的第三代建模和规约语言。UML是一种开放的方法，用于说明、可视化、构建和编写一个正在开发的、面向对象的、软件密集系统的制品的开放方法。 推荐使用https://app.lucidchart.com/在线工具进行绘制 UML模型和图形 UML分为模型和图形两大类。区分UML模型和UML图是非常重要的，UML图（包括用例图、协作图、活动图、序列图、部署图、构件图、类图、状态图）是模型中信息的图表表达形式，但是UML模型独立于UML图存在。 在UML系统开发中有三个主要的模型： 功能模型：从用户的角度展示系统的功能，包括用例图。 对象模型：采用对象，属性，操作，关联等概念展示系统的结构和基础，包括类别图、对象图。 动态模型：展现系统的内部行为。包括序列图，活动图，状态图。 UML2.2中一共定义了14种图示。 结构性图形（Structure diagrams）强调的是系统式的建模： 静态图（static diagram)：包括类图、对象图、包图 实现图（implementation diagram）：包括组件图、部署图 剖面图 复合结构图 行为式图形（Behavior diagrams）强调系统模型中触发的事件 活动图 状态图 用例图 交互性图形（Interaction diagrams），属于行为图形的子集合，强调系统模型中的资料流程 通信图 交互概述图 时序图 时间图 2 UML类图作用UML展现了一系列最佳工程实践，这些最佳实践在对大规模，复杂系统进行建模方面，特别是软件架构层次方面已经被验证有效。 我们这次介绍的主要是类图，为了解析项目的系统结构和架构层次，可以简洁明了的帮助我们理解项目中类之间的关系。 类图的作用： （1）：在软件工程中，类图是一种静态的结构图，描述了系统的类的集合，类的属性和类之间的关系，可以简化了人们对系统的理解； （2）：类图是系统分析和设计阶段的重要产物，是系统编码和测试的重要模型。 3 类图格式在UML类图中，类使用包含类名、属性(field) 和方法(method) 且带有分割线的矩形来表示， 举个栗子。一个Animal类，它包含name,age,state,isPet这4个属性，以及name相关方法。 1234567891011121314151617181920212223class Animal: NSObject &#123; public var name: String? internal var isPet: Bool? fileprivate var state: String? private var age: Int? = 0 override init() &#123; self.name = \"no name\" self.age = 0 self.isPet = true self.state = \"dead\" &#125; public func getName() -&gt; String &#123; return self.name! &#125; internal func setName(name: String?) &#123; self.name = name &#125;&#125;复制代码 对应UML类图： 类名：粗体，如果是类是抽象类则类名显示为斜体！ 属性： 可见性 名称：类型[=默认值] 可见性一般为public、private和protected，在类图分别用+、-和#表示，在Swift中没有与protected完全对应的可见控制，因此选用的是internal对应为#；名称为属性的名称；类型为数据类型；默认值如变量 age默认值为0。 方法： 可见性 名称（参数列表 参数1，参数2） ：返回类型 可见性如上名称表达式的介绍，名称就是方法名，参数列表是可选的项，多参数的话参数直接用英文逗号隔开；返回值也是个可选项，返回值类型可以说基本的数据类型、用户自定义类型和void。如果是构造方法，则无返回类型！ 4类与类之间的关系表达类图中类与类之间的关系主要由：继承、实现、依赖、关联、聚合、组合这六大类型。表示方式如下图： (1)继承关系（Generalization/extends）继承关系也叫泛化关系，指的是一个类（称为子类、子接口）继承另外的一个类（称为父类、父接口）的功能，并可以增加它自己的新功能的能力，继承是类与类或者接口与接口之间最常见的关系。 继承用实线空心箭头表示，由子类指向父类。 下面写两个子类，Fish和Cat分别继承自Animal。 1234567891011121314class Fish: Animal &#123; public var fishType: String? func swim() &#123; &#125;&#125;class Cat: Animal &#123; public var hasFeet: Bool? func playToy(doll:Doll) &#123; doll.toyMoved() &#125;&#125;复制代码 (2)实现关系（implements）指的是一个class类实现interface接口（可以是多个）的功能；实现是类与接口之间最常见的关系；在Java中此类关系通过关键字implements明确标识，在iOS中我将其理解成代理的实现。 写一个洋娃娃类Doll，该类遵循了ToyAction协议，实现了玩具移动的方法。 1234567891011121314protocol ToyAction &#123; func toyMoved() -&gt; Void&#125;class Doll: NSObject,ToyAction &#123; public var body: Body? public var cloth: Cloth? func toyMoved() &#123; //洋娃娃玩具动作具体实现 &#125;&#125;复制代码 (3)依赖关系（Dependency）可以简单的理解，就是一个类A使用到了另一个类B，而这种使用关系是具有偶然性的、、临时性的、非常弱的，但是B类的变化会影响到A；比如某人要过河，需要借用一条船，此时人与船之间的关系就是依赖；表现在代码层面，为类B作为参数被类A在某个method方法中使用。 在我们的上述代码中Cat的playToy方法中参数引用了Doll，因此他们是依赖关系。 (4)关联关系（Association）他体现的是两个类、或者类与接口之间语义级别的一种强依赖关系，比如我和我的朋友；这种关系比依赖更强、不存在依赖关系的偶然性、关系也不是临时性的，一般是长期性的，而且双方的关系一般是平等的、关联可以是单向、双向的；表现在代码层面，为被关联类B以类属性的形式出现在关联类A中，也可能是关联类A引用了一个类型为被关联类B的全局变量； 写一个Person类，他拥有一个宠物猫，他们之间的关系是关联。 123456789class Head: NSObject &#123; &#125;class Person: NSObject &#123; public var pet: Cat? public var head: Head?&#125;复制代码 (5)聚合关系（Aggregation）聚合是关联关系的一种特例，他体现的是整体与部分、拥有的关系，即has-a的关系，此时整体与部分之间是可分离的，他们可以具有各自的生命周期，部分可以属于多个整体对象，也可以为多个整体对象共享；比如计算机与CPU、公司与员工的关系等；表现在代码层面，和关联关系是一致的，只能从语义级别来区分； 12345678class Cloth: NSObject &#123; &#125;class Body: NSObject &#123; &#125;复制代码 在上述代码中Doll由Body和Cloth组成，且即使失去Cloth，Doll也可以正常存在。 (6)组合关系（Composition）组合也是关联关系的一种特例，他体现的是一种contains-a的关系，这种关系比聚合更强，也称为强聚合；他同样体现整体与部分间的关系，但此时整体与部分是不可分的，整体的生命周期结束也就意味着部分的生命周期结束；比如你和你的大脑；表现在代码层面，和关联关系是一致的，只能从语义级别来区分； 上述代码中的Person拥有Head，并且这个整体和部分是不可分割的。 最后来看看这个例子中的整体关系： 其实理解了之后我们发现还是很简单的，学会了之后就可以投入实践中了，举一个简单第三方库的类图例子，下图是Masonry的类图整理，可以看到项目结构很清晰的展示了出来。 12345转自作者：zhengyi链接：https://juejin.im/post/5d318b485188255957377ac3来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"UML","slug":"UML","permalink":"https://shang.at/tags/UML/"}]},{"title":"Java学习-设计模式","slug":"Java学习-设计模式","date":"2020-07-14T09:26:40.000Z","updated":"2020-07-21T07:13:09.334Z","comments":true,"path":"post/Java学习-设计模式/","link":"","permalink":"https://shang.at/post/Java学习-设计模式/","excerpt":"简介：Java中23种设计模式，分为创建型、结构型和行为型","text":"简介：Java中23种设计模式，分为创建型、结构型和行为型 创建型 对象实例化的模式，创建型模式用于解耦对象的实例化过程。 Singleton单例模式 某个类只能有一个实例，提供一个全局的访问点。 实现1 饿汉式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 饿汉式 * 类加载到内存后，就实例化一个单例，JVM保证线程安全 * 简单实用，推荐使用！ * 唯一缺点：不管用到与否，类装载时就完成实例化 * Class.forName(\"\") * （话说你不用的，你装载它干啥） */public class Mgr01 &#123; private static final Mgr01 INSTANCE = new Mgr01(); private Mgr01() &#123;&#125;; public static Mgr01 getInstance() &#123; return INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; Mgr01 m1 = Mgr01.getInstance(); Mgr01 m2 = Mgr01.getInstance(); System.out.println(m1 == m2); &#125;&#125;// 或者使用静态代码块进行初始化，和Mgr01一样public class Mgr02 &#123; private static final Mgr02 INSTANCE; static &#123; INSTANCE = new Mgr02(); &#125; private Mgr02() &#123;&#125;; public static Mgr02 getInstance() &#123; return INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; Mgr02 m1 = Mgr02.getInstance(); Mgr02 m2 = Mgr02.getInstance(); System.out.println(m1 == m2); &#125;&#125; 加载jdbc驱动的时候，就是用的Class.forName(“com.mysql.jdbc.Driver”)，在com.mysql.jdbc.Driver中就是在静态代码块中对com.mysql.jdbc.Driver进行的注册 实现2 懒汉式123456789101112131415161718192021222324252627282930313233343536373839404142/** * lazy loading * 也称懒汉式，需要时才加载 * 需要注意的点： * 1、DCL * 2、实例需要加volatile修饰 */public class Mgr06 &#123; private static volatile Mgr06 INSTANCE; //JIT private Mgr06() &#123; &#125; public static Mgr06 getInstance() &#123; if (INSTANCE == null) &#123; //双重检查 synchronized (Mgr06.class) &#123; if(INSTANCE == null) &#123; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; INSTANCE = new Mgr06(); &#125; &#125; &#125; return INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; for(int i=0; i&lt;100; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Mgr06.getInstance().hashCode()); &#125;).start(); &#125; &#125;&#125; 对于懒汉式的单例模式的详细解读：1、懒汉式一：synchronized加在getInstance()上，每次调用getInstance都会进行一次加锁行为，会导致效率低下2、懒汉式二：DCL模式，Double Check Lock，双重检查锁定​ a. 第一步检查，如果发现INSTANCE不为null，直接返回，不会产生加锁的行为，提高效率​ b. 第二步加锁，同时只能由一个线程获得锁，也只有一个线程能最终创建实例对象​ c. 第三步检查，加完锁之后再次检查，防止有多个线程在第一步检查时同时竞争锁，当一个线程完成了实例的创建，那么其他的线程再次获得锁时，发现实例已经不为空，那么退出临界区且返回实例对象3、INSTANCE对象需要使用volatile关键字修饰​ 貌似以上的操作已经没有什么问题了。但是这里还是有一个隐藏的bug：也就是CPU的乱序执行，指令重拍。由于CPU的乱序执行会导致实例对象的半初始化状态：即对象实例虽已创建，在内存中有对应的空间，但是这时候是还没有初始化实例属性，实例属性都是初始值。在java中，基础类型的变量都有各自的初始值，但是引用类型的初始值都是null，这个时候如果其他的线程拿到了实例对象，就会导致NullPointerException。所以INSTANCE对象需要增加volatile关键字修饰，valatile关键字可以禁用CPU的乱序执行。具体详情需要结合编译成的jvm指令集查看 实现3 静态内部类123456789101112131415161718192021222324252627282930/** * 静态内部类方式 * JVM保证单例 * 加载外部类时不会加载内部类，这样可以实现懒加载 */public class Mgr07 &#123; private Mgr07() &#123; &#125; private static class Mgr07Holder &#123; private final static Mgr07 INSTANCE = new Mgr07(); &#125; public static Mgr07 getInstance() &#123; return Mgr07Holder.INSTANCE; &#125; public void m() &#123; System.out.println(\"m\"); &#125; public static void main(String[] args) &#123; for(int i=0; i&lt;100; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Mgr07.getInstance().hashCode()); &#125;).start(); &#125; &#125;&#125; 实现4 枚举单例12345678910111213141516171819202122232425262728293031323334353637383940/** * 不仅可以解决线程同步，还可以防止反序列化。 * JVM 会阻止反射获取枚举类的私有构造方法，即可以 避免使用者使用反射的方式破坏单例的唯一性 */public enum Mgr08 &#123; INSTANCE; public void m() &#123;&#125; public static void main(String[] args) &#123; for(int i=0; i&lt;100; i++) &#123; new Thread(()-&gt;&#123; System.out.println(Mgr08.INSTANCE.hashCode()); &#125;).start(); &#125; &#125;&#125;/** * 内部隐藏的enum来生成单例对象 */public class SingletonObject7 &#123; private SingletonObject7()&#123;&#125; /** * 枚举类型是线程安全的，并且只会装载一次 */ private enum Singleton&#123; INSTANCE; private final SingletonObject7 instance; Singleton()&#123; instance = new SingletonObject7(); &#125; private SingletonObject7 getInstance()&#123; return instance; &#125; &#125; public static SingletonObject7 getInstance()&#123; return Singleton.INSTANCE.getInstance(); &#125;&#125; 但是不能设置属性呢？？？？？？ 如何防止用户破坏单例的唯一性？ 1、通过反射的方式破坏单例,反射是通过调用构造方法生成新的对象 2、如果单例类实现了序列化接口Serializable, 就可以通过反序列化破坏单例， 3、使用枚举单例 FactoryMethod工厂模式AbstractFactory抽象工厂模式Builder构建器Prototype原型结构性 把类或对象结合在一起形成一个更大的结构。 Facade门面 模式Decorator装饰器 模式Composite组合模式Flyweight享元Adapter适配器Bridge桥接Proxy静态代理与动态代理行为型 类和对象如何交互，及划分责任和算法。 Strategry策略模式ChainOfResponsibility责任链 模式Observer观察者 模式Mediator调停者 模式Iterator迭代器 模式Visitor访问者 模式Command命令 模式Memento备忘录 模式TemplateMethod模板方法State状态 模式Intepreter解释器 模式","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://shang.at/tags/设计模式/"}]},{"title":"虚拟机-Docker","slug":"虚拟机-Docker","date":"2020-07-13T23:36:10.000Z","updated":"2020-07-13T23:36:53.601Z","comments":true,"path":"post/虚拟机-Docker/","link":"","permalink":"https://shang.at/post/虚拟机-Docker/","excerpt":"简介：","text":"简介：","categories":[{"name":"虚拟机","slug":"虚拟机","permalink":"https://shang.at/categories/虚拟机/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://shang.at/tags/Docker/"}]},{"title":"操作系统-CentOS7上的开发环境配置","slug":"操作系统-CentOS7上的开发环境配置","date":"2020-07-13T14:02:29.000Z","updated":"2020-07-16T07:05:38.354Z","comments":true,"path":"post/操作系统-CentOS7上的开发环境配置/","link":"","permalink":"https://shang.at/post/操作系统-CentOS7上的开发环境配置/","excerpt":"简介：在Centos上进行开发的环境搭建","text":"简介：在Centos上进行开发的环境搭建 Linux Tools Quick Tutorial 安装必要的软件升级 yum源&amp;安装相关依赖包123yum updateyum install -y net-tools rsync mlocate wget vim ntpdate telnet gcc zlib-dev openssl-devel sqlite-devel bzip2-devel binutils qt make patch libgomp glibc-headers glibc-devel kernel-headers kernel-devel dkms strace 虚拟机软件Vagrant这里查看vagrant的详细使用 123wget https://releases.hashicorp.com/vagrant/2.2.9/vagrant_2.2.9_x86_64.rpmrpm -ivh vagrant_2.2.9_x86_64.rpm VirualBox1wget https://download.virtualbox.org/virtualbox/6.1.10/VirtualBox-6.1-6.1.10_138449_el7-1.x86_64.rpm rpm -ivh VirtualBox-6.1-6.1.10_138449_el7-1.x86_64.rpm 报错如下：libSDL-1.2.so.0()(64bit) 被 VirtualBox-6.1-6.1.10_138449_el7-1.x86_64 需要那么，需要安装SDL，yum install -y SDL 同时要保证安装的kernel-header和kernel的版本一致。安装完kernel之后，需要重启，可以使用uname -r 查看kernel版本 Docker这里查看Docker的详细使用 123456789101112131415161718192021222324252627282930# 安装工具包yum install -y yum-utils device-mapper-persistent-data lvm2# 安装ali源yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# 刷新安装源yum makecache fast# 安装 docker-ceyum -y install docker-ce# 启动dockersystemctl start docker# 停止dockersystemctl stop docker# 开机自启动systemctl enable docker# 配置aliyun的容器镜像加速器 # 登录aliyun.com 搜索容器镜像服务，进入管理后台，在左侧菜单栏下面可以找到 镜像加速器 选项sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; \"registry-mirrors\": [\"https://oooxxxoox.mirror.aliyuncs.com\"]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 实时监控网速1234567891011# 安装之前均需要安装epel源，yum -y install epel-release# 安装iftop工具yum install iftop -y# 查看开启的网卡ifconfig # 使用iftop监控，会进入一个类似top命令的界面，监控连接的地址以及网速iftop -i em4 systemctl命令详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107Systemctl是一个系统管理守护进程、工具和库的集合，用于取代System V、service和chkconfig命令，初始进程主要负责控制systemd系统和服务管理器。通过Systemctl –help可以看到该命令主要分为：查询或发送控制命令给systemd服务，管理单元服务的命令，服务文件的相关命令，任务、环境、快照相关命令，systemd服务的配置重载，系统开机关机相关的命令。 1. 列出所有可用单元 # systemctl list-unit-files2. 列出所有运行中单元 # systemctl list-units3. 列出所有失败单元 # systemctl –failed4. 检查某个单元（如 crond.service）是否启用 # systemctl is-enabledcrond.service 5. 列出所有服务 # systemctl list-unit-files –type=service6. Linux中如何启动、重启、停止、重载服务以及检查服务（如 httpd.service）状态 # systemctl start httpd.service# systemctl restart httpd.service# systemctl stop httpd.service# systemctl reload httpd.service# systemctl status httpd.service注意：当我们使用systemctl的start，restart，stop和reload命令时，终端不会输出任何内容，只有status命令可以打印输出。7. 如何激活服务并在开机时启用或禁用服务（即系统启动时自动启动mysql.service服务） # systemctl is-active mysql.service# systemctl enable mysql.service# systemctl disable mysql.service8. 如何屏蔽（让它不能启动）或显示服务（如ntpdate.service） # systemctl mask ntpdate.serviceln -s ‘/dev/null”/etc/systemd/system/ntpdate.service’# systemctl unmask ntpdate.servicerm ‘/etc/systemd/system/ntpdate.service’9. 使用systemctl命令杀死服务 # systemctl killcrond 10. 列出所有系统挂载点 # systemctl list-unit-files –type=mount11. 挂载、卸载、重新挂载、重载系统挂载点并检查系统中挂载点状态 # systemctl start tmp.mount# systemctl stop tmp.mount# systemctl restart tmp.mount# systemctl reload tmp.mount# systemctl status tmp.mount12. 在启动时激活、启用或禁用挂载点（系统启动时自动挂载） # systemctl is-active tmp.mount# systemctl enable tmp.mount# systemctl disable tmp.mount13. 在Linux中屏蔽（让它不能启用）或可见挂载点 # systemctl mask tmp.mountln -s ‘/dev/null”/etc/systemd/system/tmp.mount’# systemctl unmask tmp.mountrm ‘/etc/systemd/system/tmp.mount’14. 列出所有可用系统套接口 # systemctl list-unit-files –type=socket15. 检查某个服务的所有配置细节 # systemctl show mysql 16. 获取某个服务（httpd）的依赖性列表 # systemctl list-dependencies httpd.service17. 启动救援模式 # systemctl rescue18. 进入紧急模式 # systemctl emergency19. 列出当前使用的运行等级 # systemctl get-default20. 启动运行等级5，即图形模式 # systemctl isolate runlevel5.target或# systemctl isolate graphical.target21. 启动运行等级3，即多用户模式（命令行） # systemctl isolate runlevel3.target或# systemctl isolate multiuser.target22. 设置多用户模式或图形模式为默认运行等级 # systemctl set-default runlevel3.target# systemctl set-default runlevel5.target23. 重启、停止、挂起、休眠系统或使系统进入混合睡眠 # systemctl reboot# systemctl halt# systemctl suspend# systemctl hibernate# systemctl hybrid-sleep对于不知运行等级为何物的人，说明如下。Runlevel 0 : 关闭系统Runlevel 1 : 救援，维护模式Runlevel 3 : 多用户，无图形系统Runlevel 4 : 多用户，无图形系统Runlevel 5 : 多用户，图形化系统Runlevel 6 : 关闭并重启机器 Centos重启与关机 Linux centos重启命令： 1、reboot 2、shutdown -r now 立刻重启(root用户使用) 3、shutdown -r 10 过10分钟自动重启(root用户使用) 4、shutdown -r 20:35 在时间为20:35时候重启(root用户使用) 如果是通过shutdown命令设置重启的话，可以用shutdown -c命令取消重启 Linux centos关机命令： 1、halt 立刻关机 2、poweroff 立刻关机 3、shutdown -h now 立刻关机(root用户使用) 4、shutdown -h 10 10分钟后自动关机 如果是通过shutdown命令设置关机的话，可以用shutdown -c命令取消重启","categories":[],"tags":[{"name":"Centos开发环境","slug":"Centos开发环境","permalink":"https://shang.at/tags/Centos开发环境/"}]},{"title":"操作系统-CentOS7时区设置","slug":"操作系统-CentOS7时区设置","date":"2020-07-04T14:20:22.000Z","updated":"2020-07-04T14:20:44.981Z","comments":true,"path":"post/操作系统-CentOS7时区设置/","link":"","permalink":"https://shang.at/post/操作系统-CentOS7时区设置/","excerpt":"简介：","text":"简介： https://www.cnblogs.com/zhangeamon/p/5500744.html","categories":[],"tags":[{"name":"centos7时区设置","slug":"centos7时区设置","permalink":"https://shang.at/tags/centos7时区设置/"}]},{"title":"Linux-ssh","slug":"Linux-ssh","date":"2020-07-04T14:13:25.000Z","updated":"2020-07-04T14:13:59.375Z","comments":true,"path":"post/Linux-ssh/","link":"","permalink":"https://shang.at/post/Linux-ssh/","excerpt":"简介：","text":"简介： http://feihu.me/blog/2014/env-problem-when-ssh-executing-command-on-remote/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://shang.at/categories/Linux/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://shang.at/tags/ssh/"}]},{"title":"Python学习-deque","slug":"Python学习-deque","date":"2020-07-02T15:52:28.000Z","updated":"2020-07-02T15:52:52.037Z","comments":true,"path":"post/Python学习-deque/","link":"","permalink":"https://shang.at/post/Python学习-deque/","excerpt":"简介：","text":"简介： https://zhuanlan.zhihu.com/p/63502912","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"deque","slug":"deque","permalink":"https://shang.at/tags/deque/"}]},{"title":"数据结构与算法-单调栈和窗口及其更新结构","slug":"数据结构与算法-单调栈和窗口及其更新结构","date":"2020-07-02T11:07:20.000Z","updated":"2020-07-23T04:22:08.765Z","comments":true,"path":"post/数据结构与算法-单调栈和窗口及其更新结构/","link":"","permalink":"https://shang.at/post/数据结构与算法-单调栈和窗口及其更新结构/","excerpt":"简介：","text":"简介：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"}]},{"title":"Python学习-标准库学习","slug":"Python学习-标准库学习","date":"2020-07-01T09:17:07.000Z","updated":"2020-07-05T01:38:22.197Z","comments":true,"path":"post/Python学习-标准库学习/","link":"","permalink":"https://shang.at/post/Python学习-标准库学习/","excerpt":"简介：","text":"简介： https://docs.python.org/zh-cn/3/library/index.html","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"Python标准库","slug":"Python标准库","permalink":"https://shang.at/tags/Python标准库/"}]},{"title":"Python学习-itertools","slug":"Python学习-itertools","date":"2020-07-01T09:16:51.000Z","updated":"2020-07-05T01:37:04.811Z","comments":true,"path":"post/Python学习-itertools/","link":"","permalink":"https://shang.at/post/Python学习-itertools/","excerpt":"简介：","text":"简介： https://docs.python.org/zh-cn/3/library/itertools.html","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"itertools","slug":"itertools","permalink":"https://shang.at/tags/itertools/"}]},{"title":"Spark应用-大数据集的处理","slug":"Spark应用-大数据集的处理","date":"2020-07-01T09:12:03.000Z","updated":"2020-07-23T01:25:05.710Z","comments":true,"path":"post/Spark应用-大数据集的处理/","link":"","permalink":"https://shang.at/post/Spark应用-大数据集的处理/","excerpt":"简介：在spark上处理超大数据集的时候，有时候需要与外界系统进行交互或者需要将大数据量导出，下面讲解几种较方便的方案","text":"简介：在spark上处理超大数据集的时候，有时候需要与外界系统进行交互或者需要将大数据量导出，下面讲解几种较方便的方案 rdd.mapPartitions()业务场景：需要给大批量的数据进行解密操作，第一种解决方案是将数据load到driver端(4G内存)，然后进行解密，但是当数据量达到一定的量级，driver便会OOM了，所以有了第二种方案：使用rdd.mapPartitions()，在excutor端进行解密操作。基本原理是每个partition执行一次解密操作，每个partition的数据量基本不会导致 excutor OOM了。 遇到的问题： 1、对于rdd.mapPartitions() 接口内部的数据类型不熟悉 ​ rdd.mapPartitions()接受的函数对象func，在被调起的时候，会有一个默认的参数传递给它，是一个iterator对象，可以接成这一个partition的数据集合，每个元素都是一个Row类型的变量，可以使用asDict()函数将其转换成map对象来操作。 ​ 可以这样理解，实际上就是在远程的excutor上操作了一个集合，处理完后再将结果返回 2、因为调用解密接口的时候需要动态传递一个参数到解密服务，但是rdd.mapPartitions() 执行逻辑的时候实际上是在excutor端执行的，因此传递给rdd.mapPartitions() 的函数并不能接收参数 ​ rdd.mapPartitions()的参数是一个函数，在python中叫做高阶函数，如果熟悉java的话，那实际上类似于java8(及以后)中lambda表达式或者java中的函数回调。高阶函数可以接收函数对象作为参数，同时也可以返回一个函数对象。这里就利用高阶函数的这个特性，在func外层包装一个make_func()函数，用它来接收外部传来的参数，然后在func内部使用make_func接收的参数，这种特性叫做闭包，闭包还有一些其他的陷阱，请见这里 3、解密的字段仅仅是极个别的字段 ​ 解决方案：先将需要解密的字段加一个特殊的后缀(注意要避免其他的字段有相同的后缀)；然后在func中拿到有特定后缀的字段，将其取出，并且调用解密服务；最后将字段重新命名回来。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import copyimport jsonfrom pyspark.rdd import RDDfrom pyspark.sql import DataFramefrom pyspark.sql.types import Rowfrom asiacredit.conf.request import reqConffrom asiacredit.util.requeset.http_request import RequestKP_CHANNEL = 'XXXXXX'def make_func(channel): def func(iterator): request_obj = Request(url=reqConf.decryptConf.url, ak=reqConf.decryptConf.ak, sk=reqConf.decryptConf.sk) request_body = json.loads('&#123;\"encryptDataList\":\"\", \"decryptionChannel\":\"%s\"&#125;' % channel) _iterator = [item.asDict() for item in iterator] if len(_iterator) &lt;= 0: return [Row(**item) for item in _iterator] encrypted_columns = [k for k, v in _iterator[0].items() if '_encrypted' in k] for col in encrypted_columns: arr = [item[col] for item in _iterator if item[col] if arr: copy_request_body = copy.copy(request_body) copy_request_body['encryptDataList'] = arr request_obj.body = json.dumps(copy_request_body) encrypted_res = request_obj.request() col_res = &#123;line['encryptStr']: line['decryptStr'] for line in encrypted_res['decryptDataList']&#125; for i in range(len(_iterator)): if _iterator[i][col]: _iterator[i][col] = col_res[_iterator[i][col]] yield [Row(**item) for item in _iterator] return funcclass DecryptData: def __init__(self, encrypt_data: DataFrame, columns: list, decryption_channel=KP_CHANNEL, partition_num=0): \"\"\" 将解密操作放到集群节点上进行 如果不指定partition_num，会自动按照默认的分区数进行解密 :param encrypt_data: 要解密的DataFrame :param columns: 被加密的字段列表 :param decryption_channel: 加密的channel :param partition_num: 为1时，和第一版的接口效果一致，但是这里占用的是executor的资源，driver不会因为数据量过大二OOM executor的内存一般会比driver大很多，所以OOM的概率会降低 或者 这里在外层可以动态计算数据量设置一个合适的partition num \"\"\" self.columns = columns self.columns1 = [f'&#123;col&#125;_encrypted' for col in columns] self.source_columns = [col_name for col_name, _ in encrypt_data.dtypes] for old, new in zip(self.columns, self.columns1): encrypt_data = encrypt_data.withColumnRenamed(old, new) self.enough_data = encrypt_data.count() &gt; 0 self.encrypt_data = encrypt_data self.partition_num = partition_num if decryption_channel is None: raise AttributeError('decryption_channel can not be none') self.decryption_channel = decryption_channel def decrypt(self): if self.enough_data: encrypt_data_rdd: RDD = self.encrypt_data.rdd \\ if self.partition_num == 0 else self.encrypt_data.repartition(self.partition_num).rdd print('partition nums:' + str(encrypt_data_rdd.getNumPartitions())) res_df = encrypt_data_rdd.mapPartitions(make_func(self.decryption_channel)).flatMap( lambda line: line).toDF() else: print(\"there's no data to decrypt\") res_df = self.encrypt_data for old, new in zip(self.columns1, self.columns): res_df = res_df.withColumnRenamed(old, new) return res_df.select(*self.source_columns).distinct() toLocalIetator()https://www.waitingforcode.com/apache-spark/collecting-part-data-driver-rdd-tolocaiIterator/read toLocalIetator()的工作原理类似于python中的itertools.chain(*iterables)，类似于下面的方式： 12345def chain(*iterables): # chain('ABC', 'DEF') --&gt; A B C D E F for it in iterables: for element in it: yield element 它是将所有的partition形成一个类似于Ietator的形式，使用的时候，spark会将数据数据一部分一部分的从excutor端收集到driver端(从sparkui上也可以看出来)，并不会一次性将所有partition的数据全量收集到driver端，导致driver端的OOM，这也是一种导出大量数据时候的选择，但是数据需要经历一个从excutor向driver传输的过程，这个过程是比较费时的。一个使用案例如下： 123456789101112131415161718192021222324252627282930313233import shutildef _mkdir(base_dir, filename, delete=False): path = os.path.join(base_dir, filename) if os.path.exists(path) and delete: shutil.rmtree(path, True) if not os.path.exists(path): os.makedirs(path) return pathimport csvdef to_csv(df:DataFrame, filename, threshold=1000000, base_dir='result/'): row_num = 0 index = 0 csvwrite = None csvfile = None first = True for val in df.toLocalIterator():# print(type(val.asDict()))# print(val.asDict())# print([v for k,v in val.asDict().items()]) if first or row_num&gt;=threshold: print(filename + ' write ' + str(index) + '...' + str(threshold*index)) file_path = _mkdir(base_dir, filename, first) csvfile = open(os.path.join(file_path, filename+'_%04d.csv' % index), 'w', buffering=4096) csvwrite = csv.writer(csvfile) index += 1 row_num = 0 first = False csvwrite.writerow(val) csvfile.flush() row_num += 1 重分区写入集群，然后下载这种方案是：直接在集群内部设置适当的分区数，然后将数据写入集群内，最后将生成的文件下载下来。这种方式在数据量十分大的情况下，repartition的过程也是很费时的。 so，注意尽量不要使用repartition(numPartition, *col)，而是选择使用coalesce(numPartition)。这二者有很大区别：repartition会导致shuffle，会导致集群内部大量的数据传输，非常耗费时间；如果仅仅是将大量的partition合并成较少量的partition，可以直接使用coalesce，coalesce的原理是将多个partition直接合并成一个partition，是一个窄依赖，不会发生suffle，如果传入的参数大于当前的分区数，那么分区数不会发生变化。","categories":[{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/categories/Spark应用/"}],"tags":[{"name":"Spark大数据集防止driver OOM","slug":"Spark大数据集防止driver-OOM","permalink":"https://shang.at/tags/Spark大数据集防止driver-OOM/"}]},{"title":"Python学习-类的特殊方法","slug":"Python学习-类的特殊方法","date":"2020-07-01T02:22:24.000Z","updated":"2020-07-01T06:03:54.974Z","comments":true,"path":"post/Python学习-类的特殊方法/","link":"","permalink":"https://shang.at/post/Python学习-类的特殊方法/","excerpt":"简介：通过定义具有特殊名称的方法，类可以实现某些通过特殊语法调用的操作（例如算术运算或下标和切片）。这是Python运算符重载的方法，它允许类针对语言运算符定义自己的行为。例如，如果一个类定义了一个名为__getitem__()的方法，并且x是该类的实例，则x[i]大致等效于type(x).__getitem__(x, i)。除非另有说明，否则当未定义适当的方法（通常为AttributeError或TypeError）时，尝试执行操作会引发异常。 将特殊方法设置为None表示相应的操作不可用。例如，如果一个类将__iter__()设置为None，则该类不可迭代，因此在其实例上调用iter()会引发TypeError（而不会回到__getitem__()）。 默认情况下，我们自定义的类，它的这些特殊方法都有默认的实现。","text":"简介：通过定义具有特殊名称的方法，类可以实现某些通过特殊语法调用的操作（例如算术运算或下标和切片）。这是Python运算符重载的方法，它允许类针对语言运算符定义自己的行为。例如，如果一个类定义了一个名为__getitem__()的方法，并且x是该类的实例，则x[i]大致等效于type(x).__getitem__(x, i)。除非另有说明，否则当未定义适当的方法（通常为AttributeError或TypeError）时，尝试执行操作会引发异常。 将特殊方法设置为None表示相应的操作不可用。例如，如果一个类将__iter__()设置为None，则该类不可迭代，因此在其实例上调用iter()会引发TypeError（而不会回到__getitem__()）。 默认情况下，我们自定义的类，它的这些特殊方法都有默认的实现。 参考官网 https://docs.python.org/zh-cn/3/reference/datamodel.html Basic customization__new____init____del____repr__Customizing attribute accessCustomizing module attribute accessImplementing DescriptorsCustomizing class creationMetaclassesCustomizing instance and subclass checksEmulating generic typesEmulating callable objectsEmulating container typesEmulating numeric typesWith Statement Context ManagersSpecial method lookup","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"Python类的特殊方法","slug":"Python类的特殊方法","permalink":"https://shang.at/tags/Python类的特殊方法/"}]},{"title":"Java学习-JMH","slug":"Java学习-JMH","date":"2020-06-30T15:13:41.000Z","updated":"2020-07-05T01:38:55.632Z","comments":true,"path":"post/Java学习-JMH/","link":"","permalink":"https://shang.at/post/Java学习-JMH/","excerpt":"简介：本节学习Java准测试工具套件-JMH","text":"简介：本节学习Java准测试工具套件-JMH 官网：http://openjdk.java.net/projects/code-tools/jmh/ 官方样例：http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/ 博客：https://www.xncoding.com/2018/01/07/java/jmh.html","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA-JMH","slug":"JAVA-JMH","permalink":"https://shang.at/tags/JAVA-JMH/"}]},{"title":"算法-动态规划","slug":"算法-动态规划","date":"2020-06-29T17:54:22.000Z","updated":"2020-07-13T23:48:37.398Z","comments":true,"path":"post/算法-动态规划/","link":"","permalink":"https://shang.at/post/算法-动态规划/","excerpt":"简介：动态规划是高级的算法思想，本节主要记录DP的分析思路","text":"简介：动态规划是高级的算法思想，本节主要记录DP的分析思路 动态规划 其本质是动态递推 避免人肉递归。可以尝试画出递归树 找到最近最简方法，将其拆解成可重复解决的问题 如何找到最近最简方法：数学归纳法思维 如何区分DP问题：DP一般会被用来求解最值问题 DP与递归和分治的联系 DP与 递归或者分治没有根本上的却别 共性：找到重复子问题 差异性：DP有最优子结构，中途可以淘汰次优解 实战例题509. 斐波那契数 傻递归 - 自顶向下 画出递归树 时间复杂度：每计算一个节点，需要计算其余的两个节点，以此类推下去，这是一个二叉树的结构，所以它的时间复杂度是O($2^n$)的 优化： 加入备忘录 在傻递归的基础上，加入一个缓存，以达到避免重复子问题的重复计算的问题 使用DP的思维解决 - 自底向上 DP三步曲 找到重复子问题 根据数学归纳法：要计算第n个斐波那契数，那么我们只需要计算第n-1和n-2个斐波那契数就可以了 状态定义 - 且找到base case 假设使用a[i]表示第i个斐波那契数，那么f[i] = f[i-1]+f[i-2] 且f[0]=0,f[1]=1 DP方程 f(n)=\\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ 1 &,& n=1 \\\\ f(n-1)+f(n-2) &,& n>1 \\end{aligned} \\right. 70. 爬楼梯 考虑下变体： 不止可以上1阶或2阶：可以上1、2、3阶等 可以上1 2 3阶，且相邻的两个步伐不能相同，该如何设计 本体： f(n)=\\left\\{ \\begin{aligned} 1 &,& n=1 \\\\ 2 &,& n=2 \\\\ f(n-1)+f(n-2) &,& n>2 \\end{aligned} \\right.变体1： f(n)=\\left\\{ \\begin{aligned} 1 &,& n=1 \\\\ 2 &,& n=2 \\\\ 4 &,& n=3 \\\\ f(n-1)+f(n-2)+f(n-3) &,& n>3 \\end{aligned} \\right.变体2： 定义dp[0…2][i]， ​ dp[0][i]表示到达i最后一次走了1步； ​ dp[1][i]表示到达i最后一次走了2步； ​ dp[2][i]表示到达i最后一次走了3步 那么 ​ dp[0][i]=dp[1][i-1]+dp[2][i-1]；到当前台阶走了1步，那么前面只能再选最后一次走了2步和3步的 ​ dp[1][i]=dp[0][i-2]+dp[2][i-2]；到当前台阶走了2步，那么前面只能再选最后一次走了1步和3步 ​ dp[2][i]=dp[0][i-3]+dp[1][i-3] 到当前台阶走了3步，那么前面只能再选最后一次走了1步和2步的 结果为：dp[0][-1]+dp[1][-1]+dp[2][-1]，即到达n的所有步伐的总和 1234567891011121314151617181920212223def changeClimbStairs(self, n: int) -&gt; int: if n == 1: return 1 if n == 2: return 1 if n == 3: return 3 # dp[0][i] 最后一次走了1步到达 i # dp[1][i] 最后一次走了2步到达 i # dp[2][i] 最后一次走了3步到达 i dp = [ [1, 0, 1] + [0] * (n - 3), # 最后一次走1步 [0, 1, 1] + [0] * (n - 3), # 最后一次走2步 [0, 0, 1] + [0] * (n - 3) # 最后一次走3步 ] for i in range(3, n): # 到当前台阶走了1步，那么前面只能再选最后一次走了2步和3步的 dp[0][i] = dp[1][i - 1] + dp[2][i - 1] # 到当前台阶走了2步，那么前面只能再选最后一次走了1步和3步的 dp[1][i] = dp[0][i - 2] + dp[2][i - 2] # 到当前台阶走了3步，那么前面只能再选最后一次走了1步和2步的 dp[2][i] = dp[0][i - 3] + dp[1][i - 3] return dp[0][-1] + dp[1][-1] + dp[2][-1] 322. 零钱兑换 略过递归方案，直接上DP DP问题三步曲 找到重复子问题 根据数学归纳法：已知某金额所需的最少硬币数，可得其他金额的所需最少硬币数 状态定义 假设dp[i]表示，金额为i时需要的最少硬币数，那么因为有coins个硬币可选，所以dp[i]=min(dp[i-k] for k in coins) + 1，加1表示要选择一个面值为k的硬币 dp初始值为mount+1，因为金额为amout，所需硬币最多为amout个1元硬币，长度为amout+1 dp[0]=0，表示当金额为0时，需要的硬币数也为0 最终结果：dp[amout] DP方程 f(n) = \\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ 1 &,& n=1 \\\\ min(f(n-k))+1 &,& k \\in coins \\\\ \\end{aligned} \\right.123456789101112class Solution: def coinChange(self, coins: List[int], amount: int) -&gt; int: dp = [amount + 1] * (amount + 1) dp[0] = 0 for i in range(1, amount + 1): for coin in coins: if coin &lt;= i: dp[i] = min(dp[i], dp[i - coin] + 1) return dp[amount] if dp[amount] &lt; amount + 1 else -1 62. 不同路径 直接DP三步曲 找到重复子问题 要求从位置(i,j)到END的不同路径数，由于从某个位置出发只能向右或向下走，如果我知道了从(i+1, j)和从(i,j+1)到END的路径数，那么我就能得到path_{i,j}，path_{i,j}=path_{i+1,j}+path_{i,j+1} 状态定义 定义dp[m][n]数组，表示棋盘，每个元素表示从该位置出发，到END的不同路径数 dp[][n]=1，右边界全为1，因为右边界的位置到END，路径数都是1 dp[m][]=1，下边界全为1，因为下边界的位置到END，路径数都是1 遍历方向：从END位置开始向上遍历 最终结果：dp[0][0] DP方程 f(x,y) = \\left\\{ \\begin{aligned} 1 &,& x=m\\\\ 1 &,& y=n\\\\ f(x+1, y)+f(x, y+1) &,& 0 \\leq x < m \\& 0 \\leq y < n \\end{aligned} \\right.123456789101112131415161718192021class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: if n == 1: return 1 # 初始化dp数组，左边界和右边界都初始化为1 dp = [[1 if j == m - 1 or i == n - 1 else 0 for j in range(m)] for i in range(n)] for i in range(n - 2, -1, -1): for j in range(m - 2, -1, -1): dp[i][j] = dp[i + 1][j] + dp[i][j + 1] return dp[0][0] # 空间压缩：我们只需要一维的数组就可以存下路径的变化 def uniquePaths(self, m: int, n: int) -&gt; int: if n == 1: return 1 # 初始化dp数组为 1 dp = [1] * m for i in range(n - 2, -1, -1): for j in range(m - 2, -1, -1): dp[j] = dp[j] + dp[j + 1] return dp[0] 63. 不同路径 II 直接DP三步曲 找到重复子问题 要求从位置(i,j)到END的不同路径数，由于从某个位置出发只能向右或向下走，如果我知道了从(i+1, j)和从(i,j+1)到END的路径数，那么我就能得到path_{i,j}，path_{i,j}=path_{i+1,j}+path_{i,j+1} 状态定义 定义dp[m][n]数组，表示棋盘，每个元素表示从该位置出发，到END的不同路径数 dp[][n]=1，右边界最后一个障碍物之后全为1，最后一个障碍物及之前的位置全为0，因为右边界一旦出现了障碍物，那么在这之前的位置的路径就都被截断了 dp[m][]=1，下边界最后一个障碍物之后全为1，最后一个障碍物及之前的位置全为0，因为下边界一旦出现了障碍物，那么在这之前的位置的路径就都被截断了 需要注意的是：棋盘上为1的位置不能走，那个位置的路径数为0 遍历方向：从END位置开始遍历 最终结果：dp[0][0] DP方程 f(x,y) = \\left\\{ \\begin{aligned} 0 &,& x=m\\&x\\leq lastcol(lastrow表示下边界最后一个障碍物的位置)\\\\ 0 &,& y=n\\&y\\leq lastrow(lastrow表示右边界最后一个障碍物的位置)\\\\ 1 &,& x=m\\&x>lastcol(lastrow表示下边界最后一个障碍物的位置)\\\\ 1 &,& y=n\\&y>lastrow(lastrow表示右边界最后一个障碍物的位置)\\\\ 0 &,& obstacleGrid(x, y)=1\\\\ f(x-1, y)+f(x, y-1) &,& 0 \\leq x < m \\& 0 \\leq y < n \\end{aligned} \\right.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687class Solution: def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: m, n = len(obstacleGrid[0]), len(obstacleGrid) # if obstacleGrid[-1][-1] == 1 or obstacleGrid[0][0] == 1: return 0 # 起点和终点为1，直接返回0 # if n == 1: # 只有一行 # if sum(obstacleGrid[0]) &gt;= 1: # 行出现了1 # return 0 # else: # return 1 # if m == 1: # 只有一列 # if sum(list(chain(*obstacleGrid))) &gt;= 1: # 列出现了1 # return 0 # else: # return 1 # 如果 最后一行或最后一列出现了1，那么1出现的位置及之前的位置和1没有区别 last_row, last_col = -1, -1 # 标记最后一行(列)出现障碍的位置 for i in range(n - 1, -1, -1): if obstacleGrid[i][-1] == 1: last_row = i break for i in range(m - 1, -1, -1): if obstacleGrid[-1][i] == 1: last_col = i break # 根据棋盘的最后一行和最后一列初始化 dp数组 # 包含了 起点和终点为1 的情况 # 包含了 只有一行和只有一列的情况 dp = [[1 if i &gt; last_col and j == n - 1 or j &gt; last_row and i == m - 1 else 0 for i in range(m)] for j in range(n)] for i in range(n - 2, -1, -1): for j in range(m - 2, -1, -1): if obstacleGrid[i][j] == 1: dp[i][j] = 0 else: dp[i][j] = dp[i + 1][j] + dp[i][j + 1] return dp[0][0] def uniquePathsWithObstacles1(self, obstacleGrid: List[List[int]]) -&gt; int: if not obstacleGrid or not obstacleGrid[0]: return 0 m, n = len(obstacleGrid), len(obstacleGrid[0]) # 状态定义 - base case # dp[i][j] 表示从i,j到达finish位置的路径数 # 初始化为0 dp = [[0] * n for _ in range(m)] # 特殊考虑右边界和下边界:找到右边界和下边界最后一个障碍物的位置 # 从后往前找，不用从前往后 全部遍历 last_col, last_row = -1, -1 # 遍历状态 for i in range(m-1, -1, -1): for j in range(n-1, -1, -1): if obstacleGrid[i][j] == 1: # 遇到障碍物 dp[i][j] = 0 # 在遍历状态的过程中记录 右边界和下边界 最后一个障碍物的位置 last_col = max(last_col, j) if i==m-1 else last_col last_row = max(last_row, i) if j==n-1 else last_row else: # 正常的格子 if i&lt;m-1 and j&lt;n-1: # 不在右边界和下边界 dp[i][j] = dp[i+1][j] + dp[i][j+1] elif i==m-1: # 下边界:如果当前位置是在下边界的最后一个障碍物之后，那么赋值1，否则赋值0 dp[i][j] = 1 if j&gt;last_col else 0 elif j==n-1: # 右边界:如果当前位置是在右边界的最后一个障碍物之后，那么赋值1，否则赋值0 dp[i][j] = 1 if i&gt;last_row else 0 # 答案为dp[0][0] # print(last_col, last_row) return dp[0][0] # dp空间压缩：实际上我们在进行递推的时候，只需要一维数组就可以 # 为什么能做到这样的空间压缩？ # 当开始遍历上一层(i--)，dp[j]的值是保留了下一层的值，所以dp[j] += dp[j+1]是相当于之前的 dp[i][j] = dp[i+1][j] + dp[i][j+1] ~~~！！！！ # 同时也可以避免右边界和下边界的 特殊情况，因为一维的情况下 相当于是已经自动加了限制 def uniquePathsWithObstacles(self, obstacleGrid: List[List[int]]) -&gt; int: m, n = len(obstacleGrid[0]), len(obstacleGrid) dp = [0] * m dp[m - 1] = 1 for i in range(n - 1, -1, -1): for j in range(m - 1, -1, -1): if obstacleGrid[i][j] == 1: # 当前位置为障碍物 dp[j] = 0 elif j &lt; m - 1: # 因为要使用下一个坐标，这里要检测坐标的合法性，防止指针越界 dp[j] += dp[j + 1] return dp[0] 1143. 最长公共子序列 思维：DP最终会归结到一个状态数组中，所以拿到一个这样的题目后，就往一维状态数组上靠拢，一维搞不定，就尝试用二维数组，再不行就三维…然后依靠数学归纳法进行推导，看是否能根据已知内容的位置 推导出当前位置的值 直接上DP三步曲 找到重复子问题 abcde和abc的问题可以由下列的二维表格描述，如果要求某个位置的值，只需要知道它之前的某些位置即可 - a b c d e a LCS(a,a)=1 LCS(a,ab)=1 LCS(a,abc)=1 LCS(a,aabcd)=1 LCS(a,abcde)=1 c LCS(ac,a)=1 LCS(ac,ab)=1 LCS(ac,abc)=2 2 2 e 1 1 2 2 LCS(ace,abcde)=3 定义状态 dp[i][j]，i表示第一个字符串的位置编号，j表示第二个字符串的位置编号，整体表示两个子串的最长公共子序列的长度 dp长度初始化为text1.length+1*text2.length+1，因为至少要包含没有字符的情况 初始值：dp[0][0]=0 遍历方向：从前往后。因为base case在前部 最终结果：dp[text1.length][text2.length] DP方程 f(x, y) = \\left\\{ \\begin{aligned} 0 &,& x=0\\&y=0 \\\\ max(f(x-1, y), f(x, y-1)) &,& text1(x)!=text2(y) \\\\ f(x-1, y-1)+1 &,& text1(x)=text2(y) \\\\ \\end{aligned} \\right.1234567891011121314151617181920212223242526272829303132333435class Solution: def longestCommonSubsequence1(self, text1: str, text2: str) -&gt; int: m, n = len(text1), len(text2) dp = [[0 for _ in range(m + 1)] for _ in range(n + 1)] for i in range(1, n + 1): for j in range(1, m + 1): if text1[j - 1] == text2[i - 1]: # 当前位置的两个字符一样，取对角的值再加1(加上自身) dp[i][j] = dp[i - 1][j - 1] + 1 else: # 当前位置的两个字符不一样，取两个字符对应的最大值 dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) return dp[n][m] # dp空间压缩：发现求当前位置的值，只需要left和last_line的值 def longestCommonSubsequence(self, text1: str, text2: str) -&gt; int: m, n = len(text1), len(text2) # 当前位置，当前位置的左一位置 curr, left = 0, 0 # 上一行的值 last_line = [0] * (m + 1) for i in range(1, n + 1): for j in range(1, m + 1): if text1[j - 1] == text2[i - 1]: # 当前位置的两个字符一样，取对角的值再加1(加上自身) curr = last_line[j - 1] + 1 else: # 当前位置的两个字符不一样，取两个字符对应的最大值 curr = max(last_line[j], left) # 根据当前计算的结果 更新上一行，遍历到下一行的时候，正好可以用 if j == m: # 换行：可以更新当前位置的up位置元素，且把left重置为0 last_line[j - 1], last_line[j], left = left, curr, 0 else: # 不换行：不能更新up位置，因为在同一行中遍历的时候，要使用原始的值 last_line[j - 1], left = left, curr return curr 120. 三角形最小路径和 直接DP三步曲 找到重复子问题 要想求得到达当前位置的最小路径和，那么我只需要知道到达当前位置的up和up_left最小路径和即可 状态定义 dp[i][j]，i和j分别表示矩阵的坐标，整体表示到达该坐标的最小路径和 dp[0][0]为矩阵的第一个元素值 最终结果：min(dp[-1]) DP方程 f(x, y) = \\left\\{ \\begin{aligned} matrix(0, 0) &, & x=0\\&y=0 \\\\ min(f(x-1, y), f(x-1, y-1)) + matrix(x,y) &,& 0\\leq x \\& 0\\leq y \\end{aligned} \\right.123456789101112131415161718192021222324252627class Solution: def minimumTotal1(self, triangle: List[List[int]]) -&gt; int: m, n = len(triangle[-1]), len(triangle) dp = [[0 for _ in row] for row in triangle] dp[0][0] = triangle[0][0] for i in range(1, n): for j, val in enumerate(triangle[i]): if j == 0: dp[i][j] = dp[i - 1][j] + val elif j == len(triangle[i]) - 1: dp[i][j] = dp[i - 1][j - 1] + val else: dp[i][j] = min(dp[i - 1][j], dp[i - 1][j - 1]) + val return min(dp[-1]) # dp空间压缩 def minimumTotal(self, triangle: List[List[int]]) -&gt; int: m, n = len(triangle[-1]), len(triangle) dp = triangle for i in range(1, n): for j, val in enumerate(triangle[i]): if j == 0: dp[i][j] = dp[i - 1][j] + val elif j == len(triangle[i]) - 1: dp[i][j] = dp[i - 1][j - 1] + val else: dp[i][j] = min(dp[i - 1][j], dp[i - 1][j - 1]) + val return min(dp[-1]) 221. 最大正方形 DP三步曲 找到重复子问题 以某个位置(i,j)为右下角的正方形的边长，可以根据当前位置的up、left、left_up三个位置确定出来 状态定义 dp[i][j]表示以(i,j)为右下角的正方形的边长，那么如果(i,j)为0，dp[i][j]=0；否则dp[i][j]=min(up,left,up_left)+1 加1的目的是，至少包含它本身 DP方程 123456789101112131415class Solution: def maximalSquare(self, matrix: List[List[str]]) -&gt; int: if not matrix or not matrix[0]: return 0 m, n = len(matrix[0]), len(matrix) dp = [[0 for _ in range(m)] for _ in range(n)] max_edge = dp[0][0] = int(matrix[0][0]) for i in range(0, n): for j in range(0, m): if i == 0 or j == 0: dp[i][j] = 0 if matrix[i][j] == '0' else 1 else: dp[i][j] = 0 if matrix[i][j] == '0' else min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1 max_edge = max(dp[i][j], max_edge) return max_edge * max_edge 312. 戳气球 DP三步曲 找到重复子问题 当只有一个气球时，只需要戳一次即可 当有两个气球时，也只需要戳一次 当有三个气球时，那么肯定要先戳中间那一个 当有四个气球时，那么就要 状态定义 气球的个数为n 假设dp[i][j]表示序号在(i,j)之间的获得的硬币最大值，那么我们需要通过遍历(i,j)之间的所有序号，求出dp[i][k]和dp[k][j]，因为dp[i][j]=dp[i][k]+dp[k][j]+k， 最终结果：dp[0][n+1] DP方程 198. 打家劫舍 略过递归方案，直接上DP DP问题三步曲 找到重复子问题 根据数学归纳法：如果已知小偷在第i-1个房间对应的最高金额，那么小偷在第i个房间的对应的最高金额有以下两种情况 小偷偷了第i-1房间：那么小偷势必不能偷第i个房间，那么这个时候小偷偷的金额应该继承上一个值 小偷没有偷第i-1房间：那么小偷一定要偷第i个房间(因为是求的最大值)，这个时候小偷偷的金额应该是上一个值加上第i个房间的金额 状态定义 那么我们怎么表示某一个房间偷与不偷呢？此时我们可以考虑在一维的基础之上加一维附加这个状态，用dp[i][0]表示没有偷第i个房间，dp[i][1]表示偷了第i个房间，nums[i]表示第i间房的金额，那么 计算没有偷i房的时候在i的最大金额：dp[i][0]=max(dp[i-1][0], dp[i-1][1]) 计算偷i房的时候在i的最大金额：dp[i][1]=dp[i-1][0]+nums[i] dp的初始值为全为0，长度为n+1， 第0个元素表示房间数为0时，能偷到的金额为0， 第n个元素表示到达最后一个房间，能偷到的金额为多少 遍历从1开始 最终的结果：max(dp[n][0], dp[n][1]) DP方程定义 f(n,0) =\\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ max(f(n-1, 0), f(n-1, 1)) &,& n>1 \\end{aligned} \\right. f(n,1) =\\left\\{ \\begin{aligned} 0 &,& n=0 \\\\ f(n-1, 0)+nums(n) &,& n>1 \\end{aligned} \\right. res = max(f(n, 0), f(n, 1))1234567891011class Solution: def rob(self, nums: List[int]) -&gt; int: n = len(nums) # 房间个数 dp = [[0 for _ in range(2)] for _ in range(n + 1)] for i in range(1, n + 1): dp[i][0] = max(dp[i - 1][0], dp[i - 1][1]) # 不偷第i间房 dp[i][1] = dp[i - 1][0] + nums[i - 1] # 偷第i间房 return max(dp[n][0], dp[n][1]) # 返回第i间房 偷与不偷的最大金额 DP问题三步曲-version2 第一个版本中，我们考虑的时候，并不知道小偷有没有偷某个房间，所以加了一个维度表示某个房间偷与不偷的 对应的最大金额 这个版本中，我们换个思路，可以肯定的是，小偷一定会偷其中一间房，也就是说，如果我们假定某间房必偷的话，那么我们最终的结果应该是所有结果的最大值 找到重复子问题 根据数学归纳法：由于小偷不能偷连续的两间房，那么我们如果要计算第i间房最大金额，该怎么计算呢？这个时候，前一间房要么偷了，要么没偷，如果前一间房没偷，那么看前两间房偷没偷，前两间房如果没偷，那就继续往前推 状态定义-DP数组含义及base case dp[i]表示当偷到第i间房时的最大金额，不管第i间房偷与不偷， dp[i]=max(dp[i-1], dp[i-2]+nums[i]) 表示：偷与不偷i-1的时候，在第i间房的最大金额 dp初始化全为0，长度为n+2 dp[0], dp[1]表示第1间房前1间、两间的金额都为0 遍历从2开始 最终结果：max(dp) DP方程 f(n) = \\left\\{ \\begin{aligned} 0 &,& n=-1 \\\\ 0 &,& n=0 \\\\ max(f(n-1), f(n-2)+nums(n)) &,& n>0 \\end{aligned} \\right.12345678910111213141516171819202122232425class Solution: def robnn(self, nums: List[int]) -&gt; int: n = len(nums) dp = [0] * (n + 2) for i in range(2, n + 2): dp[i] = max(dp[i - 1], dp[i - 2] + nums[i - 2]) return max(dp) # ====&gt; 优化，去掉最后的取最大值的函数，且降低空间复杂度 def rob(self, nums: List[int]) -&gt; int: n = len(nums) i_2, i_1, res = 0, 0, 0 # 前两间房的最大金额，前一间房的最大金额，当前房间的最大金额 for i in range(n): # dp[i] = max(dp[i - 1], dp[i - 2] + nums[i - 2]) # 更新偷到当前房间的最大金额 res = max(i_1, i_2 + nums[i]) # 上一个res变成前一间房 # 上一个i_1变成前两间房 i_1, i_2 = res, i_1 return res 44. 通配符匹配 DP三步曲 找到重复子问题 要想知道p[1…j]是否匹配s[1…i]，那么需要知道p[0…j-1]与s[0…i-1]是否匹配 状态定义-DP数组含义及base case dp[i][j]表示p[1…j]是否匹配s[1…i] dp[0][0]=True 如果p和s都是空字符串，那么两者是匹配的，直接返回True 如果s为空，p不为空且只包含*这一种字符，则两者匹配，直接返回True 如果p为空，s不为空，则两者不匹配，直接返回False 如果s为空，p不为空且不止包含*这一种字符，则两者不匹配，直接返回False 状态转移方程 dp[i][j] = \\left\\{ \\begin{aligned} dp[i-1][j-1] &,& s[i]=p[j] \\\\ dp[i-1][j] ∨ dp[i][j-1] &,& p[j]='*' \\\\ dp[i-1][j-1] &,& p[j]='?' \\end{aligned} \\right. 1234567891011121314151617181920class Solution: def isMatch(self, s: str, p: str) -&gt; bool: if not p and not s: return True if not p and s: return False if not s and ('*' not in set(p) or len(set(p))&gt;1): return False dp = [[False]*(len(p)+1) for _ in range(len(s)+1)] dp[0][0] = True for j in range(1, len(p)+1): if p[j-1]=='*': dp[0][j] = dp[0][j-1] else: break for i in range(1, len(s)+1): for j in range(1, len(p)+1): if s[i-1]==p[j-1] or p[j-1]=='?': dp[i][j] = dp[i-1][j-1] elif p[j-1]=='*': dp[i][j] = dp[i][j-1] | dp[i-1][j] print(dp) return dp[len(s)][len(p)]","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://shang.at/tags/动态规划/"}]},{"title":"Python学习-python3.6-dict有序且效率更高","slug":"Python学习-python3-6-dict有序且效率更高","date":"2020-06-28T23:22:42.000Z","updated":"2020-07-02T15:49:24.809Z","comments":true,"path":"post/Python学习-python3-6-dict有序且效率更高/","link":"","permalink":"https://shang.at/post/Python学习-python3-6-dict有序且效率更高/","excerpt":"","text":"https://www.cnblogs.com/xieqiankun/p/python_dict.html","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python的dict","slug":"python的dict","permalink":"https://shang.at/tags/python的dict/"}]},{"title":"Hadoop学习","slug":"Hadoop学习","date":"2020-06-26T16:34:48.000Z","updated":"2020-07-15T09:10:12.865Z","comments":true,"path":"post/Hadoop学习/","link":"","permalink":"https://shang.at/post/Hadoop学习/","excerpt":"简介：","text":"简介： 大数据 分布式数据存储 ​ 数据一致性 - CAP - poxes 主从 ​ 单点故障 - HA ​ 内存压力 - 分片管理 分布式计算 - 计算向数据移动 ​ mapreduce 数据以一条记录为单位及经过map方法映射成KV，相同的K为一组，这一组数据条用一次reduce方法，在方法内迭代计算一组数据。 迭代器模式，数据集一般是用迭代计算的方式 ​ 为什么要有split？split只是一个逻辑上的概念。 ​ 与数据物理存储上的block解耦(软件工程上：加一层解耦)，默认情况下，split等于block，但是也可以小于block，也可能大于block 集群内部的通信采用RPC的方式，同时只会发生client向server发送请求，server处理完之后返回client响应结果，sever不会主动向client发送消息","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://shang.at/tags/Hadoop/"}]},{"title":"Hadoop学习-Yarn-Scheduler","slug":"Hadoop学习-Yarn-Scheduler","date":"2020-06-25T06:24:45.000Z","updated":"2020-06-25T06:25:31.320Z","comments":true,"path":"post/Hadoop学习-Yarn-Scheduler/","link":"","permalink":"https://shang.at/post/Hadoop学习-Yarn-Scheduler/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Yarn-Scheduler","slug":"Yarn-Scheduler","permalink":"https://shang.at/tags/Yarn-Scheduler/"}]},{"title":"Hadoop学习-Yarn-Container","slug":"Hadoop学习-Yarn-Container","date":"2020-06-25T06:24:29.000Z","updated":"2020-06-25T06:25:23.223Z","comments":true,"path":"post/Hadoop学习-Yarn-Container/","link":"","permalink":"https://shang.at/post/Hadoop学习-Yarn-Container/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Yarn-Container","slug":"Yarn-Container","permalink":"https://shang.at/tags/Yarn-Container/"}]},{"title":"Hadoop学习-如何实现一个在Yarn上的Application","slug":"Hadoop学习-如何实现一个在Yarn上的Application","date":"2020-06-25T01:27:51.000Z","updated":"2020-06-25T01:28:09.068Z","comments":true,"path":"post/Hadoop学习-如何实现一个在Yarn上的Application/","link":"","permalink":"https://shang.at/post/Hadoop学习-如何实现一个在Yarn上的Application/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"实现一个在Yarn上的Application","slug":"实现一个在Yarn上的Application","permalink":"https://shang.at/tags/实现一个在Yarn上的Application/"}]},{"title":"Hadoop学习-Mapreduce编程模式","slug":"Hadoop学习-Mapreduce编程模式","date":"2020-06-25T01:26:50.000Z","updated":"2020-06-25T01:27:19.885Z","comments":true,"path":"post/Hadoop学习-Mapreduce编程模式/","link":"","permalink":"https://shang.at/post/Hadoop学习-Mapreduce编程模式/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Mapreduce编程模式","slug":"Mapreduce编程模式","permalink":"https://shang.at/tags/Mapreduce编程模式/"}]},{"title":"Hive学习-安装","slug":"Hive学习-安装","date":"2020-06-24T07:41:54.000Z","updated":"2020-07-04T14:15:46.359Z","comments":true,"path":"post/Hive学习-安装/","link":"","permalink":"https://shang.at/post/Hive学习-安装/","excerpt":"","text":"配置过程 mysql 安装 配置可以远程连接 jdbc driver hive使用的jdbc drive要与mysql的版本匹配 hive配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;!-- hive-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hostmachine:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;the URL of the MySQL database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive1234&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; &lt;value&gt;SchemaTable&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置数据存放在hdfs上的路径 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置metastore service 的节点 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node2:9083&lt;/value&gt; &lt;description&gt;IP address (or fully-qualified domain name) and port of the metastore host&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动过程 首先启动hdfs：start-dfs.sh 初始化metestore：schematool --dbType mysql --initSchema 启动metastore service：hive --service metastore 启动hive：hive https://www.jianshu.com/p/6108e0aed204","categories":[{"name":"Hive学习","slug":"Hive学习","permalink":"https://shang.at/categories/Hive学习/"}],"tags":[{"name":"Hive安装","slug":"Hive安装","permalink":"https://shang.at/tags/Hive安装/"}]},{"title":"数据库-mysql-环境配置","slug":"数据库-mysql-环境配置","date":"2020-06-24T06:16:33.000Z","updated":"2020-06-24T07:15:44.507Z","comments":true,"path":"post/数据库-mysql-环境配置/","link":"","permalink":"https://shang.at/post/数据库-mysql-环境配置/","excerpt":"","text":"platform：MAC 安装1234567891011121314brew istall mysqlWe've installed your MySQL database without a root password. To secure it run: mysql_secure_installationMySQL is configured to only allow connections from localhost by defaultTo connect run: mysql -urootTo have launchd start mysql now and restart at login: brew services start mysqlOr, if you don't want/need a background service you can just run: mysql.server start 新安装的mysql，需要重置密码： The initial root account may or may not have a password. Choose whichever of the following procedures applies: If the root account exists with an initial random password that has been expired, connect to the server as root using that password, then choose a new password. This is the case if the data directory was initialized using mysqld —initialize, either manually or using an installer that does not give you the option of specifying a password during the install operation. Because the password exists, you must use it to connect to the server. But because the password is expired, you cannot use the account for any purpose other than to choose a new password, until you do choose one. If you do not know the initial random password, look in the server error log. Connect to the server as root using the password: 12shell&gt; mysql -u root -p Enter password: (enter the random root password here) Choose a new password to replace the random password: mysql&gt; ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘root-password’; If the root account exists but has no password, connect to the server as root using no password, then assign a password. This is the case if you initialized the data directory using mysqld —initialize-insecure. Connect to the server as root using no password: 1shell&gt; mysql -u root --skip-password Assign a password: 1mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'root-password'; After assigning the root account a password, you must supply that password whenever you connect to the server using the account. For example, to connect to the server using the mysql client, use this command: 12shell&gt; mysql -u root -p Enter password: (enter root password here) To shut down the server with mysqladmin, use this command: 12shell&gt; mysqladmin -u root -p shutdown Enter password: (enter root password here) 设置远程访问设置my.cnf使用brew 安装的mysql，my.cnf文件在/usr/local/etc/my.cnf，修改bind-address为0.0.0.0，然后重启brew services restart mysql 创建用户，赋予权限1234567891011121314# 登录mysqlmysql -u root -p 123456# 创建新用户create user 'hive' identified by 'hive1234';# 授权grant all privileges on *.* to 'hive'@'%' with grant option;# *.* 前边的*号指的是数据库，后面的*号指的是表，*.*的意思就是任意数据库下的任意表# 'root'@'%'，'root'用户名，'%'任意的主机名。# 这条配置信息就是说，允许任意节点以root身份登录，并且可以访问mysql里的任意库下的任意表# 刷新flush privileges; 至此，便可以在其他host上访问mysql服务了。 其他问题 jar记得更新 server 时区","categories":[{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"}],"tags":[{"name":"mysql环境配置","slug":"mysql环境配置","permalink":"https://shang.at/tags/mysql环境配置/"}]},{"title":"Hadoop学习-Shell脚本学习","slug":"Hadoop学习-Shell脚本学习","date":"2020-06-24T05:28:09.000Z","updated":"2020-06-24T05:28:38.353Z","comments":true,"path":"post/Hadoop学习-Shell脚本学习/","link":"","permalink":"https://shang.at/post/Hadoop学习-Shell脚本学习/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop-Shell","slug":"Hadoop-Shell","permalink":"https://shang.at/tags/Hadoop-Shell/"}]},{"title":"工具使用-iterm2","slug":"工具使用-iterm2","date":"2020-06-23T10:30:55.000Z","updated":"2020-06-23T10:32:31.790Z","comments":true,"path":"post/工具使用-iterm2/","link":"","permalink":"https://shang.at/post/工具使用-iterm2/","excerpt":"","text":"标签12345新建标签：command + t关闭标签：command + w切换标签：command + 数字 command + 左右方向键切换全屏：command + enter查找：command + f 分屏12345垂直分屏：command + d水平分屏：command + shift + d切换屏幕：command + option + 方向键 command + [ 或 command + ]查看历史命令：command + ;查看剪贴板历史：command + shift + h 其他1234567891011121314151617181920212223清除当前行：ctrl + u到行首：ctrl + a到行尾：ctrl + e前进后退：ctrl + f/b (相当于左右方向键)上一条命令：ctrl + p搜索命令历史：ctrl + r删除当前光标的字符：ctrl + d删除光标之前的字符：ctrl + h删除光标之前的单词：ctrl + w删除到文本末尾：ctrl + k交换光标处文本：ctrl + t清屏1：command + r清屏2：ctrl + l自带有哪些很实用的功能/快捷键⌘ + 数字在各 tab 标签直接来回切换选择即复制 + 鼠标中键粘贴，这个很实用⌘ + f 所查找的内容会被自动复制⌘ + d 横着分屏 / ⌘ + shift + d 竖着分屏⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏ctrl + u 清空当前行，无论光标在什么位置输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令⌘ + shift + h 会列出剪切板历史可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现 常用的一些快捷键1234567891011121314⌘ + 1 / 2 左右 tab 之间来回切换，这个在 前面 已经介绍过了⌘← / ⌘→ 到一行命令最左边/最右边 ，这个功能同 C+a / C+e⌥← / ⌥→ 按单词前移/后移，相当与 C+f / C+b，其实这个功能在Iterm中已经预定义好了，⌥f / ⌥b，看个人习惯了好像就这几个设置方法如下当然除了这些可以自定义的也不能忘了 linux 下那些好用的组合C+a / C+e 这个几乎在哪都可以使用C+p / !! 上一条命令C+k 从光标处删至命令行尾 (本来 C+u 是删至命令行首，但iterm中是删掉整行)C+w A+d 从光标处删至字首/尾C+h C+d 删掉光标前后的自负C+y 粘贴至光标后C+r 搜索命令历史，这个较常用 选中即复制iterm2 有 2 种好用的选中即复制模式。 一种是用鼠标，在 iterm2 中，选中某个路径或者某个词汇，那么，iterm2 就自动复制了。 另一种是无鼠标模式，command+f,弹出 iterm2 的查找模式，输入要查找并复制的内容的前几个字母，确认找到的是自己的内容之后，输入 tab，查找窗口将自动变化内容，并将其复制。如果输入的是 shift+tab，则自动将查找内容的左边选中并复制。 自动完成输入打头几个字母，然后输入 command+; iterm2 将自动列出之前输入过的类似命令。 剪切历史输入 command+shift+h，iterm2 将自动列出剪切板的历史记录。如果需要将剪切板的历史记录保存到磁盘，在 Preferences &gt; General &gt; Save copy/paste history to disk 中设置。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"iterm2","slug":"iterm2","permalink":"https://shang.at/tags/iterm2/"}]},{"title":"Hadoop学习-源码编译","slug":"Hadoop学习-源码编译","date":"2020-06-23T10:18:16.000Z","updated":"2020-06-23T10:19:33.849Z","comments":true,"path":"post/Hadoop学习-源码编译/","link":"","permalink":"https://shang.at/post/Hadoop学习-源码编译/","excerpt":"","text":"","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop源码编译","slug":"Hadoop源码编译","permalink":"https://shang.at/tags/Hadoop源码编译/"}]},{"title":"Hadoop学习-集群搭建","slug":"Hadoop学习-集群搭建","date":"2020-06-23T10:17:38.000Z","updated":"2020-06-26T03:45:57.288Z","comments":true,"path":"post/Hadoop学习-集群搭建/","link":"","permalink":"https://shang.at/post/Hadoop学习-集群搭建/","excerpt":"","text":"配置12345678# hadoop.sh -&gt; /etc/profile.d/export HADOOP_PREFIX=/root/hadoopexport HADOOP_YARN_HOME=$&#123;HADOOP_PREFIX&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_PREFIX&#125;/etc/hadoopexport YARN_LOG_DIR=$&#123;HADOOP_YARN_HOME&#125;/logsexport YARN_IDENT_STRING=rootexport HADOOP_MAPRED_IDENT_STRING=rootexport PATH=$&#123;HADOOP_PREFIX&#125;/bin:$&#123;HADOOP_PREFIX&#125;/sbin:$&#123;PATH&#125; 12345678910111213&lt;!-- core-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/core-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node4:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- hdfs-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///root/data/hadoop/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!-- config secondary namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node3:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- enable webhdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- disable permissions; only for development, of course --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122&lt;!-- yarn-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-common/yarn-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- enable log aggregation, this is false by default --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt; &lt;value&gt;node4&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 123456789101112&lt;!-- mapred-site.xml -&gt; path_to_hadoop/etc/hadoop/ --&gt;&lt;!-- https://hadoop.apache.org/docs/r2.7.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node3:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动流程Step1. 第一次启动HDFS，需要格式化一下hdfs 1$HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster-name&gt; Step2. 启动 1234start-dfs.shstart-yarn.shyarn-daemon.sh start proxyservermr-jobhistory-daemon.sh start historyserver 停止流程1234stop-dfs.shstop-yarn.shyarn-daemon.sh stop proxyservermr-jobhistory-daemon.sh stop historyserver 访问Once the Hadoop cluster is up and running check the web-ui of the components as described below: Daemon Web Interface Notes NameNode http://nn_host:port/ Default HTTP port is 50070. SecondaryNameNode http://nn_host:port/ Default HTTP port is 50090. ResourceManager http://rm_host:port/ Default HTTP port is 8088. YarnWebProxy http://proxy_host:port/ no Default HTTP port. 需要自定义 MapReduce JobHistory Server http://jhs_host:port/ Default HTTP port is 19888.","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop集群搭建","slug":"Hadoop集群搭建","permalink":"https://shang.at/tags/Hadoop集群搭建/"}]},{"title":"Hadoop学习-配置详解","slug":"Hadoop学习-配置详解","date":"2020-06-23T09:12:52.000Z","updated":"2020-06-26T02:48:44.351Z","comments":true,"path":"post/Hadoop学习-配置详解/","link":"","permalink":"https://shang.at/post/Hadoop学习-配置详解/","excerpt":"","text":"版本：2.7.3 core-default.xml parameter default value notes fs.defaultFS file:/// 定义namenode的URI，改成hdfs://host:port/ hadoop.tmp.dir /tmp/hadoop-${user.name} 定义其他临时目录的根目录 io.file.buffer.size 4096 读写文件操作时的缓存字节数，必须是硬件上的内存页大小的整数倍 hdfs-default.xmlnamenode parameter default value note dfs.namenode.http(s)-address 0.0.0.0:50070(50470) 配置dfs的web ui界面，不建议修改。可以通过http://namenode_hostname:50070访问 dfs.namenode.name.dir file://${hadoop.tmp.dir}/dfs/name 配置DFS namenode的fsimage文件存放在本次文件系统的路径。如果配置了使用逗号分隔的多个路径，那么namemode会在每个目录下面都冗余的存放一份。 dfs.namenode.edits.dir dfs.namenode.name.dir 配置DFS namenode的edits文件存放在本次文件系统的路径。如果配置了使用逗号分隔的多个路径，那么namemode会在每个目录下面都冗余的存放一份。 dfs.namenode.fs-limits.min-block-size 1048576 1m 最小块大小（以字节为单位），由Namenode在创建时强制执行。这样可以防止意外创建具有很小块大小（因此有很多块）的文件，这会降低性能。减少数据块的数量， dfs.namenode.handler.count 10 namenode端服务的线程数，测试时可以配置的小一些，减少内存占用 datanode parameter default value notes dfs.datanode.data.dir file://${hadoop.tmp.dir}/dfs/data 文件块的在local filesystem中的存放路径。如果提供的是逗号分隔的目录列表，那么数据将会存储在所有的目录中，(通常目录列表是在不同的设备上)。目录应该被相应的存储类型所标记(HDFS上有四种存储设备：SSD、DISK、ARCHIVE、RAM_DISK)，如果没有指定，默认是DISK。如果目录不存在，那么会自动创建(需要获取目录权限) dfs.datanode.handler.count 10 namenode端服务的线程数，测试时可以配置的小一些，减少内存占用 secondary namenode parameter default value notes dfs.namenode.secondary.http(s)-address 0.0.0.0:50090(50091) 配置secondary namenode的http server和端口 dfs.namenode.checkpoint.dir file://${hadoop.tmp.dir}/dfs/namesecondary dfs.namenode.checkpoint.edits.dir ${dfs.namenode.checkpoint.dir} dfs.namenode.checkpoint.period 3600 dfs.namenode.checkpoint.txns 1000000 dfs.namenode.checkpoint.check.period 60 dfs.namenode.checkpoint.max-retries 3 dfs.namenode.num.checkpoints.retained 2 dfs parameter default value notes dfs.permissions.enabled true 配置是否启用权限检查，默认是启用的。测试时可以设置为false。当开启状态时，dfs不会检测文件的权限检测。HDFS PermissionHDFS默认启动namenode的user为superuser，这个 dfs.blocksize 134217728 128m 单位字节，新文件的block 大小 dfs.hosts / dfs.hosts.exclude List of permitted/excluded DataNodes.If necessary, use these files to control the list of allowable datanodes. dfs.replication 3 块副本数 dfs.webhdfs.enabled true 启动namenode和datanode上的WebHHDFS(REST API) mapred-default.xmlMapReduce Applications Parameter Value Notes mapreduce.framework.name yarn Execution framework set to Hadoop YARN. mapreduce.map.memory.mb 1536 Larger resource limit for maps. mapreduce.map.java.opts -Xmx1024M Larger heap-size for child jvms of maps. mapreduce.reduce.memory.mb 3072 Larger resource limit for reduces. mapreduce.reduce.java.opts -Xmx2560M Larger heap-size for child jvms of reduces. mapreduce.task.io.sort.mb 100 Higher memory-limit while sorting data for efficiency. mapreduce.task.io.sort.factor 10 More streams merged at once while sorting files. mapreduce.reduce.shuffle.parallelcopies 5 Higher number of parallel copies run by reduces to fetch outputs from very large number of maps. MapReduce JobHistory Server Parameter Value Notes mapreduce.jobhistory.address 0.0.0.0:10020 MapReduce JobHistory Server IPC host:port mapreduce.jobhistory.webapp.address 0.0.0.0:19888 MapReduce JobHistory Server Web UI host:port yarn.app.mapreduce.am.staging-dir /tmp/hadoop-yarn/staging The staging dir used while submitting jobs. mapreduce.jobhistory.intermediate-done-dir ${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate Directory where history files are written by MapReduce jobs. mapreduce.jobhistory.done-dir ${yarn.app.mapreduce.am.staging-dir}/history/done Directory where history files are managed by the MR JobHistory Server. yarn-default.xmlResourceManager and NodeManager Parameter default Value Notes yarn.acl.enable false 是否开启ACLs yarn.admin.acl * ACL to set admins on the cluster. ACLs are of for comma-separated-usersspacecomma-separated-groups. Defaults to special value of * which means anyone. Special value of just space means no one has access. yarn.log-aggregation-enable false 是否启动日志聚合。日志聚合会收集每个container的日志并且在应用完成后将他们移动到HDFS中。具体目录由下面两个选项配置yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix。用户可以通过Application Timeline Server访问这些日志文件 ResourceManager Parameter default Value Notes yarn.resourcemanager.address ${yarn.resourcemanager.hostname}:8032 配置RM的URIfor clients to submit jobs. yarn.resourcemanager.scheduler.address ${yarn.resourcemanager.hostname}:8030 资源调度器URIfor ApplicationMasters to talk to Scheduler to obtain resources. yarn.resourcemanager.resource-tracker.address ${yarn.resourcemanager.hostname}:8031 for NodeManagers. yarn.resourcemanager.admin.address ${yarn.resourcemanager.hostname}:8033 for administrative commands. yarn.resourcemanager.webapp.address ${yarn.resourcemanager.hostname}:8088 RM web application yarn.resourcemanager.hostname 0.0.0.0 ResourceManager host.应该改成特定的hostname yarn.web-proxy.address 默认没有配置，会作为RM的一部分运行 The address for the web proxy as HOST:PORT, if this is not given then the proxy will run as part of the RM yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.&lt;br /&gt;scheduler.capacity.CapacityScheduler 指定RM使用的调度器：CapacityScheduler (recommended), FairScheduler (also recommended), or FifoScheduler yarn.scheduler.minimum-allocation-mb 1024 In MBs，Minimum limit of memory to allocate to each container request at the ResourceManager. yarn.scheduler.maximum-allocation-mb 8192 In MBs，Maximum limit of memory to allocate to each container request at the Resource Manager. yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path List of permitted/excluded NodeManagers.If necessary, use these files to control the list of allowable NodeManagers. NodeManager Parameter default Value Notes yarn.nodemanager.resource.memory-mb 8192 Resource i.e. available physical memory, in MB, for given NodeManager.Defines total available resources on the NodeManager to be made available to running containers yarn.nodemanager.vmem-pmem-ratio 2.1Maximum ratio by which virtual memory usage of tasks may exceed physical memory The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio. yarn.nodemanager.local-dirs ${hadoop.tmp.dir}/nm-local-dirComma-separated list of paths on the local filesystem where intermediate data is written. Multiple paths help spread disk i/o. yarn.nodemanager.log-dirs ${yarn.log.dir}/userlogsComma-separated list of paths on the local filesystem where logs are written. Multiple paths help spread disk i/o. yarn.nodemanager.log.retain-seconds 10800 Default time (in seconds) to retain log files on the NodeManager. Only applicable if log-aggregation is disabled. yarn.nodemanager.remote-app-log-dir /tmp/logs HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled. yarn.nodemanager.remote-app-log-dir-suffix logs Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled. yarn.nodemanager.aux-services mapreduce_shuffle Shuffle service that needs to be set for Map Reduce applications. History Server (Needs to be moved elsewhere) Parameter default Value Notes yarn.log-aggregation.retain-seconds -1 How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node. yarn.log-aggregation.retain-check-interval-seconds -1 Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node.","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop配置详解","slug":"Hadoop配置详解","permalink":"https://shang.at/tags/Hadoop配置详解/"}]},{"title":"Shell编程-常用命令","slug":"Shell编程-常用命令","date":"2020-06-23T08:53:17.000Z","updated":"2020-07-16T07:03:49.566Z","comments":true,"path":"post/Shell编程-常用命令/","link":"","permalink":"https://shang.at/post/Shell编程-常用命令/","excerpt":"","text":"文本编辑sedawkLinux上的定时器cron命令xargsexec系统管理tophttps://blog.csdn.net/hsd2012/article/details/51387332 strace strace 命令是一种强大的工具, 能够显示任何由用户空间程式发出的系统调用. strace 显示这些调用的参数并返回符号形式的值. strace 从内核接收信息, 而且无需以任何特别的方式来构建内核. strace 的每一行输出包括系统调用名称, 然后是参数和返回值. 远程控制sshscprsync软件管理rpmRPM 全名是『 RedHat Package Manager 安装12345678910111213141516171819rpm -i *.rpm # 安装rpm包rpm -ivh *.rmp选项与参数：-i ：install 的意思-v ：察看更细部的安装资讯画面-h ：以安装资讯列显示安装进度范例一：安装 rp-pppoe-3.5-32.1.i386.rpmrpm -ivh rp-pppoe-3.5-32.1.i386.rpmPreparing... ####################################### [100%] 1:rp-pppoe ####################################### [100%] 范例二、一口气安装两个以上的软件时：rpm -ivh a.i386.rpm b.i386.rpm *.rpm# 后面直接接上许多的软件文件！范例三、直接由网络上面的某个文件安装，以网址来安装：rpm -ivh http://website.name/path/pkgname.rpm rpm 安装时常用的选项与参数说明 可下达的选项 代表意义 —nodeps 使用时机：当发生软件属性相依问题而无法安装，但你执意安装时 危险性： 软件会有相依性的原因是因为彼此会使用到对方的机制或功能，如果强制安装而不考虑软件的属性相依， 则可能会造成该软件的无法正常使用！ —replacefiles 使用时机： 如果在安装的过程当中出现了『某个文件已经被安装在你的系统上面』的资讯，又或许出现版本不合的信息 (confilcting files) 时，可以使用这个参数来直接覆盖文件。 危险性： 覆盖的动作是无法复原的！所以，你必须要很清楚的知道被覆盖的文件是真的可以被覆盖喔！否则会欲哭无泪！ —replacepkgs 使用时机： 重新安装某个已经安装过的软件！如果你要安装一堆 RPM 软件文件时，可以使用 rpm -ivh *.rpm ，但若某些软件已经安装过了， 此时系统会出现『某软件已安装』的资讯，导致无法继续安装。此时可使用这个选项来重复安装喔！ —force 使用时机：这个参数其实就是 —replacefiles 与 —replacepkgs 的综合体！ —test 使用时机： 想要测试一下该软件是否可以被安装到使用者的 Linux 环境当中，可找出是否有属性相依的问题。范例为： rpm -ivh pkgname.i386.rpm —test —justdb 使用时机： 由於 RPM 数据库破损或者是某些缘故产生错误时，可使用这个选项来升级软件在数据库内的相关资讯。 —nosignature 使用时机： 想要略过数码签章的检查时，可以使用这个选项。 —prefix 新路径 使用时机： 要将软件安装到其他非正规目录时。举例来说，你想要将某软件安装到 /usr/local 而非正规的 /bin, /etc 等目录， 就可以使用『 —prefix /usr/local 』来处理了。 —noscripts 使用时机：不想让该软件在安装过程中自行运行某些系统命令。 说明： RPM 的优点除了可以将文件放置到定位之外，还可以自动运行一些前置作业的命令，例如数据库的初始化。 如果你不想要让 RPM 帮你自动运行这一类型的命令，就加上他吧！ 升级12rpm -Uvh *.rpm rpm -Fvh *.rpm 选项 含义 -Uvh 后面接的软件即使没有安装过，则系统将予以直接安装； 若后面接的软件有安装过旧版，则系统自动升级至新版； -Fvh 如果后面接的软件并未安装到你的 Linux 系统上，则该软件不会被安装；亦即只有已安装至你 Linux 系统内的软件会被『升级』！ 查询123456789101112131415161718rpm -qa &lt;==已安装软件rpm -q[licdR] 已安装的软件名称 &lt;==已安装软件rpm -qf 存在於系统上面的某个档名 &lt;==已安装软件rpm -qp[licdR] 未安装的某个文件名称 &lt;==查阅RPM文件选项与参数：查询已安装软件的资讯：-q ：仅查询，后面接的软件名称是否有安装；-qa ：列出所有的，已经安装在本机 Linux 系统上面的所有软件名称；-qi ：列出该软件的详细资讯 (information)，包含开发商、版本与说明等；-ql ：列出该软件所有的文件与目录所在完整档名 (list)；-qc ：列出该软件的所有配置档 (找出在 /etc/ 底下的档名而已)-qd ：列出该软件的所有说明档 (找出与 man 有关的文件而已)-qR ：列出与该软件有关的相依软件所含的文件 (Required 的意思)-qf ：由后面接的文件名称，找出该文件属於哪一个已安装的软件；查询某个 RPM 文件内含有的资讯：-qp[icdlR]：注意 -qp 后面接的所有参数以上面的说明一致。但用途仅在於找出 某个 RPM 文件内的资讯，而非已安装的软件资讯！注意！ 练习 12345678910111213141516例题：我想要知道我的系统当中，以 c 开头的软件有几个，如何实做？我的 WWW 服务器为 Apache ，我知道他使用的 RPM 软件档名为 httpd 。现在，我想要知道这个软件的所有配置档放置在何处，可以怎么作？承上题，如果查出来的配置文件已经被我改过，但是我忘记了曾经修改过哪些地方，所以想要直接重新安装一次该软件，该如何作？如果我误砍了某个重要文件，例如 /etc/crontab，偏偏不晓得他属於哪一个软件，该怎么办？答：rpm -qa | grep ^c | wc -lrpm -qc httpd假设该软件在网络上的网址为：http://web.site.name/path/httpd-x.x.xx.i386.rpm则我可以这样做：rpm -ivh http://web.site.name/path/httpd-x.x.xx.i386.rpm --replacepkgs虽然已经没有这个文件了，不过没有关系，因为 RPM 有记录在 /var/lib/rpm 当中的数据库啊！所以直接下达：rpm -qf /etc/crontab就可以知道是那个软件罗！重新安装一次该软件即可！ 卸载1234rpm -e *** # 可能会因为依赖的关系，导致卸载失败rpm -e *** --nodeps # 强制卸载rpm --rebuilddb # 当rpm数据库/var/lib/rpm内的文件破损，可以使用这个命令重建数据库(很慢) yum","categories":[{"name":"Shell编程","slug":"Shell编程","permalink":"https://shang.at/categories/Shell编程/"}],"tags":[{"name":"常见命令","slug":"常见命令","permalink":"https://shang.at/tags/常见命令/"}]},{"title":"操作系统-centos7修改hostname","slug":"操作系统-centos7修改hostname","date":"2020-06-23T04:31:11.000Z","updated":"2020-06-23T04:36:45.901Z","comments":true,"path":"post/操作系统-centos7修改hostname/","link":"","permalink":"https://shang.at/post/操作系统-centos7修改hostname/","excerpt":"","text":"在CentOS7中，有三种定义的主机名:静态的（static）、瞬态的（transient）、灵活的（pretty）。“静态”主机名也称为内核主机名，是系统在启动时从/etc/hostname自动初始化的主机名。“瞬态”主机名是在系统运行时临时分配的主机名，例如，通过DHCP或mDNS服务器分配。静态主机名和瞬态主机名都遵从作为互联网域名同样的字符限制规则。而另一方面，“灵活”主机名则允许使用自由形式（包括特殊/空白字符）的主机名，以展示给终端用户。 方法一1234567891011121314151617181920212223242526[root@Geeklp201 ~]# hostnamectl #查看一下当前主机名的情况 Static hostname: Geeklp201 Icon name: computer-vm Chassis: vm Machine ID: 77efa27de81d470883b5bb0ed04f468c Boot ID: fa62bd1c0f5e4e53a0691fb97971594f Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-693.el7.x86_64 Architecture: x86-64[root@Geeklp201 ~]# hostnamectl set-hostname geeklp --static[root@Geeklp201 ~]# hostnamectl status Static hostname: geeklp Pretty hostname: Geeklp201 Icon name: computer-vm Chassis: vm Machine ID: 77efa27de81d470883b5bb0ed04f468c Boot ID: fa62bd1c0f5e4e53a0691fb97971594f Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-693.el7.x86_64 Architecture: x86-64重启VM 方法二通过修改文件/etc/hostname来实现主机名的修改。把该文件内容替换成自己想要的主机名重启即可。","categories":[],"tags":[{"name":"centos7修改hostname","slug":"centos7修改hostname","permalink":"https://shang.at/tags/centos7修改hostname/"}]},{"title":"工具使用-vim","slug":"工具使用-vim","date":"2020-06-23T04:22:22.000Z","updated":"2020-06-23T04:29:27.472Z","comments":true,"path":"post/工具使用-vim/","link":"","permalink":"https://shang.at/post/工具使用-vim/","excerpt":"","text":"vim快捷键 命令模式：esc 编辑模式： 在当前字符前开始编辑：命令模式下按i 在当前字符后开始编辑：命令模式下按a 另起一行：命令模式下按o 复制 单行复制 在命令模式下，将光标移动到将要复制的行处，按“yy”进行复制，将光标移动到将要粘贴的行处，按“p”进行粘贴； 多行复制 在命令模式下，将光标移动到将要复制的首行处，按“nyy”复制n行；其中n为1、2、3…… 2、粘贴 在命令模式下，将光标移动到将要粘贴的行处，按“p”进行粘贴 查询 输入： /abc 查询 abc 开头的单词 之后，所以以abc开头的单词都会标记高亮，输入 n 会查找下一个结果 ?pattern 向上搜索#继续搜索上一个 查看 暂时显示/取消行号： 使用Vim打开文件后，在Normal模式下输入 :set number（或 :set nu）显示行号 :set nonumber （或 :set nonu）取消行号 永久显示行号 查找Vim设定文件 sudo find / -name vimrc 修改Vim设定文件 /etc/vimrc ,末尾添加 set number （或 set nu） 保存即可。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://shang.at/tags/vim/"}]},{"title":"大数据-常见端口","slug":"大数据-常见端口","date":"2020-06-21T01:17:50.000Z","updated":"2020-06-23T07:50:07.013Z","comments":true,"path":"post/大数据-常见端口/","link":"","permalink":"https://shang.at/post/大数据-常见端口/","excerpt":"","text":"mysql：3306 redis： zookeeper：2181 kafka：9092 eagle：8048 hdfs：50070 yarn：8088 spark：4041 flink：8081","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"大数据端口","slug":"大数据端口","permalink":"https://shang.at/tags/大数据端口/"}]},{"title":"大数据-环境配置","slug":"大数据-环境配置","date":"2020-06-20T10:01:00.000Z","updated":"2020-06-22T08:51:19.705Z","comments":true,"path":"post/大数据-环境配置/","link":"","permalink":"https://shang.at/post/大数据-环境配置/","excerpt":"","text":"linux的文件和目录的权限规则使用ls -l命令可以查看当前目录的文件列表以及权限信息，显示如下 drwxr-xr-x linux安装rpm包免密登录123456789# 生成密钥 公钥sudo ssh-keygen -t rsa -f ~/.ssh/id_rsa# 将公钥拷贝到目标机器ssh-copy-id user@tartget_hostame# 这样之后就可以在节点之间 免密登录了ssh user@tartget_hostamessh tartget_hostame # 如果当前登录的用户和target的用户名一致，则不需要加user@ 注意，如果执行ssh-copy-id 输入密码后仍报错(Permission denied (publickey,gssapi-keyex,gssapi-with-mic))，可以尝试按照如下方案解决： 进入target机器，进入cd /etc/ssh/sshd_config，然后修改为PasswordAuthentication yes，最后重启sshd服务service sshd restart即可 同步时钟ntpdate cn.pool.ntp.org | ntp[1-7].aliyun.com clock -w","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"大数据环境","slug":"大数据环境","permalink":"https://shang.at/tags/大数据环境/"}]},{"title":"虚拟机-Vagrant使用","slug":"虚拟机-Vagrant使用","date":"2020-06-20T03:48:16.000Z","updated":"2020-07-13T17:07:13.994Z","comments":true,"path":"post/虚拟机-Vagrant使用/","link":"","permalink":"https://shang.at/post/虚拟机-Vagrant使用/","excerpt":"","text":"什么是Vagrantvagrant是一个基于VirtualBox, VMware, AWS等平台的一个构造和管理VM的工具，它提供了一个简单的工作流程，让VM的创建和管理全都自动化。vagrant的配置是基于ruby的 Vagrant是基于Box的，Box是针对Vagrant运行环境的封装 如何使用VagrantStep 1：安装 官网：https://www.vagrantup.com 虚拟机位置：/root/.vagrant.d/ Step 2：初始化环境 123mkdir vm-workspacecd vm-workspacevagrant init 123# box下载很慢的时候，可以先下载box，然后通过命令安装boxwget https://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox.boxvagrant box add --name centos/7 virtualbox.box Step 3：配置Vagrantfile 经过vagrant init之后，会在vm-workspace下生成一个Vagrantfile，内容如下(删除了注释) 12345# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(\"2\") do |config| config.vm.box = \"centos/7\"end 配置文件简介： 2 是指当前配置的Vagrant config的版本，目前Vagrant只支持两个版本 1和2，这里我不用改，用2就可以 config 就是配置对象，我们可以对他进行配置 do … end 是ruby的语法，就是一个代码块 下面是创建了 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- mode: ruby -*-# vi: set ft=ruby :# 2 指定了配置的版本Vagrant.configure(\"2\") do |config| # 指定 box为 centos/7 config.vm.box = \"centos/7\" # 使用define定义vm的配置节点：一个配置节点就是一个虚拟机。 # 这里表示：在config中配置一个master的vm，该vm的配置对象命名为master，下面可以对该配置进行配置 config.vm.define :master do |master| # 配置master的hostname为master master.vm.hostname = \"master\" # 定义 虚机容器提供者配置，这里使用virtualbox。打开virtualbox后，可以在里面看到对应的vm实例 master.vm.provider :virtualbox do |v| v.name = \"master\" # vm的名称 v.memory = 1024 # vm的内存 v.cpus = 1 # vm可以使用的CPU个数 end # 配置master vm使用host-only网络模式，ip为10.211.55.100 master.vm.network :private_network, ip: \"10.211.55.100\" # 配置vagrant 启动vm的时候，需要执行的命令或脚本 # master.vm.provision :shell, path: \"bootstrap_master.sh\" end # 再次循环给vm创建3个配置节点，即再创建3个虚拟机 (1..3).each do |i| config.vm.define \"node#&#123;i&#125;\" do |node| node.vm.hostname = \"node#&#123;i&#125;\" node.vm.provider :virtualbox do |v| v.name = \"node#&#123;i&#125;\" v.memory = 1024 v.cpus = 1 end node.vm.network :private_network, ip: \"10.211.55.10#&#123;i&#125;\" # node.vm.provision :shell, path: \"bootstrap_master.sh\" end end # config.vm.synced_folder \"\" \"\" # vagrant默认会把当前工作目录挂载在vm的/vagrant目录下 config.vm.provision :shell, path: \"bootstrap.sh\" config.vm.provision :shell, path: \"sshd.sh\"end 123456789101112131415161718# bootstrap.sh:虚拟机初始化的过程，并且配置java、sshkey、hostssudo yum -y updatesudo yum -y upgradesudo yum groupinstall -y developmentsudo yum install -y java-1.8.0-openjdk net-tools rsync mlocate wget vim \\ gcc zlib-dev openssl-devel sqlite-devel bzip2-devel python-devel# set Javaecho 'export JAVA_HOME=/usr/lib/jvm/jre' &gt;&gt; /etc/profile.d/java.shecho 'export PATH=/usr/lib/jvm/jre/bin:$PATH' &gt;&gt; /etc/profile.d/java.sh# sshkeysudo ssh-keygen -t rsa -f ~/.ssh/id_rsa# set hostsecho '10.211.55.100 node0' &gt;&gt; /etc/hostsecho '10.211.55.101 node1' &gt;&gt; /etc/hostsecho '10.211.55.102 node2' &gt;&gt; /etc/hostsecho '10.211.55.102 node3' &gt;&gt; /etc/hosts 12345678# sshd.sh: centos/7下的ssh 默认没有开启PasswordAuthentication，所以单独使用这个脚本开启一下# 免去后面配置免密登录的时候，再去修改# open PasswordAuthenticationsed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/g' /etc/ssh/sshd_configsed -i 's/PasswordAuthentication no/#PasswordAuthentication yes/g' /etc/ssh/sshd_config# restart sshdservice sshd restart 解释： Vagrant的网络连接方式有三种： NAT : 缺省创建，用于让vm可以通过host转发访问局域网甚至互联网。 host-only : 只有主机可以访问vm，其他机器无法访问它。 bridge : 此模式下vm就像局域网中的一台独立的机器，可以被其他机器访问。 根据vagrantfile的层次，分为： configure级：它定义在 Vagrant.configure(“2”) 的下一层次，形如： config.vm.provision … vm级：它定义在 config.vm.define :master do |master| 的下一层次，master.vm.provision … 执行的顺序是先执行configure级任务，再执行vm级任务，即便configure级任务在vm定义的下面才定义 Step 4：启动Vagrant 1vagrant up Step 5：连接vm 1vagrant ssh node0 Step 6：切换到root用户 12supassword默认是vagrant 至此，就可以使用vm了。 有一个非常有用的命令：vagrant rsync-auto。因为我们可能会经常性的修改共享文件夹，这个命令可以触发共享文件夹的同步，而不需要执行vagrant reload来同步共享文件夹，该命令会重启VM Vagrant常用命令vagrant help vagrant help 1vagrant help vagrant [command] -h 1vagrant box -h vagrant box 添加box 1vagrant box add --name centos-7.4-base centos-7.4-base.box 查看box 1vagrant box list 移除box 1vagrant box remove centos-7.4-base vagrant vm 初始化 如果自己编写Vagrantfile，不需要这一步 这一步会把config.vm.box写入 1vagrant init centos-7.4-base 启动虚拟机 1vagrant up 查看状态 1vagrant status ssh 1vagrant ssh 暂停虚拟机 1vagrant suspend 恢复虚拟机 1vagrant resume 关闭虚拟机 1vagrant halt 销毁虚拟机 1vagrant destroy 更新Vagrantfile，刷新容器 1vagrant reload vagrant snapshot使用虚拟机快照命令需要先启动虚拟机 保存虚拟机快照 1vagrant snapshot save snap1 list虚拟机快照 1vagrant snapshot list 恢复虚拟机快照 1vagrant snapshot restore snap1 删除虚拟机快照 1vagrant snapshot delete snap1","categories":[{"name":"虚拟机","slug":"虚拟机","permalink":"https://shang.at/categories/虚拟机/"}],"tags":[{"name":"Vagrant","slug":"Vagrant","permalink":"https://shang.at/tags/Vagrant/"}]},{"title":"大数据-消息队列-数据采集-Kafka","slug":"大数据-消息队列-数据采集-Kafka","date":"2020-06-20T03:14:01.000Z","updated":"2020-06-23T06:51:13.466Z","comments":true,"path":"post/大数据-消息队列-数据采集-Kafka/","link":"","permalink":"https://shang.at/post/大数据-消息队列-数据采集-Kafka/","excerpt":"","text":"Kafka的基本概念和架构Apache Kafka是Apache软件基金会的开源的流处理平台，该平台提供了消息的订阅与发布的消息队列，一般用作系统间解耦、异步通信、削峰填谷等作用 逻辑上概念： Producer：生产消息的客户端 Consumer：消费消息的客户端 Consumer Group：同一个ConsumerGroup中的Consumer往往是一个服务的多个实例，用来提高消费的效率，也就是说同一个CG中的多个C不能重复消费消息；不同CG往往代表了多种服务，他们处理不同的业务，所以，不同的CG中的C对于消息的处理是相互独立的，如CG2中的C2可以重复的消费在CG1中的C1已经消费过的消息 消费者使用Consumer Group名称标记自己，并且发布到Topic的每条记录都会传递到每个订阅Consumer Group中的一个消费者实例。 ​ 如果所有Consumer实例都具有相同的Consumer Group，那么Topic中的记录会在该ConsumerGroup中的Consumer实例进行均分消费； ​ 如果所有Consumer实例具有不同的ConsumerGroup，则每条记录将广播到所有Consumer Group进程。 更常见的是，我们发现Topic具有少量的Consumer Group，每个Consumer Group可以理解为一个“逻辑的订阅者”。每个Consumer Group均由许多Consumer实例组成，以实现可伸缩性和容错能力。这无非就是发布-订阅模型，其中订阅者是消费者的集群而不是单个进程。这种消费方式Kafka会将Topic按照分区的方式均分给一个Consumer Group下的实例，如果ConsumerGroup下有新的成员介入，则新介入的Consumer实例会去接管ConsumerGroup内其他消费者负责的某些分区，同样如果一下ConsumerGroup下的有其他Consumer实例宕机，则由该ConsumerGroup中其他Consumer实例接管。 Tocpic：一组Record可以作为一个Topic在集群中被管理 Record：Producer生产的每一条消息就是一个Record，每一个Record只能属于一个Topic 由于Kafka的Topic的分区策略，因此Kafka仅提供分区中记录的有序性，也就意味着相同Topic的不同分区记录之间无顺序。因为针对于绝大多数的大数据应用和使用场景， 使用分区内部有序或者使用key进行分区策略已经足够满足绝大多数应用场景。但是，如果您需要记录全局有序，则可以通过只有一个分区Topic来实现，尽管这将意味着每个ConsumerGroup只有一个Consumer进程 Partition：每个Topic会有num.partitions(默认)个分区，每个Topic在创建的时候，也可以被指定分区的个数。 Kafka中对Topic实现日志分区的有以下目的： 支持集群存储的横向扩容。如果单一服务器的资源不够用，那么增加集群节点即可 每个服务器充当其某些分区的Leader，也可能充当其他分区的Follwer，因此群集中的负载得到了很好的平衡。 同一个Topic 多个分区可以提高Consumer消费的并行度 在kafka中同一个partition的record是严格有序的，但是不同partition的record并不是严格有序的。也就是说，kafka只能保证partition内部record的有序消费 Duplicate(副本)：每个分区会有--replication-factor个副本，是在Topic被创建的时候指定的 offset (非常重要的一个概念) — 待补充 在kafka中对于消息的生产和消费都是通过offset控制的。同一个partition的消息record的offset是递增的，消费者消费的时候，也是消费的指定的offset之后的消息。 消费者会定期的上传自己消费的offset给kafka server Segments — 待补充 架构上的概念 Broker：Kafka集群内的节点被称为broker Leader：kafka采用主从的架构，每个partition都有自己的leader。每个partition的leader负责消息的读写 Follower：每个 ISR kafka的基本使用命令 创建topic 123456kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--create \\--topic topic01 \\--partitions 3 \\--replication-factor 3 查看topic列表 123kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--list 查看topic详情 1234kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--describe \\--topic topic01 修改topic 123456789101112kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--create \\--topic topic03 \\--partitions 1 \\--replication-factor 1kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--alter \\--topic topic03 \\--partitions 2 删除topic 1234kafka-topics.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--delete \\--topic topic03 消费者订阅topic 1234567kafka-console-consumer.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--topic topic01 \\--group g1 \\--property print.key=true \\--property print.value=true \\--property key.separator=, 消费者组 12345678kafka-consumer-groups.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--listkafka-consumer-groups.sh \\--bootstrap-server node1:9092,node2:9092,node3:9092 \\--describe \\--group g1 生产者生产消息 123kafka-console-producer.sh \\--broker-list node1:9092,node2:9092,node3:9092 \\--topic topic01 API基本API高级API架构进阶高性能分析之零拷贝&amp;源码分析数据同步机制kafka与其他软件的集成与Flume的集成与SpringBoot的集成在大数据流计算中的应用ZookeeperKafka中的leader监控和topic的元数据，都是存在zk中","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://shang.at/tags/Kafka/"}]},{"title":"大数据-知识点","slug":"大数据-知识点","date":"2020-06-20T02:46:39.000Z","updated":"2020-06-22T10:30:39.392Z","comments":true,"path":"post/大数据-知识点/","link":"","permalink":"https://shang.at/post/大数据-知识点/","excerpt":"","text":"生态管理员-Zookeeper 节点通信-RPC Hadoop 数据存储-HDFS 资源管理-YARN 任务调度-AppMaster：所有第三方的应用可以实现AppMaster，即可将任务跑在YARN上 数据采集-Flume 消息队列-数据采集-Kafka 数仓-Hive 数据存储-HBase 流批一体-Spark 流批一体-Flink 机器学习-Spark mlib 机器学习-FlinkML","categories":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"}],"tags":[{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/tags/大数据生态/"}]},{"title":"java学习-底层知识总结","slug":"java学习-底层知识总结","date":"2020-06-13T00:20:45.000Z","updated":"2020-06-13T02:18:33.907Z","comments":true,"path":"post/java学习-底层知识总结/","link":"","permalink":"https://shang.at/post/java学习-底层知识总结/","excerpt":"","text":"JVMJVM：Java Virtual Machine JMM：Java Memory Model 1：JVM基础知识 什么是JVM 常见的JVM 2：ClassFileFormat3：类编译-加载-初始化hashcode锁的信息（2位 四种组合）GC信息（年龄）如果是数组，数组的长度 4：JMMnew Cat()pointer -&gt; Cat.class寻找方法的信息 5：对象1：句柄池 （指针池）间接指针，节省内存2：直接指针，访问速度快 6：GC基础知识栈上分配TLAB（Thread Local Allocation Buffer）OldEden老不死 - &gt; Old 7：GC常用垃圾回收器new Object()markword 8个字节类型指针 8个字节实例变量 0补齐 016字节（压缩 非压缩）Object o8个字节JVM参数指定压缩或非压缩","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"java底层","slug":"java底层","permalink":"https://shang.at/tags/java底层/"}]},{"title":"java学习-class文件格式","slug":"java学习-class文件格式","date":"2020-06-13T00:12:38.000Z","updated":"2020-06-13T00:18:59.170Z","comments":true,"path":"post/java学习-class文件格式/","link":"","permalink":"https://shang.at/post/java学习-class文件格式/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"class文件格式","slug":"class文件格式","permalink":"https://shang.at/tags/class文件格式/"}]},{"title":"Python学习-metaclass实现单例","slug":"Python学习-metaclass实现单例","date":"2020-06-12T01:35:16.000Z","updated":"2020-06-12T01:35:16.633Z","comments":true,"path":"post/Python学习-metaclass实现单例/","link":"","permalink":"https://shang.at/post/Python学习-metaclass实现单例/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"java学习-运行时线程","slug":"java学习-运行时线程","date":"2020-06-11T03:40:56.000Z","updated":"2020-06-11T03:52:13.279Z","comments":true,"path":"post/java学习-运行时线程/","link":"","permalink":"https://shang.at/post/java学习-运行时线程/","excerpt":"","text":"一个Java进程启动之后，至少会创建以下几个线程： main Finalizer Reference Handler Signal Dispatcher Others: 用户自己创建的线程","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"运行时线程","slug":"运行时线程","permalink":"https://shang.at/tags/运行时线程/"}]},{"title":"数据结构与算法学习笔记-JOIN的算法实现","slug":"数据结构与算法学习笔记-JOIN的算法实现","date":"2020-06-10T06:40:09.000Z","updated":"2020-07-13T13:33:48.925Z","comments":true,"path":"post/数据结构与算法学习笔记-JOIN的算法实现/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-JOIN的算法实现/","excerpt":"https://www.infoq.cn/article/6XGx92FyQ45cMXpj2mgZ","text":"https://www.infoq.cn/article/6XGx92FyQ45cMXpj2mgZ JOIN(INNER JOIN) 内连接：左右两个集合的交集，只保留左右两个集合都有的元素 内连接的时候会先排序(O($NlogN$))，然后关联两个有序集合(O($N$)) 12 LEFT JOIN 左外连接：以左集合为主，右集合不存在的元素补null RIGHT JOIN 右外连接：以右集合为主，左集合不存在的元素补null FULL JOIN 全连接：左右两个集合的全集，其中 左集合有但是右集合不存在的元素，右集合对应的字段补null，反之亦然 left_semi 过滤出左集合中和右集合共有的部分，只返回左集合中的rows left_anti 过滤出左集合中 右集合没有的部分，只返回左集合中的rows","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"JOIN的算法实现","slug":"JOIN的算法实现","permalink":"https://shang.at/tags/JOIN的算法实现/"}]},{"title":"java学习-JDK环境切换","slug":"java学习-JDK环境切换","date":"2020-06-10T05:40:50.000Z","updated":"2020-06-10T05:47:55.073Z","comments":true,"path":"post/java学习-JDK环境切换/","link":"","permalink":"https://shang.at/post/java学习-JDK环境切换/","excerpt":"","text":"近期，JDK版本更新十分频繁，如果要想快速切换JDK版本，可以通过linux的alias命令来简单实现： 12345678910111213141516# mac环境# ~/.bash_profileexport JAVA_8_HOME=$(/usr/libexec/java_home -v 1.8)export JAVA_9_HOME=$(/usr/libexec/java_home -v 9)export JAVA_10_HOME=$(/usr/libexec/java_home -v 10)export JAVA_11_HOME=$(/usr/libexec/java_home -v 11)export JAVA_HOME=$JAVA_8_HOMEexport PATH=$JAVA_HOME/bin:$PATH# ~/.zshrc 注意要放到~/.zshrc文件的最下面# multi jdk configalias jdk8=\"export PATH=$JAVA_8_HOME/bin:$PATH\"alias jdk9=\"export PATH=$JAVA_9_HOME/bin:$PATH\"alias jdk10=\"export PATH=$JAVA_10_HOME/bin:$PATH\"alias jdk11=\"export PATH=$JAVA_11_HOME/bin:$PATH\" 默认环境为 1234➜ ~ java -versionjava version \"1.8.0_144\"Java(TM) SE Runtime Environment (build 1.8.0_144-b01)Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) 切换jdk9之后 12345➜ ~ jdk9➜ ~ java -versionjava version \"9.0.4\"Java(TM) SE Runtime Environment (build 9.0.4+11)Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode) 这样就可以在当前的terminal session中使用jdk9的新特性了，比如jshell","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JDK环境切换","slug":"JDK环境切换","permalink":"https://shang.at/tags/JDK环境切换/"}]},{"title":"java学习-JVM虚拟机栈","slug":"java学习-JVM虚拟机栈","date":"2020-06-09T07:39:37.000Z","updated":"2020-06-09T07:40:05.128Z","comments":true,"path":"post/java学习-JVM虚拟机栈/","link":"","permalink":"https://shang.at/post/java学习-JVM虚拟机栈/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM虚拟机栈","slug":"JVM虚拟机栈","permalink":"https://shang.at/tags/JVM虚拟机栈/"}]},{"title":"java学习-JVM疑问","slug":"java学习-JVM疑问","date":"2020-06-05T09:01:58.000Z","updated":"2020-06-09T07:36:50.916Z","comments":true,"path":"post/java学习-JVM疑问/","link":"","permalink":"https://shang.at/post/java学习-JVM疑问/","excerpt":"","text":"在学习JVM的内存模型的时候，我有这样一些疑惑： 1、我们通常只是定义了堆大小(-Xms初始，-Xmx最大)，虚拟机栈大小(-Xss)。但是我发现这并不能计算出一个java进程占用的全部内存大小。以下是我自己理解的(JDK1.8)：java进程占用的内存​ =JVM管理的内存+非JVM管理的内存​ =线程独立的内存+线程共享的内存​ =n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)+堆内存(heap)+非堆内存(non-heap)+元空间(metaspace)+堆外内存(off-heap:direct memory)其中：​ JVM管理的内存：n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)+堆内存(heap)+非堆内存(non-heap)​ 非JVM管理的内存：元空间(metaspace)+堆外内存(off-heap:direct memory)​ 线程独立的内存：n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)，n是线程数​ 线程共享的内存：堆内存(heap)+非堆内存(non-heap)+元空间(metaspace)+对外内存(off-heap:direct memory) 2、在JDK1.7及以前，有个永久代(PermGen)，也就是文中说的方法区。这块区域也被称为非堆内存​ 那么在JDK1.8及以后，永久代变成了元空间，到了JVM管理之外了，那么JDK1.8及以后的版本中还有非堆内存(non-heap)的说法吗？如果有的话，是指什么呢？ 3、关于线程独立的这块内存\\{n*(虚拟机栈内存+程序计数器内存+本地方法栈内存)，n是线程数}，它是完全独立于其他的内存的吗？​ 还是会分享堆内存，受到堆内存大小的限制​ 还是说Thread对象是建立在堆内存，然后每个Thread对应的虚拟机栈都是独立的吗？ 换句话说，随着Thread的增加(堆内存充足：还能给新的对象分配内存)，java进程占用的内存会越来越大——-我觉得这肯定不对，但是我却无法解释 4、我做了一些测试(JDK1.8)：​ 4.1、指定很小的堆内存，改变虚拟机栈大小​ 4.1.1、-Xms2m -Xmx2m -Xss16m 启动java进程，直到递归调用1,016,085深度，会报StackOverflowError​ 4.1.2、-Xms2m -Xmx2m -Xss8m 启动java进程，直到递归调用318,031深度，会报StackOverflowError​ 4.2、指定很小的堆内存，如-Xms2m -Xmx2m，最终会报OutOfMemoryError 我谈一下我的理解，首先，新创建的线程对象肯定是放在堆中的；每个线程独立的虚拟机栈，存放了很多的栈帧，每个栈帧实际上存放了局部变量表(和其他三部分)，每个栈帧对应了一个函数调用，在这个线程中执行的每个函数中的变量可能会存放在堆里面，也有可能会直接在栈上分配内存，因为这里有一个逃逸的概念(仅函数内部使用的局部变量直接在栈上分配内存，不会占用堆内存)。 所以在我的理解里面，如果虚拟机栈是一个完全独立于堆的内存，那么虚拟机栈就不会受到堆内存大小的限制(比如我上面做的实验：当堆内存远小于虚拟机栈大小，最终报的异常仍然是StackOverflowError，而不是OutOfMemoryError) 所以我才会想到，如果虚拟机栈是一个完全独立于堆的内存，无限的创建线程，每个线程的虚拟机栈如果都无限接近于-Xss分配的最大限度，那么最终会耗尽系统的所有 内存吧 我又做了一个实验，就是无限的创建线程，然后在调用start()的时候，报了OutOfMemoryError:unable to create new native thread的异常，看来操作系统在这里是对线程数是有限制的。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM疑问","slug":"JVM疑问","permalink":"https://shang.at/tags/JVM疑问/"}]},{"title":"java学习-基本类型和包装类型","slug":"java学习-基本类型和包装类型","date":"2020-06-04T03:37:10.000Z","updated":"2020-06-04T06:32:35.963Z","comments":true,"path":"post/java学习-基本类型和包装类型/","link":"","permalink":"https://shang.at/post/java学习-基本类型和包装类型/","excerpt":"","text":"Java 的每个基本类型都对应了一个包装类型，比如说 int 的包装类型为 Integer，double 的包装类型为 Double。基本类型和包装类型的区别主要有以下 4 点。 01、包装类型可以为 null，而基本类型不可以别小看这一点区别，它使得包装类型可以应用于 POJO 中，而基本类型则不行。 POJO 是什么呢？这里稍微说明一下。 POJO 的英文全称是 Plain Ordinary Java Object，翻译一下就是，简单无规则的 Java 对象，只有属性字段以及 setter 和 getter 方法，示例如下。 1234567891011121314151617181920class Writer &#123;private Integer age;private String name;public Integer getAge() &#123;return age;&#125;public void setAge(Integer age) &#123;this.age = age;&#125;public String getName() &#123;return name;&#125;public void setName(String name) &#123;this.name = name;&#125;&#125; 和 POJO 类似的，还有数据传输对象 DTO（Data Transfer Object，泛指用于展示层与服务层之间的数据传输对象）、视图对象 VO（View Object，把某个页面的数据封装起来）、持久化对象 PO（Persistant Object，可以看成是与数据库中的表映射的 Java 对象）。 那为什么 POJO 的属性必须要用包装类型呢？ 《阿里巴巴 Java 开发手册》上有详细的说明，我们来大声朗读一下（预备，起）。 数据库的查询结果可能是 null，如果使用基本类型的话，因为要自动拆箱（将包装类型转为基本类型，比如说把 Integer 对象转换成 int 值），就会抛出 NullPointerException 的异常。 02、包装类型可用于泛型，而基本类型不可以泛型不能使用基本类型，因为使用基本类型时会编译出错。 12List&lt;int&gt; list = new ArrayList&lt;&gt;(); // 提示 Syntax error, insert &quot;Dimensions&quot; to complete ReferenceTypeList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); 为什么呢？因为泛型在编译时会进行类型擦除，最后只保留原始类型，而原始类型只能是 Object 类及其子类——基本类型是个特例。 03、基本类型比包装类型更高效基本类型在栈中直接存储的具体数值，而包装类型则存储的是堆中的引用。 很显然，相比较于基本类型而言，包装类型需要占用更多的内存空间。假如没有基本类型的话，对于数值这类经常使用到的数据来说，每次都要通过 new 一个包装类型就显得非常笨重。 03、两个包装类型的值可以相同，但却不相等两个包装类型的值可以相同，但却不相等——这句话怎么理解呢？来看一段代码就明明白白了。 12345Integer chenmo = new Integer(10);Integer wanger = new Integer(10);System.out.println(chenmo == wanger); // falseSystem.out.println(chenmo.equals(wanger )); // true 两个包装类型在使用“==”进行判断的时候，判断的是其指向的地址是否相等。chenmo 和 wanger 两个变量使用了 new 关键字，导致它们在“==”的时候输出了 false。 而 chenmo.equals(wanger) 的输出结果为 true，是因为 equals 方法内部比较的是两个 int 值是否相等。源码如下。 1234567891011private final int value;public int intValue() &#123;return value;&#125;public boolean equals(Object obj) &#123;if (obj instanceof Integer) &#123;return value == ((Integer)obj).intValue();&#125;return false;&#125; 瞧，虽然 chenmo 和 wanger 的值都是 10，但他们并不相等。换句话说就是：将“==”操作符应用于包装类型比较的时候，其结果很可能会和预期的不符。 04、自动装箱和自动拆箱既然有了基本类型和包装类型，肯定有些时候要在它们之间进行转换。把基本类型转换成包装类型的过程叫做装箱（boxing）。反之，把包装类型转换成基本类型的过程叫做拆箱（unboxing）。 在 Java SE5 之前，开发人员要手动进行装拆箱，比如说： 12Integer chenmo = new Integer(10); // 手动装箱int wanger = chenmo.intValue(); // 手动拆箱 Java SE5 为了减少开发人员的工作，提供了自动装箱与自动拆箱的功能。 12Integer chenmo = 10; // 自动装箱int wanger = chenmo; // 自动拆箱 上面这段代码使用 JAD 反编译后的结果如下所示： 12Integer chenmo = Integer.valueOf(10);int wanger = chenmo.intValue(); 也就是说，自动装箱是通过 Integer.valueOf() 完成的；自动拆箱是通过 Integer.intValue() 完成的。理解了原理之后，我们再来看一道老马当年给我出的面试题。 1234567891011121314// 1）基本类型和包装类型int a = 100;Integer b = 100;System.out.println(a == b);// 2）两个包装类型Integer c = 100;Integer d = 100;System.out.println(c == d);// 3）c = 200;d = 200;System.out.println(c == d); 答案是什么呢？有举手要回答的吗？答对的奖励一朵小红花哦。 第一段代码，基本类型和包装类型进行 == 比较，这时候 b 会自动拆箱，直接和 a 比较值，所以结果为 true。 第二段代码，两个包装类型都被赋值为了 100，这时候会进行自动装箱，那 == 的结果会是什么呢？ 我们之前的结论是：将“==”操作符应用于包装类型比较的时候，其结果很可能会和预期的不符。那结果是 false？但这次的结果却是 true，是不是感觉很意外？ 第三段代码，两个包装类型重新被赋值为了 200，这时候仍然会进行自动装箱，那 == 的结果会是什么呢？ 吃了第二段代码的亏后，是不是有点怀疑人生了，这次结果是 true 还是 false 呢？扔个硬币吧，哈哈。我先告诉你结果吧，false。 为什么？为什么？为什么呢？ 事情到了这一步，必须使出杀手锏了——分析源码吧。 之前我们已经知道了，自动装箱是通过 Integer.valueOf() 完成的，那我们就来看看这个方法的源码吧。 12345public static Integer valueOf(int i) &#123;if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)return IntegerCache.cache[i + (-IntegerCache.low)];return new Integer(i);&#125; 难不成是 IntegerCache 在作怪？你猜对了！ 12345678910111213141516171819202122private static class IntegerCache &#123;static final int low = -128;static final int high;static final Integer cache[];static &#123;// high value may be configured by propertyint h = 127;int i = parseInt(integerCacheHighPropValue);i = Math.max(i, 127);h = Math.min(i, Integer.MAX_VALUE - (-low) -1);high = h;cache = new Integer[(high - low) + 1];int j = low;for(int k = 0; k &lt; cache.length; k++)cache[k] = new Integer(j++);// range [-128, 127] must be interned (JLS7 5.1.7)assert IntegerCache.high &gt;= 127;&#125;&#125; 大致瞟一下这段代码你就全明白了。-128 到 127 之间的数会从 IntegerCache 中取，然后比较，所以第二段代码（100 在这个范围之内）的结果是 true，而第三段代码（200 不在这个范围之内，所以 new 出来了两个 Integer 对象）的结果是 false。 看完上面的分析之后，我希望大家记住一点：当需要进行自动装箱时，如果数字在 -128 至 127 之间时，会直接使用缓存中的对象，而不是重新创建一个对象。 自动装拆箱是一个很好的功能，大大节省了我们开发人员的精力，但也会引发一些麻烦，比如下面这段代码，性能就很差。 1234567long t1 = System.currentTimeMillis();Long sum = 0L;for (int i = 0; i &lt; Integer.MAX_VALUE;i++) &#123; sum += i;&#125;long t2 = System.currentTimeMillis();System.out.println(t2-t1); sum 由于被声明成了包装类型 Long 而不是基本类型 long，所以 sum += i 进行了大量的拆装箱操作（sum 先拆箱和 i 相加，然后再装箱赋值给 sum），导致这段代码运行完花费的时间足足有 2986 毫秒；如果把 sum 换成基本类型 long，时间就仅有 554 毫秒，完全不一个等量级啊。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"基本类型和包装类型","slug":"基本类型和包装类型","permalink":"https://shang.at/tags/基本类型和包装类型/"}]},{"title":"java学习-JVM-heap-non-heap-off-heap","slug":"java学习-JVM-heap-non-heap-off-heap","date":"2020-06-03T22:50:26.000Z","updated":"2020-06-03T22:50:26.253Z","comments":true,"path":"post/java学习-JVM-heap-non-heap-off-heap/","link":"","permalink":"https://shang.at/post/java学习-JVM-heap-non-heap-off-heap/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"java网络编程-零拷贝","slug":"java网络编程-零拷贝","date":"2020-06-03T16:32:39.000Z","updated":"2020-06-03T23:37:38.921Z","comments":true,"path":"post/java网络编程-零拷贝/","link":"","permalink":"https://shang.at/post/java网络编程-零拷贝/","excerpt":"","text":"什么是零拷贝刚才讲阻塞 IO 的时候我讲到，系统内核处理 IO 操作分为两个阶段——等待数据和拷贝数据。等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中；而拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中。以下是具体流程： 应用进程的每一次写操作，都会把数据写到用户空间的缓冲区中，再由 CPU 将数据拷贝到系统内核的缓冲区中，之后再由 DMA 将这份数据拷贝到网卡中，最后由网卡发送出去。这里我们可以看到，一次写操作数据要拷贝两次才能通过网卡发送出去，而用户进程的读操作则是将整个流程反过来，数据同样会拷贝两次才能让应用程序读取到数据。 应用进程的一次完整的读写操作，都需要在用户空间与内核空间中来回拷贝，并且每一次拷贝，都需要 CPU 进行一次上下文切换（由用户进程切换到系统内核，或由系统内核切换到用户进程），这样是不是很浪费 CPU 和性能呢？那有没有什么方式，可以减少进程间的数据拷贝，提高数据传输的效率呢？ 这时我们就需要零拷贝（Zero-copy）技术。 所谓的零拷贝，就是取消用户空间与内核空间之间的数据拷贝操作，应用进程每一次的读写操作，都可以通过一种方式，让应用进程向用户空间写入或者读取数据，就如同直接向内核空间写入或者读取数据一样，再通过 DMA 将内核中的数据拷贝到网卡，或将网卡中的数据 copy 到内核。 那怎么做到零拷贝？你想一下是不是用户空间与内核空间都将数据写到一个地方，就不需要拷贝了？此时你有没有想到虚拟内存？ 零拷贝有两种解决方式，分别是 mmap+write 方式和 sendfile 方式，mmap+write 方式的核心原理就是通过虚拟内存来解决的。 java中的零拷贝Netty中的零拷贝 具体去读Netty的源码再详细补充这里 了解完零拷贝，我们再看看 Netty 中的零拷贝。 我刚才讲到，RPC 框架在网络通信框架的选型上，我们最优的选择是基于 Reactor 模式实现的框架，如 Java 语言，首选的便是 Netty 框架。那么 Netty 框架是否也有零拷贝机制呢？Netty 框架中的零拷贝和我之前讲的零拷贝又有什么不同呢？ 刚才我讲的零拷贝是操作系统层面上的零拷贝，主要目标是避免用户空间与内核空间之间的数据拷贝操作，可以提升 CPU 的利用率。 而 Netty 的零拷贝则不大一样，他完全站在了用户空间上，也就是 JVM 上，它的零拷贝主要是偏向于数据操作的优化上。 那么 Netty 这么做的意义是什么呢？ 回想下[第 02 讲]，在这一讲中我讲解了 RPC 框架如何去设计协议，其中我讲到：在传输过程中，RPC 并不会把请求参数的所有二进制数据整体一下子发送到对端机器上，中间可能会拆分成好几个数据包，也可能会合并其他请求的数据包，所以消息都需要有边界。那么一端的机器收到消息之后，就需要对数据包进行处理，根据边界对数据包进行分割和合并，最终获得一条完整的消息。 那收到消息后，对数据包的分割和合并，是在用户空间完成，还是在内核空间完成的呢？ 当然是在用户空间，因为对数据包的处理工作都是由应用程序来处理的，那么这里有没有可能存在数据的拷贝操作？可能会存在，当然不是在用户空间与内核空间之间的拷贝，是用户空间内部内存中的拷贝处理操作。Netty 的零拷贝就是为了解决这个问题，在用户空间对数据操作进行优化。 那么 Netty 是怎么对数据操作进行优化的呢？ Netty 提供了 CompositeByteBuf 类，它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免了各个 ByteBuf 之间的拷贝。 ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝。 通过 wrap 操作，我们可以将 byte[] 数组、ByteBuf、ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免拷贝操作。 Netty 框架中很多内部的 ChannelHandler 实现类，都是通过 CompositeByteBuf、slice、wrap 操作来处理 TCP 传输中的拆包与粘包问题的。 那么 Netty 有没有解决用户空间与内核空间之间的数据拷贝问题的方法呢？ Netty 的 ByteBuffer 可以采用 Direct Buffers，使用堆外直接内存进行 Socket 的读写操作，最终的效果与我刚才讲解的虚拟内存所实现的效果是一样的。(mmap方式) Netty 还提供 FileRegion 中包装 NIO 的 FileChannel.transferTo() 方法实现了零拷贝，这与 Linux 中的 sendfile 方式在原理上也是一样的。 总结零拷贝带来的好处就是避免没必要的 CPU 拷贝，让 CPU 解脱出来去做其他的事，同时也减少了 CPU 在用户空间与内核空间之间的上下文切换，从而提升了网络通信效率与应用程序的整体性能。 而 Netty 的零拷贝与操作系统的零拷贝是有些区别的，Netty 的零拷贝偏向于用户空间中对数据操作的优化，这对处理 TCP 传输中的拆包粘包问题有着重要的意义，对应用程序处理请求数据与返回数据也有重要的意义。 在 RPC 框架的开发与使用过程中，我们要深入了解网络通信相关的原理知识，尽量做到零拷贝，如使用 Netty 框架；我们要合理使用 ByteBuf 子类，做到完全零拷贝，提升 RPC 框架的整体性能。 其他关于零拷贝技术的文章： Java中的零拷贝","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"网络编程-零拷贝","slug":"网络编程-零拷贝","permalink":"https://shang.at/tags/网络编程-零拷贝/"}]},{"title":"java学习-JVM参数","slug":"java学习-JVM参数","date":"2020-06-03T10:36:02.000Z","updated":"2020-06-04T03:30:03.517Z","comments":true,"path":"post/java学习-JVM参数/","link":"","permalink":"https://shang.at/post/java学习-JVM参数/","excerpt":"","text":"查看JVM参数启动应用的时候分别加以下的参数可以打印相关的参数： java -XX:+PrintFlagsInitial 打印所有的JVM初始参数，但是会立刻终止应用 java -XX:+PrintFlagsFinal 打印所有的设置后的JVM参数，不会终止应用 jinfo [options] pid ​ jinfo -flags pid 查看指定pid的jvm的所有设置参数 -XX:+PrintVMOptions 程序运行时，打印虚拟机接受到的命令行显式参数。不会终止应用 -XX:+PrintCommandLineFlags 打印传递给虚拟机的显式和隐式参数。不会终止应用 常见的JVM参数 InitialHeapSize(单位是字节)：初始堆大小，默认是物理内存的1/64，最小为2m(设置了1m，发现PrintFlagsFinal打印出来的是2m)，可以使用-Xms指定，如-Xms64m，只能指定m、g这样的单位 MaxHeapSize：最大堆大小，默认是物理内存的1/4，最小为2m，可以使用-Xmx指定，如-Xmx64m MaxNewSize： NewSize： OldSize： MetaspaceSize： MaxMetaspaceSize： ThreadStackSize(单位是kb)：虚拟机栈大小，默认是1024k，可以使用-Xss指定，如-Xss256k MaxTenuringThreshold：对象晋升到老年代的年龄阈值，默认是15，可以使用-XX:MaxTenuringThreshold指定，如-XX:MaxTenuringThreshold=20 参数调优 默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM参数","slug":"JVM参数","permalink":"https://shang.at/tags/JVM参数/"}]},{"title":"Java学习-常见异常","slug":"Java学习-常见异常","date":"2020-06-03T09:31:00.000Z","updated":"2020-06-12T02:06:08.511Z","comments":true,"path":"post/Java学习-常见异常/","link":"","permalink":"https://shang.at/post/Java学习-常见异常/","excerpt":"","text":"StackOverflowError和OutOfMemoryError的区别(JDK1.8)StackOverflowError：Thrown when a stack overflow occurs because an application recurses too deeply. OutOfMemoryError：Thrown when the Java Virtual Machine cannot allocate an object because it is out of memory, and no more memory could be made available by the garbage collector. 从对这两个Error的注释来看， 由于应用程序递归过深而在堆栈溢出时会抛出StackOverflowError；同一个函数递归调用时，内存不足 当Java虚拟机由于内存不足而无法分配对象，并且垃圾回收器无法再提供更多内存时，抛出OutOfMemoryError。创建新的对象(包括虚拟机栈：函数调用(也包括递归调用)时创建虚拟机栈)时，内存不足 在java应用启动的时候，可以通过-Xss来设置虚拟机栈大小，虚拟机栈默认大小为1024k 在java里，函数的递归调用受到以下几方面的影响： 虚拟机栈大小：设置的虚拟机栈 局部变量表大小 https://blog.csdn.net/chengyun19830206/article/details/78452321 https://dzone.com/articles/outofmemoryerror-unable-create https://blog.csdn.net/kylinsoong/article/details/16879653 https://www.jianshu.com/p/c09727dc8f92","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"常见异常","slug":"常见异常","permalink":"https://shang.at/tags/常见异常/"}]},{"title":"JAVA并发编程-12-并发框架(Disruptor)","slug":"JAVA并发编程-12-并发框架-Disruptor","date":"2020-06-03T02:54:59.000Z","updated":"2020-06-03T03:00:26.307Z","comments":true,"path":"post/JAVA并发编程-12-并发框架-Disruptor/","link":"","permalink":"https://shang.at/post/JAVA并发编程-12-并发框架-Disruptor/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"Disruptor","slug":"Disruptor","permalink":"https://shang.at/tags/Disruptor/"}]},{"title":"JAVA并发编程-11-响应式编程(RxJava)","slug":"JAVA并发编程-11-响应式编程-RxJava","date":"2020-06-03T02:53:44.000Z","updated":"2020-06-03T08:24:11.250Z","comments":true,"path":"post/JAVA并发编程-11-响应式编程-RxJava/","link":"","permalink":"https://shang.at/post/JAVA并发编程-11-响应式编程-RxJava/","excerpt":"","text":"https://juejin.im/post/5ed62cabf265da7709526718","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"RxJava","slug":"RxJava","permalink":"https://shang.at/tags/RxJava/"}]},{"title":"JAVA并发编程-10-协程","slug":"JAVA并发编程-10-协程","date":"2020-06-03T02:53:19.000Z","updated":"2020-06-03T08:24:17.642Z","comments":true,"path":"post/JAVA并发编程-10-协程/","link":"","permalink":"https://shang.at/post/JAVA并发编程-10-协程/","excerpt":"","text":"https://juejin.im/post/5ed62cabf265da7709526718","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"协程","slug":"协程","permalink":"https://shang.at/tags/协程/"}]},{"title":"JAVA并发编程-9-线程池","slug":"JAVA并发编程-9-线程池","date":"2020-06-03T02:52:35.000Z","updated":"2020-06-03T02:59:46.242Z","comments":true,"path":"post/JAVA并发编程-9-线程池/","link":"","permalink":"https://shang.at/post/JAVA并发编程-9-线程池/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"线程池","slug":"线程池","permalink":"https://shang.at/tags/线程池/"}]},{"title":"JAVA并发编程-8-阻塞队列","slug":"JAVA并发编程-8-阻塞队列","date":"2020-06-03T02:52:16.000Z","updated":"2020-06-03T02:59:37.175Z","comments":true,"path":"post/JAVA并发编程-8-阻塞队列/","link":"","permalink":"https://shang.at/post/JAVA并发编程-8-阻塞队列/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"阻塞队列","slug":"阻塞队列","permalink":"https://shang.at/tags/阻塞队列/"}]},{"title":"JAVA并发编程-7-Atomic","slug":"JAVA并发编程-7-Atomic","date":"2020-06-03T02:51:56.000Z","updated":"2020-06-03T02:59:29.958Z","comments":true,"path":"post/JAVA并发编程-7-Atomic/","link":"","permalink":"https://shang.at/post/JAVA并发编程-7-Atomic/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"Atomic","slug":"Atomic","permalink":"https://shang.at/tags/Atomic/"}]},{"title":"JAVA并发编程-6-并发集合","slug":"JAVA并发编程-6-并发集合","date":"2020-06-03T02:51:41.000Z","updated":"2020-06-03T02:59:22.524Z","comments":true,"path":"post/JAVA并发编程-6-并发集合/","link":"","permalink":"https://shang.at/post/JAVA并发编程-6-并发集合/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发集合","slug":"并发集合","permalink":"https://shang.at/tags/并发集合/"}]},{"title":"JAVA并发编程-5-其他","slug":"JAVA并发编程-5-其他","date":"2020-06-03T02:51:25.000Z","updated":"2020-06-03T02:59:14.785Z","comments":true,"path":"post/JAVA并发编程-5-其他/","link":"","permalink":"https://shang.at/post/JAVA并发编程-5-其他/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"ThreadLocal-Fork&Join","slug":"ThreadLocal-Fork-Join","permalink":"https://shang.at/tags/ThreadLocal-Fork-Join/"}]},{"title":"JAVA并发编程-4-并发工具类","slug":"JAVA并发编程-4-并发工具类","date":"2020-06-03T02:51:14.000Z","updated":"2020-06-03T02:58:42.622Z","comments":true,"path":"post/JAVA并发编程-4-并发工具类/","link":"","permalink":"https://shang.at/post/JAVA并发编程-4-并发工具类/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发工具类","slug":"并发工具类","permalink":"https://shang.at/tags/并发工具类/"}]},{"title":"JAVA并发编程-3-锁","slug":"JAVA并发编程-3-锁","date":"2020-06-03T02:51:06.000Z","updated":"2020-06-03T02:58:32.008Z","comments":true,"path":"post/JAVA并发编程-3-锁/","link":"","permalink":"https://shang.at/post/JAVA并发编程-3-锁/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"锁","slug":"锁","permalink":"https://shang.at/tags/锁/"}]},{"title":"JAVA并发编程-2-并发基础","slug":"JAVA并发编程-2-并发基础","date":"2020-06-03T02:50:55.000Z","updated":"2020-06-03T02:58:23.366Z","comments":true,"path":"post/JAVA并发编程-2-并发基础/","link":"","permalink":"https://shang.at/post/JAVA并发编程-2-并发基础/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发基础","slug":"并发基础","permalink":"https://shang.at/tags/并发基础/"}]},{"title":"JAVA并发编程-1-内存模型","slug":"JAVA并发编程-1-内存模型","date":"2020-06-03T02:50:11.000Z","updated":"2020-06-04T06:39:17.641Z","comments":true,"path":"post/JAVA并发编程-1-内存模型/","link":"","permalink":"https://shang.at/post/JAVA并发编程-1-内存模型/","excerpt":"","text":"","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"内存模型JMM","slug":"内存模型JMM","permalink":"https://shang.at/tags/内存模型JMM/"}]},{"title":"Spark应用-Scheduler","slug":"Spark应用-Scheduler","date":"2020-06-01T06:22:50.000Z","updated":"2020-07-01T09:11:10.004Z","comments":true,"path":"post/Spark应用-Scheduler/","link":"","permalink":"https://shang.at/post/Spark应用-Scheduler/","excerpt":"","text":"Spark任务有四种提交方式： local standalone yarn(这里着重讲) mesos 这里涉及到两层的任务调度： 第一层：schedule across applications，应用间的任务调度Spark的application提交到yarn平台，yarn平台负责Spark application的调度，这里也分为两层： 第一层：Yarn的队列，Spark application和其他运行在Yarn平台上的应用并无二致，都要统一服从yarn平台的安排yarn有三种任务调度模型： FIFO scheduler：先入先出调度器，整个Yarn集群只有一个任务队列，所有提交的任务都要等待上一个任务完全执行完才能执行 Capacity scheduler：容量调度器，以Capacity为中心，把资源划分到若干个队列中，各个队列内根据自己的逻辑分配资源。例如下图中队列A可以调度的资源可以占80%，队列B占有剩下的20%，各队列接受相应的作业请求，在自己的资源中分配 Fair scheduler：秉承公平性原则，尽可能让各个作业得到的资源平均。先提交的job1马上占满了集群资源，那么作业2提交之后，原本Job1占有的资源拨出一些给作业2，从而达到“公平”(但是要等到job1的某些task执行完毕之后才能把资源让出来) 第二层：Yarn队列内的调度当使用FIFO scheduler，自不必说，它只有一个先进先出的队列，也就是队列内部的任务调度；Capacity scheduler会把 集群分成若干个队列，每个队列内部采用FIFO的策略；Fair scheduler可以通过设置，每个Fair Queue内部使用不同的schedulingPolicy，但是会有一个文档级别的默认策略的配置defaultQueueSchedulingPolicy，如果每个Queue没有自己的设置，那么就用defaultQueueSchedulingPolicy 第二层：schedule within application，同一个SparkContext内的job调度谈Spark下并行执行多个Job的问题 一次 Spark SQL 性能提升10倍的经历 Spark调度（一）：Task调度算法，FIFO还是FAIR 理解YARN Scheduler","categories":[],"tags":[{"name":"Scheduler","slug":"Scheduler","permalink":"https://shang.at/tags/Scheduler/"}]},{"title":"分布式常见思想-Bloomfilter","slug":"分布式常见思想-Bloomfilter","date":"2020-06-01T01:58:27.000Z","updated":"2020-06-01T01:58:58.674Z","comments":true,"path":"post/分布式常见思想-Bloomfilter/","link":"","permalink":"https://shang.at/post/分布式常见思想-Bloomfilter/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"}],"tags":[{"name":"BloomFilter","slug":"BloomFilter","permalink":"https://shang.at/tags/BloomFilter/"}]},{"title":"数据结构与算法学习笔记-内存不足","slug":"数据结构与算法学习笔记-内存不足","date":"2020-05-31T23:35:34.000Z","updated":"2020-07-02T11:07:58.668Z","comments":true,"path":"post/数据结构与算法学习笔记-内存不足/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-内存不足/","excerpt":"","text":"中位数定义：数字排序之后，位于中间的那个数。比如将100亿个数字进行排序，排序之后，位于第50亿个位置的那个数 就是中位数。 ①内存够：内存够还慌什么啊，直接把100亿个全部排序了，你用冒泡都可以…然后找到中间那个就可以了。但是你以为面试官会给你内存？？ ②内存不够：题目说是整数，我们认为是带符号的int,所以4字节，占32位。 假设100亿个数字保存在一个大文件中，依次读一部分文件到内存(不超过内存的限制)，将每个数字用二进制表示，比较二进制的最高位(第32位，符号位，0是正，1是负)，如果数字的最高位为0，则将这个数字写入 file_0文件中；如果最高位为 1，则将该数字写入file_1文件中。 从而将100亿个数字分成了两个文件，假设 file_0文件中有 60亿 个数字，file_1文件中有 40亿 个数字。那么中位数就在 file_0 文件中，并且是 file_0 文件中所有数字排序之后的第 10亿 个数字。（file_1中的数都是负数，file_0中的数都是正数，也即这里一共只有40亿个负数，那么排序之后的第50亿个数一定位于file_0中） 现在，我们只需要处理 file_0 文件了（不需要再考虑file_1文件）。对于 file_0 文件，同样采取上面的措施处理：将file_0文件依次读一部分到内存(不超内存限制)，将每个数字用二进制表示，比较二进制的 次高位（第31位），如果数字的次高位为0，写入file_0_0文件中；如果次高位为1，写入file_0_1文件 中。 现假设 file_0_0文件中有30亿个数字，file_0_1中也有30亿个数字，则中位数就是：file_0_0文件中的数字从小到大排序之后的第10亿个数字。 抛弃file_0_1文件，继续对 file_0_0文件 根据 次次高位(第30位) 划分，假设此次划分的两个文件为：file_0_0_0中有5亿个数字，file_0_0_1中有25亿个数字，那么中位数就是 file_0_0_1文件中的所有数字排序之后的 第 5亿 个数。 按照上述思路，直到划分的文件可直接加载进内存时，就可以直接对数字进行快速排序，找出中位数了。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"外排-分组归并-桶排序","slug":"外排-分组归并-桶排序","permalink":"https://shang.at/tags/外排-分组归并-桶排序/"}]},{"title":"Java学习-Future","slug":"Java学习-Future","date":"2020-05-31T22:44:35.000Z","updated":"2020-06-03T09:32:37.246Z","comments":true,"path":"post/Java学习-Future/","link":"","permalink":"https://shang.at/post/Java学习-Future/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"并发编程-Future","slug":"并发编程-Future","permalink":"https://shang.at/tags/并发编程-Future/"}]},{"title":"Python学习-一些常见的操作","slug":"Python学习-一些常见的操作","date":"2020-05-29T09:15:13.000Z","updated":"2020-06-23T02:20:49.775Z","comments":true,"path":"post/Python学习-一些常见的操作/","link":"","permalink":"https://shang.at/post/Python学习-一些常见的操作/","excerpt":"","text":"数组 如何初始化一个一维数组 123n=10l = [0]*nl1 = [0 for _ in range(n)] 如何初始化一个二维数组 123m, n = 10, 7l = [[0]*m]*n # 会有赋值问题：n个[0]*m 实际上都是同一个对象l1 = [[0 for _ in range(m)] for _ in range(n)] # 没有赋值问题 如何初始化一个二维数组并且设置右边界和下边界为1(根据实际情况处理) 12m, n = 10, 7dp = [[1 if i == m - 1 or j == n - 1 else 0 for i in range(m)] for j in range(n)] 正序遍历 123n=10for i in range(n): print(i) 倒序遍历 123n=10for i in range(n-1, -1, -1): print(i) 二维数组一维化 1234567891011121314151617181920212223242526272829303132333435363738394041from itertools import chainb=[[1,2,3], [5,8], [7,8,9]]c=list(chain(*b))print(c)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------ab = [[1,2,3], [5,8], [7,8,9]]print([i for item in ab for i in item])[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------import operatorfrom functools import reducea = [[1,2,3], [4,6], [7,8,9,8]]print(reduce(operator.add, a))[1, 2, 3, 4, 6, 7, 8, 9, 8]### --------------------------------------------------------------------------------a = [[1,2,3], [5, 8], [7,8,9]]l=[]for m in range(0,3): for i in a[m]: l.append(i)print(l)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------a=[[1,2,3], [5,8], [7,8,9]]a= eval('['+str(a).replace(' ','').replace('[','').replace(']','')+']')print(a)[1, 2, 3, 5, 8, 7, 8, 9]### --------------------------------------------------------------------------------def flatten(a): if not isinstance(a, (list, )): return [a] else: b = [] for item in a: b += flatten(item)if __name__ == '__main__': a = [[[1,2],3],[4,[5,6]],[7,8,9]] print(flatten(a))[1, 2, 3, 4, 5, 6, 7, 8, 9]### -------------------------------------------------------------------------------- 如何拷贝一个一维数组 12x = [1,2,3,4]y = x[:] 如何拷贝一个二维数组 1234567891011121314151617181920212223### --------------------------------------------------------------------------------x=[[1,2,3], [4,6]]y = [row[:] for row in x]### --------------------------------------------------------------------------------x=[[1,2,3], [4,6]]from copy import copy, deepcopyy = deepcopy(x)### --------------------------------------------------------------------------------old_array = [[2, 3], [4, 5]]# python2.*new_array = map(list, old_array)# python3.*new_array = list(map(list, old_array))### --------------------------------------------------------------------------------arr = [[1,2],[3,4]]deepcopy1d2d = lambda lVals: [x if not isinstance(x, list) else x[:] for x in lVals]dst = deepcopy1d2d(arr)dst[1][1]=150print dstprint arr ascii码的转换12345# 获取一个字符的ascii码ord('a')# 将ascii码转换成字符chr(97) 内存12345678910# 内存地址a = 9id(a)# 查看一个对象内存占用大小(变量所占字节的大小)import syssys.getsizeof(a)# 查看变量类型type(a) 进制123456789101112131415161718192021222324252627282930# 二进制Ob1010# 获取一个数字的二进制&gt;&gt;&gt; bin(3)'0b11'&gt;&gt;&gt; bin(-10)'-0b1010'# 八进制0o176# 获取一个数字的二进制&gt;&gt;&gt; oct(3)'0o3'&gt;&gt;&gt; bin(-3)'-0o3'# 十进制43# 十六进制0x12abhex(3)&gt;&gt;&gt; '0x3'# 2 8 16 进制转换成 十进制int('101010', 2) # int('0b101010', 0)int('37621', 8) # int('0o37621', 0)int('23abcf', 16) # int('0x23abcf', 0) 位操作123456&gt;&gt; # 右移：向右移1位可以看成除以2&lt;&lt; # 左移：向左移一位可以看成乘以2&amp; # 与| # 或~ # 取反：效果是对n的内部表示的每一位求补^ # 异或 交换元素12345678910111213# 通常写法a=1b=2a,b = b,a# 数组元素交换l = [1,2,3,4,5]l[3], l[1] = l[1], l[3]# 但是要同时交换数组和index就不行了i=0i, l[i] = l[i], 7 # 失败：这里不能用这种写法，因为修改了i的值之后，等号前面的self.p[i]就会立刻指向修改后的i的位置。这里和普通的a b交换有区别l[i], i = 7, l[i] # 等号后面的l[i]实际上是值传递","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"常见操作技巧","slug":"常见操作技巧","permalink":"https://shang.at/tags/常见操作技巧/"}]},{"title":"Python学习-lru_cache","slug":"Python学习-lru-cache","date":"2020-05-28T01:48:49.000Z","updated":"2020-05-28T01:49:40.668Z","comments":true,"path":"post/Python学习-lru-cache/","link":"","permalink":"https://shang.at/post/Python学习-lru-cache/","excerpt":"","text":"","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"lru_cache","slug":"lru-cache","permalink":"https://shang.at/tags/lru-cache/"}]},{"title":"Java学习-Thread","slug":"Java学习-Thread","date":"2020-05-24T11:17:33.000Z","updated":"2020-06-12T01:57:34.925Z","comments":true,"path":"post/Java学习-Thread/","link":"","permalink":"https://shang.at/post/Java学习-Thread/","excerpt":"","text":"线程的基本概念进程：每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1–n个线程。（进程是资源分配的最小单位）线程：同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位） 线程的状态创建、就绪、运行、阻塞、终止 创建：新创建的一个线程对象 就绪：线程对象创建成功后，其他的线程调用该对象的start方法。该状态的线程位于可运行线程池内，变的可运行，等待获取CPU的使用权呢 运行：就绪状态的线程获取了CPU使用权，执行程序代码 阻塞：阻塞状态是线程因为某种原因放弃了CPU使用权，暂停运行。直到线程再次进入就绪状态，才有机会转到运行状态。阻塞的情况分为以下三种： 等待阻塞：运行的线程执行wait方法，JVM会把该线程放入等待池中(wait会释放持有的锁) 同步阻塞：运行的线程在获取对象同步锁的时，如果该同步锁被别的线程占用，则JVM会把改线程放入锁池中 其他阻塞：运行的线程执行sleep或join方法，或者发出了I/O(文件读写、网络请求)请求，JVM会把该线程置为阻塞状态。当sleep状态超时、join等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态(sleep不会释放持有的锁) 终止：线程执行完了或者因异常推出了run方法，该线程结束生命周期 线程创建和切换的代价：https://www.jianshu.com/p/ece1bb5fa88b","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"并发编程-Thread","slug":"并发编程-Thread","permalink":"https://shang.at/tags/并发编程-Thread/"}]},{"title":"Java学习-并发编程","slug":"Java学习-并发编程","date":"2020-05-24T10:46:42.000Z","updated":"2020-06-04T09:11:21.419Z","comments":true,"path":"post/Java学习-并发编程/","link":"","permalink":"https://shang.at/post/Java学习-并发编程/","excerpt":"","text":"整体脑图 并发编程领域可以抽象成三个核心问题：分工、同步和互斥 分工不同的线程负责不同的任务，可以并发的执行 同步互斥 JAVA并发编程-1-内存模型 JAVA并发编程-2-并发基础 JAVA并发编程-3-锁 JAVA并发编程-4-并发工具类 JAVA并发编程-5-ThreadLocal&amp;Fork&amp;Join JAVA并发编程-6-并发集合 JAVA并发编程-7-Atomic JAVA并发编程-8-阻塞队列 JAVA并发编程-9-线程池 JAVA并发编程-10-响应式编程-RxJava JAVA并发编程-12-并发框架-Disruptor","categories":[{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"}]},{"title":"Java学习-JVM","slug":"Java学习-JVM","date":"2020-05-24T10:42:22.000Z","updated":"2020-06-08T23:31:05.439Z","comments":true,"path":"post/Java学习-JVM/","link":"","permalink":"https://shang.at/post/Java学习-JVM/","excerpt":"","text":"转载地址 : JavaGuide 点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。 Java 内存区域详解如果没有特殊说明，都是针对的是 HotSpot 虚拟机。 写在前面 (常见面试题)基本问题 介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 拓展问题 String 类和常量池 8 种基本类型的包装类和常量池 一 概述对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 二 运行时数据区域Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8 之前： JDK 1.8 ： 线程私有的：不会带来并发问题 程序计数器 虚拟机栈 本地方法栈 线程共享的：在并发问题中主要解决的就是内存共享问题，其中最主要的是堆内存的管理 堆 方法区 直接内存 (非运行时数据区的一部分) 疑问：线程私有的内存占用是JVM管理的内存的哪一部分？ Xss参数是设定虚拟机中每个线程占用的栈内存大小，而虚拟机栈可分配的内存又跟物理机的内存大小、Java堆内存、方法区(JDK1.7及以前)等内存大小相关，其它的区分得的内存越大，虚拟机栈能够分得的内存就越小，并发的线程数量也就越小————这句话是对的吗？ 测试：单线程 启动参数 -Xss16m -Xms2m -Xmx2m，stack depth:415910 报了StackOverflowError 2.1 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.2 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种错误：StackOverFlowError 和 OutOfMemoryError。二者差异 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 错误。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 疑问：虚拟机栈到底是占用的哪块内存呢？JVM直接开辟的系统内存空间，还是会共享heap空间？由于每次创建线程，都会创建其对应的虚拟机栈，那么线程创建太多会带来极大的内存消耗代价(以及线程切换代价) 做了一个实验： ​ -Xss4m -Xms8m -Xmx8m：stack depth:215835 报了java.lang.StackOverflowError ​ -Xss16m -Xms8m -Xmx8m：stack depth:416734 报了java.lang.StackOverflowError ​ -Xss32m -Xms8m -Xmx8m：stack depth:835116 报了java.lang.StackOverflowError 从实验结果来看，限制了内存大小之后，改变虚拟机栈大小，无论把虚拟机栈设置多大，最终都是报的java.lang.StackOverflowError，是不是可以得出这样的结论，hotspot的设计中虚拟机栈使用的是独立于java堆内存的系统空间呢？ 2.3 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 2.4 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java世界中“几乎”所有的对象都在堆中分配，但是，随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从jdk 1.7开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 7 版本及JDK 7 版本之前，堆内存被通常被分为下面三部分： 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation) JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。 动态年龄计算的代码如下 123456789101112131415&gt; uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) &#123;&gt; //survivor_capacity是survivor空间的大小&gt; size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100);&gt; size_t total = 0;&gt; uint age = 1;&gt; while (age &lt; table_size) &#123;&gt; total += sizes[age];//sizes数组是每个年龄段对象大小&gt; if (total &gt; desired_survivor_size) break;&gt; age++;&gt; &#125;&gt; uint result = age &lt; MaxTenuringThreshold ? age : MaxTenuringThreshold;&gt; ...&gt; &#125;&gt; &gt; &gt;&gt; 堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如： OutOfMemoryError: GC Overhead Limit Exceeded ： 当JVM花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发java.lang.OutOfMemoryError: Java heap space 错误。(和本机物理内存无关，和你配置的内存大小有关！) …… 2.5 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 2.5.1 方法区和永久代的关系 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 2.5.2 常用参数JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 12-XX:PermSize=N //方法区 (永久代) 初始大小-XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： 12-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 2.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当你元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。 2.6 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 修正(issue747，reference)： JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代 。 JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace) 实验：以下三组参数分别运行同一段代码 123456789&gt; public static void main(String[] args) throws Throwable &#123;&gt; List&lt;String&gt; list = new ArrayList&lt;String&gt;();&gt; int i=0;&gt; while(true)&#123;&gt; list.add(String.valueOf(i++).intern());&gt; &#125;&gt; &gt; &#125;&gt; &gt; JDK1.6 -XX:PermSize=20M -XX:MaxPermSize=20M设置永久区大小，报OutOfMemoryError： PermGen space，说明字符串常量位于PermGen JDK1.7 -XX:PermSize=20M -XX:MaxPermSize=20M设置永久区大小，OutOfMemoryError： Java heap space，说明字符串常量位于了heap内 JDK1.8 -XX:MetaspaceSize=20M -XX:MaxMetaspaceSize=20M设置永久区大小，OutOfMemoryError： Java heap space，说明字符串常量还在heap内 相关问题：JVM 常量池中存储的是对象还是引用呢？： https://www.zhihu.com/question/57109429/answer/151717241 by RednaxelaFX 2.7 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。这就是网络编程中常说的Zero-copy(零拷贝) 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 三 HotSpot 虚拟机对象探秘通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。 3.1 对象的创建下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 Step1:类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 Step3:初始化零值内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 3.2 对象的内存布局在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3.3 对象的访问定位建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 四 重点补充内容4.1 String 类和常量池String 对象的两种创建方式： 12345String str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向\"abcd\"\"；String str2 = new String(\"abcd\");//堆中创建一个新的对象String str3 = new String(\"abcd\");//堆中创建一个新的对象System.out.println(str1==str2);//falseSystem.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。 再给大家一个图应该更容易理解，图片来源：https://www.journaldev.com/797/what-is-java-string-pool： String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7之前（不包含1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。 123456String s1 = new String(\"计算机\");String s2 = s1.intern();String s3 = \"计算机\";System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象，System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象 字符串拼接: 123456789String str1 = \"str\";String str2 = \"ing\"; String str3 = \"str\" + \"ing\";//常量池中的对象String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = \"string\";//常量池中的对象System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 4.2 String s1 = new String(“abc”);这句话创建了几个字符串对象？将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。 验证： 1234String s1 = new String(\"abc\");// 堆内存的地址值String s2 = \"abc\";System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。System.out.println(s1.equals(s2));// 输出 true 结果： 12falsetrue 4.3 8 种基本类型的包装类和常量池Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；前面 4 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，Character创建了数值在[0,127]范围的缓存数据，Boolean 直接返回True Or False。如果超出对应范围仍然会去创建新的对象。 为啥把缓存设置为[-128，127]区间？（参见issue/461）性能和资源之间的权衡。 123public static Boolean valueOf(boolean b) &#123; return (b ? TRUE : FALSE);&#125; 123456789private static class CharacterCache &#123; private CharacterCache()&#123;&#125; static final Character cache[] = new Character[127 + 1]; static &#123; for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); &#125; &#125; 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。** 123456789Integer i1 = 33;Integer i2 = 33;System.out.println(i1 == i2);// 输出 trueInteger i11 = 333;Integer i22 = 333;System.out.println(i11 == i22);// 输出 falseDouble i3 = 1.2;Double i4 = 1.2;System.out.println(i3 == i4);// 输出 false Integer 缓存源代码： 12345678/***此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。*/ public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 应用场景： Integer i1=40；Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。 Integer i1 = new Integer(40);这种情况下会创建新的对象。 123Integer i1 = 40;Integer i2 = new Integer(40);System.out.println(i1==i2);//输出 false Integer 比较更丰富的一个例子: 12345678910111213Integer i1 = 40;Integer i2 = 40;Integer i3 = 0;Integer i4 = new Integer(40);Integer i5 = new Integer(40);Integer i6 = new Integer(0);System.out.println(\"i1=i2 \" + (i1 == i2));System.out.println(\"i1=i2+i3 \" + (i1 == i2 + i3));System.out.println(\"i1=i4 \" + (i1 == i4));System.out.println(\"i4=i5 \" + (i4 == i5));System.out.println(\"i4=i5+i6 \" + (i4 == i5 + i6)); System.out.println(\"40=i5+i6 \" + (40 == i5 + i6)); 结果： 123456i1=i2 truei1=i2+i3 truei1=i4 falsei4=i5 falsei4=i5+i6 true40=i5+i6 true 解释： 语句 i4 == i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 == 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。 参考 《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 《实战 java 虚拟机》 https://docs.oracle.com/javase/specs/index.html http://www.pointsoftware.ch/en/under-the-hood-runtime-data-areas-javas-memory-model/ https://dzone.com/articles/jvm-permgen-%E2%80%93-where-art-thou https://stackoverflow.com/questions/9095748/method-area-and-permgen 深入解析String#internhttps://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.html 公众号如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。 《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 “Java面试突击” 即可免费领取！ Java工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://shang.at/tags/JVM/"}]},{"title":"Java学习-wait&notify","slug":"Java学习-wait-notify","date":"2020-05-22T05:09:37.000Z","updated":"2020-06-03T09:33:41.264Z","comments":true,"path":"post/Java学习-wait-notify/","link":"","permalink":"https://shang.at/post/Java学习-wait-notify/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"wait&notify","slug":"wait-notify","permalink":"https://shang.at/tags/wait-notify/"}]},{"title":"Java学习-Timer","slug":"Java学习-Timer","date":"2020-05-22T02:31:16.000Z","updated":"2020-06-03T09:33:28.680Z","comments":true,"path":"post/Java学习-Timer/","link":"","permalink":"https://shang.at/post/Java学习-Timer/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"Timer","slug":"Timer","permalink":"https://shang.at/tags/Timer/"}]},{"title":"Java学习-NIO","slug":"Java学习-NIO","date":"2020-05-19T18:46:53.000Z","updated":"2020-06-03T09:33:10.621Z","comments":true,"path":"post/Java学习-NIO/","link":"","permalink":"https://shang.at/post/Java学习-NIO/","excerpt":"","text":"Java NIO系列教程（六） 多路复用器Selector ServerSocketChannel：一个面向流的侦听套接字 通道。用于监听是否有新的连接到来，可以调用accept函数获取到来的连接(SocketChannel) SocketChannel：一个面向流的连接套接字 通道。用于从连接中读取数据，和向连接写入数据 Selector：选择器，可以管理一批注册的通道集合的信息和它们的就绪状态(OP_CONNECT|OP_ACCEPT|OP_READ|OP_WRITE) SelectionKey：选择键封装了特定的通道与特定的选择器的注册关系，一个key就代表了一个channel SelectableChannel可以被注册到Selector对象上，然后调用Selector.select()方法可以更新SelectionKeys(即处于某种就绪状态的通道集合)，然后我们就可以遍历这些通过处理相应的事件。比如连接就绪的通道，我们会把他们注册成读状态，等待它读就绪；连接读就绪，我们就可以从中读取数据；连接写就绪，我们就可以向连接写数据","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"NIO","slug":"NIO","permalink":"https://shang.at/tags/NIO/"}]},{"title":"Java学习-线程安全的集合类","slug":"Java学习-线程安全的集合类","date":"2020-05-17T15:16:15.000Z","updated":"2020-05-17T15:16:15.157Z","comments":true,"path":"post/Java学习-线程安全的集合类/","link":"","permalink":"https://shang.at/post/Java学习-线程安全的集合类/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Java学习-Iterator","slug":"Java学习-Iterator","date":"2020-05-17T07:27:13.000Z","updated":"2020-06-03T09:33:04.816Z","comments":true,"path":"post/Java学习-Iterator/","link":"","permalink":"https://shang.at/post/Java学习-Iterator/","excerpt":"","text":"在看Iterator之前，先看一个早期版本的迭代器java.util.Enumeration 1234public interface Enumeration&lt;E&gt; &#123; boolean hasMoreElements(); E nextElement();&#125; 1234* NOTE: The functionality of this interface is duplicated by the Iterator* interface. In addition, Iterator adds an optional remove operation, and* has shorter method names. New implementations should consider using* Iterator in preference to Enumeration. 现在来看Iterator： 12345678910111213141516public interface Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); default void remove() &#123; throw new UnsupportedOperationException(\"remove\"); &#125; default void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (hasNext()) action.accept(next()); &#125;&#125; 注意的点： Iterator在好的设计下可以在遍历的过程中对列表进行增加和删除和修改元素 Iterator在遍历的过程只能进行一遍，即遍历完的对象不能再次遍历， 因为大多数Iterator在实现的过程中都是维护了了cursor指针，这个指针一般只会增加，不会减少 同时大都没有充值cursor指针的接口 关键是看Iterator的设计如何 例如下面的ListIterator 1234567891011public interface ListIterator&lt;E&gt; extends Iterator&lt;E&gt; &#123; boolean hasNext(); E next(); boolean hasPrevious(); E previous(); int nextIndex(); int previousIndex(); void remove(); void set(E e); void add(E e);&#125; ListIterator在原来的Iterator的基础上扩展了，使之可以往前遍历，同时可以修改和增加元素","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA-Iterator","slug":"JAVA-Iterator","permalink":"https://shang.at/tags/JAVA-Iterator/"}]},{"title":"Java学习-函数式编程","slug":"Java学习-函数式编程","date":"2020-05-17T00:38:33.000Z","updated":"2020-06-03T09:32:49.333Z","comments":true,"path":"post/Java学习-函数式编程/","link":"","permalink":"https://shang.at/post/Java学习-函数式编程/","excerpt":"","text":"Java8之后加入了一种全新的方式来实现方法(功能)作为参数传递的机制：lambda表达式 像python语言，天生就支持将function作为参数传递给函数. 可以想象，既然是一种实现方法作为参数传递的机制，java是一种面向对象的编程语言，也就是说在java中除了原始数据类型之外，都是对象： 1234567891011&gt; @FunctionalInterface&gt; interface CharBinaryOperator &#123;&gt; String applyAsChar(char left, char right);&gt; &#125;&gt; &gt; CharBinaryOperator charBinaryOperator = (char a, char b) -&gt; &#123;&gt; System.out.println(a);&gt; System.out.println(b);&gt; return String.valueOf(a + b);&gt; &#125;;&gt; &gt; charBinaryOperator instanceof CharBinaryOperator charBinaryOperator是CharBinaryOperator的一个实例 CharBinaryOperator.class instanceof Class CharBinaryOperator.class是Class的一个实例 Class.class instanceof Class Class.class同时也是Class的一个实例 在java中，传递的参数要么是原始数据类型，要么是对象(类型也是对象，所以能够传递)，不能是其他的类型。在JDK8之前，要想将一个功能传递到函数内部(这一般会被称为函数回调，是大多数异步编程的常用套路：到达某个时间节点或满足某中情况触发一个操作)，那么就只能显示的先定义一个接口，然后创建一个实现了这个接口的类，然后再实例化这个类得到一个对象，最后将这个对象作为参数传入函数，函数内部调用对象实现的方法，如： 123456789CharBinaryOperator charBinaryOperator1 = new CharBinaryOperator() &#123; @Override public String applyAsChar(char a, char b) &#123; System.out.println(a); System.out.println(b); return String.valueOf(a + b); &#125;&#125;;charBinaryOperator.applyAsChar('a', 'b'); 在JDK8及之后，我们不需要再显示的做这一系列的事情(当然你这么做也不会有问题)。 在JDK8及之后，所有满足条件的interface都会被解释函数式接口，即都可以通过lambda表达式的形式代替上述流程 什么样的interface才算满足条件呢？ 只声明了一个未实现的函数的interface就可以。在JDK8及以后，interface中定义的函数也可以有默认的实现 在声明接口的时候，可以使用java.lang.FunctionalInterface注解，表示该interface是一个函数式接口(当然可以不加，compiler会自动判断) 如果在有多个未实现的函数的interface上加这个注解的时候，编译阶段就会报错：Multiple non-overriding abstract methods found in interface OOXX lambda表达式只不过是为了实现这个机制的一种解决方案，可以提高开发效率，同时隐藏了interface的定义细节，compilier完全是按照参数列表来推断当前的lambda表达式是和哪一个interface绑定的(compilier直接找到接口的定义，不是推断的)。如果没有预定义的，那么就会在编译期间报错，所以在java中lambda表达式的使用是有一定的限制的。 同时，在使用JDK预定义的操作时，在内部是调用了接口内定义的那个具体的函数的，所以对于开发者来说也是透明的，如列表的forEach()函数 12345678910111213&gt; default void forEach(Consumer&lt;? super T&gt; action) &#123;&gt; Objects.requireNonNull(action);&gt; for (T t : this) &#123;&gt; action.accept(t);&gt; &#125;&gt; &#125;&gt; // 我们在使用的时候是这样的&gt; List&lt;Integer&gt; integers = new ArrayList&lt;&gt;();&gt; integers.add(1);&gt; integers.forEach(i -&gt; &#123;&gt; System.out.println(i);&gt; &#125;);&gt; &gt;&gt;&gt; 本质上，还是要先有interface的定义(在JDK中已经预定义了大部分的interface，所以我们才不用自己手动定义，我上面的例子中就是一个没有被预定义的例子)，运行结果也是创建了一个实现了指定接口的对象，然后将对象作为参数传递给函数 如果我们完全脱离了JDK预定义的操作，那么我们就需要自己定义innterface，并且在我们使用该接口的地方显示的声明方法的使用，但是在外层传递方法参数的调用，我们仍可以使用简洁明了的lambda表达式，无论怎么说，lambda表达式的这种机制极大的方便了开发人员 Lambda 表达式和匿名类之间的区别 this 关键字。对于匿名类 this 关键字解析为匿名类，而对于 Lambda 表达式，this 关键字解析为包含写入 Lambda 的类。 JDK中预定义的interface在java.util.function可以看到全部的预定义的interface，以下四种是最有代表性的 Function R apply(T) Function compose(Function&lt;? super V, ? extends T&gt; before) Function andThen(Function&lt;? super R, ? extends V&gt; after) Function identity() Consumer void accept(T) Consumer&lt; T&gt; addThen(Consumer&lt;? super T&gt;) Predicate boolean test() Predicate&lt; T&gt; add(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; negate() Predicate&lt; T&gt; or(Predicate&lt;? super T&gt;) Predicate&lt; T&gt; isEqual(Object) Supplier T get() Lambda 表达式的例子1 线程初始化线程可以初始化如下： 123456789101112// Old waynew Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"Hello world\"); &#125;&#125;).start();// New waynew Thread( () -&gt; System.out.println(\"Hello world\")).start(); 2 事件处理事件处理可以用 Java 8 使用 Lambda 表达式来完成。以下代码显示了将 ActionListener 添加到 UI 组件的新旧方式： 123456789101112// Old waybutton.addActionListener(new ActionListener() &#123; @Override public void actionPerformed(ActionEvent e) &#123; System.out.println(\"Hello world\"); &#125;&#125;);// New waybutton.addActionListener( (e) -&gt; &#123; System.out.println(\"Hello world\");&#125;); 3 遍例输出（方法引用）输出给定数组的所有元素的简单代码。请注意，还有一种使用 Lambda 表达式的方式。 1234567891011// old wayList&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);for (Integer n : list) &#123; System.out.println(n);&#125;// 使用 -&gt; 的 Lambda 表达式list.forEach(n -&gt; System.out.println(n));// 使用 :: 的 Lambda 表达式list.forEach(System.out::println); 6.4 逻辑操作输出通过逻辑判断的数据。 123456789101112131415161718192021222324252627282930313233343536package com.wuxianjiezh.demo.lambda;import java.util.Arrays;import java.util.List;import java.util.function.Predicate;public class Main &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); System.out.print(\"输出所有数字：\"); evaluate(list, (n) -&gt; true); System.out.print(\"不输出：\"); evaluate(list, (n) -&gt; false); System.out.print(\"输出偶数：\"); evaluate(list, (n) -&gt; n % 2 == 0); System.out.print(\"输出奇数：\"); evaluate(list, (n) -&gt; n % 2 == 1); System.out.print(\"输出大于 5 的数字：\"); evaluate(list, (n) -&gt; n &gt; 5); &#125; public static void evaluate(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) &#123; for (Integer n : list) &#123; if (predicate.test(n)) &#123; System.out.print(n + \" \"); &#125; &#125; System.out.println(); &#125;&#125; 运行结果： 12345输出所有数字：1 2 3 4 5 6 7 不输出：输出偶数：2 4 6 输出奇数：1 3 5 7 输出大于 5 的数字：6 7 6.4 Stream API 示例java.util.stream.Stream接口 和 Lambda 表达式一样，都是 Java 8 新引入的。所有 Stream 的操作必须以 Lambda 表达式为参数。Stream 接口中带有大量有用的方法，比如 map() 的作用就是将 input Stream 的每个元素，映射成output Stream 的另外一个元素。 下面的例子，我们将 Lambda 表达式 x -&gt; x*x 传递给 map() 方法，将其应用于流的所有元素。之后，我们使用 forEach 打印列表的所有元素。 12345678910// old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);for(Integer n : list) &#123; int x = n * n; System.out.println(x);&#125;// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);list.stream().map((x) -&gt; x*x).forEach(System.out::println); 下面的示例中，我们给定一个列表，然后求列表中每个元素的平方和。这个例子中，我们使用了 reduce() 方法，这个方法的主要作用是把 Stream 元素组合起来。 12345678910111213// old wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = 0;for(Integer n : list) &#123; int x = n * n; sum = sum + x;&#125;System.out.println(sum);// new wayList&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7);int sum = list.stream().map(x -&gt; x*x).reduce((x,y) -&gt; x + y).get();System.out.println(sum);","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"}]},{"title":"Java学习-集合类","slug":"Java学习-集合类","date":"2020-05-16T23:27:00.000Z","updated":"2020-06-03T09:32:54.364Z","comments":true,"path":"post/Java学习-集合类/","link":"","permalink":"https://shang.at/post/Java学习-集合类/","excerpt":"","text":"[TOC] 总览 注：这里只列举了单一线线程使用的集合对象(Vector除外) JVAV中列表类集合，按照数据的存储方式可以分为两大类：基于数组和基于链表。两种方式各有好处，需要根据实际业务场景做出选择。 数组 优点 支持随机访问，给定下标的访问是O(1)的时间复杂度 缺点 内存必须是连续的，否则会申请空间失败 查找、插入、扩容、删除都是O(n)的时间复杂度 有容量的限制，增加节点时，可能会因为数组大小不够导致扩容，扩容的时间复杂度是O(n)的 使用注意 最好能够预估数据的最大容量，可以预先设计capacity，尽量避免扩容操作 但是在特别的使用场景下，基于数组的实现效率会更好，比如下面要说的ArrayDeque 具体实现 ArrayList Vector Stack ArrayDeque 链表 优点 内存不用是连续的 插入、删除都是O(1)的时间复杂度 没有容量的限制，按理说限制就是JVAV堆的大小限制 缺点 查找是O(n)的时间复杂度 使用 单独使用链表的时候，还挺少的，毕竟一个没有附加特性的链表结构，仅仅只能够做到新增和删除的时间复杂度为O(1)，但是查询却需要O(n)，并且还需要额外的空间存储链表结构。数组可以通过预估容量的方式尽量减少扩容的操作，对比发现，使用基于数组的集合性价比更高 具体实现 LinkList LinkList在定位低index个元素的时候，有个优化的点可以学习 123456789101112131415Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 总结：可以发现，基于数组和基于链表的集合实现方式，想要从他们中查询到具体的元素，时间复杂度都是O(n)的，这是因为，这里仅仅考虑了数据的存储方式，并没有额外的信息给出来，所以是没有办法加速查询的。要想实现加速查询，那么就必须在这基础上增加新的特性： 数组 有序性：可以借助有序性使用二分查找，把查找时间复杂度降到O(log n) 链表 建立树结构： 二叉搜索树：前序遍历就是正向排序，可以把查询的时间复杂度降到O(log n)，但是要维护二叉搜索，尽量保证他是平衡的(但是这个的时间复杂度是O(1)的) 堆：查找最大(最小)值是O(1)的时间复杂度 升维：比如跳表，就是在有序的链表上建立多级索引来实现加速查询的，可以把查找时间复杂度降到O(log n)，但是在新增和删除节点时需要维护多级索引(但是这个的时间复杂度是O(1)的) List 接口信息如下： Method Return Comment Insert add(E) boolean 向队列加入元素，如果空间不足，会触发扩容 Insert add(int, E) void 向指定位置插入元素，可能会抛IndexOutOfBoundsException Remove remove(Object) boolean 移除指定的元素，没有的话返回false，有的话返回true Remove remove(int) E 移除指定index的元素，可能会抛IndexOutOfBoundsException Examine get(int) E 返回指定index的元素，可能会抛IndexOutOfBoundsException Update set(int, E) E 更新指定index的元素，可能会抛IndexOutOfBoundsException 在List的源码中发现多处这样的代码： 12345678910111213// 只返回第一个遇到的o，当o为null的时候，o.equals会报错public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 详看扩容操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;// DEFAULT_CAPACITY=10 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // newCapacity = int(1.5*oldCapacity) int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; // 整形溢出：Integer.MAX_VALUE + 8&lt;0 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Queue 设计了一套支持队列操作的接口，如下： Method Return Comment Insert add(E) boolean 向队列添加一个元素，如果没有空间会抛出IllegalStateException Insert offer(E) boolean 向队列添加一个元素，如果没有空间会返回false Remove remove() E 移除并返回头结点，如果队列为空的话，会抛NoSuchElementException Remove poll() E 移除并返回头结点，如果队列为空的话，会返回null Examine element() E 返回头结点，如果队列为空的话，会抛NoSuchElementException Examine peek() E 返回头结点，如果队列为空的话，会返回null Deque Stack 总结：通常使用ArrayDeque来作为先进先出的Queue，后进先出的Stack","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JAVA集合类-列表","slug":"JAVA集合类-列表","permalink":"https://shang.at/tags/JAVA集合类-列表/"}]},{"title":"分布式服务框架-IPC&RPC","slug":"分布式服务框架-IPC-RPC","date":"2020-05-14T03:38:18.000Z","updated":"2020-06-02T22:35:24.864Z","comments":true,"path":"post/分布式服务框架-IPC-RPC/","link":"","permalink":"https://shang.at/post/分布式服务框架-IPC-RPC/","excerpt":"","text":"","categories":[{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"}],"tags":[{"name":"IPC&RPC","slug":"IPC-RPC","permalink":"https://shang.at/tags/IPC-RPC/"}]},{"title":"Java学习-动态代理","slug":"Java学习-动态代理","date":"2020-05-14T03:34:00.000Z","updated":"2020-06-03T03:01:02.693Z","comments":true,"path":"post/Java学习-动态代理/","link":"","permalink":"https://shang.at/post/Java学习-动态代理/","excerpt":"","text":"","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"动态代理","slug":"动态代理","permalink":"https://shang.at/tags/动态代理/"}]},{"title":"Mysql学习-事务和隔离级别","slug":"Mysql学习-事务和隔离级别","date":"2020-05-12T17:31:26.000Z","updated":"2020-06-25T01:16:15.635Z","comments":true,"path":"post/Mysql学习-事务和隔离级别/","link":"","permalink":"https://shang.at/post/Mysql学习-事务和隔离级别/","excerpt":"","text":"MYSQL事务和隔离级别一、事务事务是由一组SQL语句组成的逻辑处理单元，是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。事务具有以下4个属性，通常简称为事务的ACID属性: 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。 这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 二、并发一致性问题1、更新丢失(Lost Update)T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。 如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。 2、脏读一句话：事务B读取到了事务A已修改但尚未提交的的数据，还在这个数据基础上做了操作。此时，如果A事务回滚Rollback，B读取的数据无效，不符合一致性要求。 解决办法: 把数据库的事务隔离级别调整到 READ_COMMITTED T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 3、不可重复读(Non-Repeatable Reads) 在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 一句话：一个事务范围内两个相同的查询却返回了不同数据。 同时操作，事务1分别读取事务2操作时和提交后的数据，读取的记录内容不一致。不可重复读是指在同一个事务内，两个相同的查询返回了不同的结果。 解决办法: 如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。把数据库的事务隔离级别调整到REPEATABLE_READ T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4、幻读一个事务T1按相同的查询条件重新读取以前检索过的数据，却发现其他事务T2插入了满足其查询条件的新数据，这种现象就称为“幻读”。（和可重复读类似，但是事务 T2 的数据操作仅仅是插入和删除，不是修改数据，读取的记录数量前后不一致） 一句话：事务A 读取到了事务B提交的新增数据，不符合隔离性。 解决办法: 如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。把数据库的事务隔离级别调整到 SERIALIZABLE_READ。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 三、事务隔离级别“脏读”、”不可重复读”和”幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。 MYSQL常看当前数据库的事务隔离级别：show variables like &#39;tx_isolation&#39;; 1、读未提交 (Read Uncommitted)最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。 2、读已提交 (Read Committed)被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。 3、可重复读(Repeatable Read)所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。(引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。) MYSQL默认是REPEATABLE-READ。 4、串行化(Serializable)所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。 5、总结 读未提交: 一个事务还没提交时，它做的变更就能被别的事务看到。 读提交: 一个事务提交之后，它做的变更才会被其他事务看到。 可重复读 : 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化: 顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 四个级别逐渐增强，每个级别解决一个问题，事务级别越高，性能越差，大多数环境(Read committed 就可以用了) 隔离级别 读数据一致性 脏读 不可重复读 幻读 未提交读 最低级别 √ √ √ 提交读 语句级 × √ √ 可重复读 事务级 × × √ 可串行化 最高级别,事务级 × × × 参考","categories":[{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"}],"tags":[{"name":"mysql事务和隔离级别","slug":"mysql事务和隔离级别","permalink":"https://shang.at/tags/mysql事务和隔离级别/"}]},{"title":"Mysql学习-第三范式","slug":"Mysql学习-第三范式","date":"2020-05-12T17:19:15.000Z","updated":"2020-06-25T01:16:25.915Z","comments":true,"path":"post/Mysql学习-第三范式/","link":"","permalink":"https://shang.at/post/Mysql学习-第三范式/","excerpt":"","text":"第一范式(1NF) 数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性 符合1NF的关系中的每个属性都不可再分 有两点要求： schema定义：每个属性不可再分，即字段的含义要明确，同一个字段不应该有多于1个的含义 图中的这种schema在RDBMS中是不可能存在的，也就是无法创建的。可以改成如下的schema: 存储的数据：同一列中不能有多个值 图中的一个字段里面存了多个值，这种情况在RDBMS中是可以存在的，但是该字段是可再分的，应该。可以将数据分成多条存储，如下图 第二范式(2NF) 满足第一范式 没有部分依赖 在同一个表中，不能存在某些字段依赖一些键，而另一些字段依赖另外一些键 员工表的一个候选键是{id，mobile，deptNo}，而deptName依赖于deptNo，同样 name 依赖于 id，因此不是 2NF的。为了满足第二范式的条件，需要将这个表拆分成employee、dept、employee_dept、employee_mobile四个表 不满足2NF的表，可能存在的问题：修改异常、新增异常、删除异常 第三范式(3NF) 满足第二范式 没有传递依赖 在同一个表中，不要存在字段A依赖字段B，同时字段B依赖字段C，推导出来字段A间接依赖字段C的关系。 员工表的province、city、district依赖于zip，而zip依赖于id，换句话说，province、city、district传递依赖于id，违反了 3NF 规则。为了满足第三范式的条件，可以将这个表拆分成employee和zip两个表 但是这种关系也不是一定不能存在，视具体的业务而定吧 示例假设有一个名为employee的员工表，它有九个属性：id(员工编号)、name(员工名称)、mobile(电话)、zip(邮编)、province(省份)、city(城市)、district(区县)、deptNo(所属部门编号)、deptName(所属部门名称)、表总数据如下： id name mobile zip province city district deptNo deptName 101 张三 1391000000113910000002 100001 北京 北京 海淀区 D1 部门1 101 张三 1391000000113910000002 100001 北京 北京 海淀区 D2 部门2 102 李四 13910000003 200001 上海 上海 静安区 D3 部门3 103 王五 13910000004 510001 广东省 广州 白云区 D4 部门4 103 王五 13910000004 510001 广东省 广州 白云区 D5 部门 5 将上表改成满足第1范式，如下： id name mobile zip province city district deptNo deptName 101 张三 13910000001 100001 北京 北京 海淀区 D1 部门1 101 张三 13910000002 100001 北京 北京 海淀区 D1 部门1 101 张三 13910000001 100001 北京 北京 海淀区 D2 部门2 101 张三 13910000002 100001 北京 北京 海淀区 D2 部门2 102 李四 13910000003 200001 上海 上海 静安区 D3 部门3 103 王五 13910000004 510001 广东省 广州 白云区 D4 部门4 103 王五 13910000004 510001 广东省 广州 白云区 D5 部门5 仍存在的问题 修改异常：上表中张三、王五都有多条记录，因为他隶属于两个部门。如果我们要修改王五的地址，必修修改两行记录。假如一个部门得到了王五的新地址并进行了更新，而另一个部门没有，那么此时王五在表中会存在两个不同的地址，导致了数据不一致 新增异常：假如一个新员工假如公司，他正处于入职培训阶段，还没有被正式分配到某个部门，如果deptNo字段不允许为空，我们就无法向employee表中新增该员工的数据。 删除异常：假设公司撤销了D3部门，那么在删除deptNo为D3的行时，会将李四的信息也一并删除。因为他隶属于D3这一部门。 为了解决上面的问题，我们可以将上述表设计成满足3NF 在关系数据库模型设计中，一般需要满足第三范式的要求。如果一个表具有良好的主外键设计，就应该是满足3NF的表。规范化带来的好处是通过减少数据冗余提高更新数据的效率，同时保证数据完整性。然而，我们在实际应用中也要防止过度规范化的问题。规范化程度越高，划分的表就越多，在查询数据时越有可能使用表连接操作。而如果连接的表过多，会影响查询性能。关键的问题是要依据业务需求，仔细权衡数据查询和数据更新关系，指定最合适的规范化程度。不要为了遵循严格的规范化规则而修改业务需求。 参考","categories":[{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"}],"tags":[{"name":"mysql第三范式","slug":"mysql第三范式","permalink":"https://shang.at/tags/mysql第三范式/"}]},{"title":"Python学习-函数参数传递","slug":"Python学习-函数参数传递","date":"2020-04-19T04:00:49.000Z","updated":"2020-04-19T04:26:43.749Z","comments":true,"path":"post/Python学习-函数参数传递/","link":"","permalink":"https://shang.at/post/Python学习-函数参数传递/","excerpt":"","text":"在Python(估计也适用于其他的语言)中，函数参数的传递分为两类 值传递和引用传递，实际上这两类传递类型都是属于变量传值，即： 值传递：将实际参数值复制一份传递到函数内，这样在函数内对参数进行修改，就不会影响到原参数 引用传递：将实际参数的地址直接传递到函数内，那么在函数内对参数所进行的修改，将可能会影响到原参数 要注意的是，在函数内修改参数，实际上又分为两种情况(仅说引用传递)： 1、对参数(a)重新进行赋值操作(a=new_obj)，此时，实际上修改的已经不是传递给函数的最初的参数(a)了，它已经指向了其他的内存地址，这时再修改a，实际上就和之前的对象没有任何关系了 2、直接对a进行修改，比如说a.name=’sdd’，这时，原始的对象就会发生变化","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python学习","slug":"python学习","permalink":"https://shang.at/tags/python学习/"}]},{"title":"Python学习-OrderedDict","slug":"Python学习-OrderedDict","date":"2020-04-16T08:50:57.000Z","updated":"2020-07-01T09:20:30.458Z","comments":true,"path":"post/Python学习-OrderedDict/","link":"","permalink":"https://shang.at/post/Python学习-OrderedDict/","excerpt":"","text":"12from collections import OrderedDict # 记录插入顺序的dict，操作方式和dict一样。# 是基于dict和双端队列实现，可以用来实现LRUcache","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"OrderedDict","slug":"OrderedDict","permalink":"https://shang.at/tags/OrderedDict/"}]},{"title":"Python学习-bisect","slug":"Python学习-bisect","date":"2020-04-16T08:47:50.000Z","updated":"2020-07-01T09:20:11.928Z","comments":true,"path":"post/Python学习-bisect/","link":"","permalink":"https://shang.at/post/Python学习-bisect/","excerpt":"","text":"这个模块对有序列表提供了支持，使得他们可以在插入新数据仍然保持有序。对于长列表，如果其包含元素的比较操作十分昂贵的话，这可以是对更常见方法的改进 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192\"\"\"Bisection algorithms.\"\"\"def insort_right(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the right of the rightmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 a.insert(lo, x)insort = insort_right # backward compatibilitydef bisect_right(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt;= x, and all e in a[i:] have e &gt; x. So if x already appears in the list, a.insert(x) will insert just after the rightmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if x &lt; a[mid]: hi = mid else: lo = mid+1 return lobisect = bisect_right # backward compatibilitydef insort_left(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the left of the leftmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid a.insert(lo, x)def bisect_left(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt; x, and all e in a[i:] have e &gt;= x. So if x already appears in the list, a.insert(x) will insert just before the leftmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 if a[mid] &lt; x: lo = mid+1 else: hi = mid return lo# Overwrite above definitions with a fast C implementationtry: from _bisect import *except ImportError: pass","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"bisect","slug":"bisect","permalink":"https://shang.at/tags/bisect/"}]},{"title":"数据结构与算法学习笔记-查找算法","slug":"数据结构与算法学习笔记-查找算法","date":"2020-04-10T00:58:45.000Z","updated":"2020-04-10T01:00:39.177Z","comments":true,"path":"post/数据结构与算法学习笔记-查找算法/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-查找算法/","excerpt":"","text":"总结 查找算法 时间复杂度 二分查找 O(logn) O(logn)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"查找算法","slug":"查找算法","permalink":"https://shang.at/tags/查找算法/"}]},{"title":"Spark应用之import spark.implicits._","slug":"Spark应用之import-spark-implicits","date":"2020-03-25T09:10:54.000Z","updated":"2020-07-01T09:12:15.408Z","comments":true,"path":"post/Spark应用之import-spark-implicits/","link":"","permalink":"https://shang.at/post/Spark应用之import-spark-implicits/","excerpt":"","text":"在初期使用spark的时候，大家都会遇见一个很奇怪的写法import spark.implicits._ 这里面包含了四个关键字：import、spark、implicits、_ import和_实际上是Scala中包引入的写法，表示引入指定包内的所有成员 本文主要想记录一下另外两个关键字：spark、implicits 关键字一：spark关键字二：implicits","categories":[{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/categories/Spark应用/"}],"tags":[{"name":"Spark的scala隐式转换","slug":"Spark的scala隐式转换","permalink":"https://shang.at/tags/Spark的scala隐式转换/"}]},{"title":"hadoop源码学习","slug":"hadoop源码学习一","date":"2019-07-10T03:03:43.000Z","updated":"2020-06-23T10:19:50.217Z","comments":true,"path":"post/hadoop源码学习一/","link":"","permalink":"https://shang.at/post/hadoop源码学习一/","excerpt":"","text":"先导知识 JAVA-动态代理 IPC/RPC hadoop一共包含以下几个组成部分 hadoop-common fs io ipc：是hadoop的灵魂，连接了集群的每一个节点 hdfs fs NN：NameNode，NN server+filesystem manager DN：DataNode，管理数据block的存储 定时向NN汇报 yarn RM NM tools Hadoop 中的 IPC框架Server 流程图 线程模型 我们使用以下的代码，来创建一个IPC Server 123456 // Get RPC server for server side implementation server = new RPC.Builder(conf).setProtocol(TestRpcService.class) .setNumHandlers(4) // .setnumReaders(4) .setInstance(service).setBindAddress(ADDRESS).setPort(PORT).build();server.start() 启动Server，则会得到如下的线程模型 Client 流程图 线程模型","categories":[{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"}],"tags":[{"name":"Hadoop-IPC","slug":"Hadoop-IPC","permalink":"https://shang.at/tags/Hadoop-IPC/"}]},{"title":"Pandas-学习","slug":"Pandas-学习","date":"2019-06-11T01:53:35.000Z","updated":"2020-06-25T01:17:10.958Z","comments":true,"path":"post/Pandas-学习/","link":"","permalink":"https://shang.at/post/Pandas-学习/","excerpt":"","text":"","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://shang.at/tags/Pandas/"}]},{"title":"Python学习-时间处理","slug":"Python学习-时间处理","date":"2019-06-06T08:37:46.000Z","updated":"2019-08-03T02:17:49.584Z","comments":true,"path":"post/Python学习-时间处理/","link":"","permalink":"https://shang.at/post/Python学习-时间处理/","excerpt":"","text":"关于时间戳的几个概念时间戳，根据1970年1月1日00:00:00开始按秒计算的偏移量。时间元组（struct_time），包含9个元素。 1time.struct_time(tm_year=2017, tm_mon=10, tm_mday=1, tm_hour=14, tm_min=21, tm_sec=57, tm_wday=6, tm_yday=274, tm_isdst=0) 时间格式字符串，字符串形式的时间。time模块与时间戳和时间相关的重要函数 12345time.time() # 生成当前的时间戳，格式为10位整数的浮点数。time.strftime() # 根据时间元组生成时间格式化字符串。time.strptime() # 根据时间格式化字符串生成时间元组。time.strptime()与time.strftime()为互操作。time.localtime() # 根据时间戳生成当前时区的时间元组。time.mktime() # 根据时间元组生成时间戳。 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344import time##生成当前时间的时间戳，只有一个参数即时间戳的位数，默认为10位，输入位数即生成相应位数的时间戳，比如可以生成常用的13位时间戳def now_to_timestamp(digits = 10): time_stamp = time.time() digits = 10 ** (digits -10) time_stamp = int(round(time_stamp*digits)) return time_stamp##将时间戳规范为10位时间戳def timestamp_to_timestamp10(time_stamp): time_stamp = int (time_stamp* (10 ** (10-len(str(time_stamp))))) return time_stamp##将当前时间转换为时间字符串，默认为2017-10-01 13:37:04格式def now_to_date(format_string=\"%Y-%m-%d %H:%M:%S\"): time_stamp = int(time.time()) time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将10位时间戳转换为时间字符串，默认为2017-10-01 13:37:04格式def timestamp_to_date(time_stamp, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.localtime(time_stamp) str_date = time.strftime(format_string, time_array) return str_date##将时间字符串转换为10位时间戳，时间字符串默认为2017-10-01 13:37:04格式def date_to_timestamp(date, format_string=\"%Y-%m-%d %H:%M:%S\"): time_array = time.strptime(date, format_string) time_stamp = int(time.mktime(time_array)) return time_stamp##不同时间格式字符串的转换def date_style_transfomation(date, format_string1=\"%Y-%m-%d %H:%M:%S\",format_string2=\"%Y-%m-%d %H-%M-%S\"): time_array = time.strptime(date, format_string1) str_date = time.strftime(format_string2, time_array) return str_dateprint(now_to_date())print(timestamp_to_date(1506816572))print(date_to_timestamp('2017-10-01 08:09:32'))print(timestamp_to_timestamp10(1506816572546))print(date_style_transfomation('2017-10-01 08:09:32')) 结果为 1234515068362240002017-10-01 13:37:042017-10-01 08:09:3215068165721506816572","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"python中的时间处理","slug":"python中的时间处理","permalink":"https://shang.at/tags/python中的时间处理/"}]},{"title":"Spark学习笔记-Configuration","slug":"Spark学习笔记-Configuration","date":"2019-06-03T09:32:33.000Z","updated":"2020-07-01T09:10:09.964Z","comments":true,"path":"post/Spark学习笔记-Configuration/","link":"","permalink":"https://shang.at/post/Spark学习笔记-Configuration/","excerpt":"","text":"submit 参数 运行时可配置参数：在代码中使用spark.conf.set(‘’， ‘’)的方式设置。运行时设置的参数不会在WebUI中显示","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"Configuration","slug":"Configuration","permalink":"https://shang.at/tags/Configuration/"}]},{"title":"Python学习-队列","slug":"Python学习-队列","date":"2019-06-03T02:10:48.000Z","updated":"2020-07-01T09:21:20.616Z","comments":true,"path":"post/Python学习-队列/","link":"","permalink":"https://shang.at/post/Python学习-队列/","excerpt":"","text":"队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from queue import Queue #LILO队列q = Queue() #创建队列对象q.put(0) #在队列尾部插入元素q.put(1)q.put(2)print('LILO队列',q.queue) #查看队列中的所有元素print(q.get()) #返回并删除队列头部元素print(q.queue)from queue import LifoQueue #LIFO队列lifoQueue = LifoQueue()lifoQueue.put(1)lifoQueue.put(2)lifoQueue.put(3)print('LIFO队列',lifoQueue.queue)lifoQueue.get() #返回并删除队列尾部元素lifoQueue.get()print(lifoQueue.queue)from queue import PriorityQueue #优先队列priorityQueue = PriorityQueue() #创建优先队列对象priorityQueue.put(3) #插入元素priorityQueue.put(78) #插入元素priorityQueue.put(100) #插入元素print(priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.put(1) #插入元素priorityQueue.put(2) #插入元素print('优先级队列:',priorityQueue.queue) #查看优先级队列中的所有元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue)priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('删除后剩余元素',priorityQueue.queue) #删除后剩余元素priorityQueue.get() #返回并删除优先级最低的元素print('全部被删除后:',priorityQueue.queue) #查看优先级队列中的所有元素from collections import deque #双端队列dequeQueue = deque(['Eric','John','Smith'])print(dequeQueue)dequeQueue.append('Tom') #在右侧插入新元素dequeQueue.appendleft('Terry') #在左侧插入新元素print(dequeQueue)dequeQueue.rotate(2) #循环右移2次print('循环右移2次后的队列',dequeQueue)dequeQueue.popleft() #返回并删除队列最左端元素print('删除最左端元素后的队列：',dequeQueue)dequeQueue.pop() #返回并删除队列最右端元素print('删除最右端元素后的队列：',dequeQueue)","categories":[{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://shang.at/tags/队列/"}]},{"title":"Spark学习笔记-广播变量","slug":"Spark学习笔记-广播变量","date":"2019-05-28T08:19:03.000Z","updated":"2020-07-01T09:09:58.493Z","comments":true,"path":"post/Spark学习笔记-广播变量/","link":"","permalink":"https://shang.at/post/Spark学习笔记-广播变量/","excerpt":"","text":"Shared Variables通常，当在远程集群节点上执行传递给Spark操作（例如mapor reduce）的函数时，它将在函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上的变量的更新不会传播回驱动程序。支持跨任务的通用，读写共享变量效率低下。但是，Spark确实为两种常见的使用模式提供了两种有限类型的共享变量：广播变量和累加器。 Broadcast广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。 Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 广播变量是v通过调用从变量创建的SparkContext.broadcast(v)。广播变量是一个包装器v，可以通过调用该value 方法来访问它的值。下面的代码显示了这个： 12345&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;&gt;&gt;&gt; broadcastVar.value[1, 2, 3] 创建广播变量后，应该使用它来代替v群集上运行的任何函数中的值，这样v就不会多次传送到节点。此外，在v广播之后不应修改对象 ，以确保所有节点获得相同的广播变量值（例如，如果稍后将变量发送到新节点）。 Performance Tuning 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108== Physical Plan ==InMemoryTableScan [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075] +- InMemoryRelation [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_outstanding_amount_ex_dp90#6075], true, 10000, StorageLevel(disk, 1 replicas) +- *(34) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, ((coalesce(nanvl(total_disburse_amount#5203, null), 0.0) - cast(coalesce(total_repay_principal_amount#5974, 0) as double)) - coalesce(nanvl(total_write_off_principal#5986, null), 0.0)) AS total_outstanding_amount_ex_dp90#6075] +- SortMergeJoin [bill_create_date#4955], [write_off_date#4776], LeftOuter :- *(23) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203, total_repay_principal_amount#5974] : +- SortMergeJoin [bill_create_date#4955], [repay_date#5789], LeftOuter : :- *(6) Sort [bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(bill_create_date#4955, 200) : : +- *(5) Project [bill_create_date#4955, week_last_day#5015, month_last_day#5075, total_disburse_amount#5203] : : +- Window [sum(disburse_amount#5197) windowspecdefinition(1, bill_create_date#4955 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_disburse_amount#5203], [1], [bill_create_date#4955 ASC NULLS FIRST] : : +- *(4) Sort [1 ASC NULLS FIRST, bill_create_date#4955 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(1, 200) : : +- *(3) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[sum(cast(principal#615 as double))]) : : +- Exchange hashpartitioning(bill_create_date#4955, week_last_day#5015, month_last_day#5075, 200) : : +- *(2) HashAggregate(keys=[bill_create_date#4955, week_last_day#5015, month_last_day#5075], functions=[partial_sum(cast(principal#615 as double))]) : : +- *(2) Project [principal#615, cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) AS bill_create_date#4955, next_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), Sun) AS week_last_day#5015, last_day(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date)) AS month_last_day#5075] : : +- *(2) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : :- *(2) Project [id#392] : : : +- *(2) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : +- *(2) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : : +- *(1) Project [loan_id#609, principal#615, create_time#632L] : : +- *(1) Filter ((cast(from_utc_timestamp(cast(from_unixtime(cast((cast(create_time#632L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) &lt;= 18043) &amp;&amp; isnotnull(loan_id#609)) : : +- *(1) FileScan parquet [loan_id#609,principal#615,create_time#632L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id)], ReadSchema: struct&lt;loan_id:string,principal:string,create_time:bigint&gt; : +- *(22) Sort [repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(21) Project [repay_date#5789, total_repay_principal_amount#5974] : +- Window [sum(repay_principal_amount#5970) windowspecdefinition(1, repay_date#5789 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_repay_principal_amount#5974], [1], [repay_date#5789 ASC NULLS FIRST] : +- *(20) Sort [1 ASC NULLS FIRST, repay_date#5789 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(1, 200) : +- *(19) HashAggregate(keys=[repay_date#5789], functions=[sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- Exchange hashpartitioning(repay_date#5789, 200) : +- *(18) HashAggregate(keys=[repay_date#5789], functions=[partial_sum(CASE WHEN (isnull(write_off_date#4776) || (write_off_date#4776 &gt; repay_date#5789)) THEN repaid_principal#684 END)]) : +- *(18) Project [repaid_principal#684, write_off_date#4776, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repay_date#5789] : +- SortMergeJoin [loan_id#609], [loan_id#5672], LeftOuter : :- *(12) Sort [loan_id#609 ASC NULLS FIRST], false, 0 : : +- Exchange hashpartitioning(loan_id#609, 200) : : +- *(11) Project [loan_id#609, repaid_principal#684, repay_time#692] : : +- *(11) BroadcastHashJoin [id#608], [bill_id#670], Inner, BuildRight : : :- *(11) Project [id#608, loan_id#609] : : : +- *(11) BroadcastHashJoin [id#392], [loan_id#609], Inner, BuildRight : : : :- *(11) Project [id#392] : : : : +- *(11) Filter (status#397 IN (COMPLETED,CURRENT,LATE) &amp;&amp; isnotnull(id#392)) : : : : +- *(11) FileScan parquet [id#392,status#397] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [In(status, [COMPLETED,CURRENT,LATE]), IsNotNull(id)], ReadSchema: struct&lt;id:string,status:string&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true])) : : : +- *(7) Project [id#608, loan_id#609] : : : +- *(7) Filter (isnotnull(loan_id#609) &amp;&amp; isnotnull(id#608)) : : : +- *(7) FileScan parquet [id#608,loan_id#609] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(loan_id), IsNotNull(id)], ReadSchema: struct&lt;id:string,loan_id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(10) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(10) Filter ((isnotnull(rn#5382) &amp;&amp; (rn#5382 = 1)) &amp;&amp; (cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) &lt;= 18043)) : : +- Window [row_number() windowspecdefinition(bill_id#670, repay_time#692 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#5382], [bill_id#670], [repay_time#692 DESC NULLS LAST] : : +- *(9) Sort [bill_id#670 ASC NULLS FIRST, repay_time#692 DESC NULLS LAST], false, 0 : : +- Exchange hashpartitioning(bill_id#670, 200) : : +- *(8) Project [bill_id#670, repaid_principal#684, repay_time#692] : : +- *(8) Filter isnotnull(bill_id#670) : : +- *(8) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [IsNotNull(bill_id)], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt; : +- *(17) Sort [loan_id#5672 ASC NULLS FIRST], false, 0 : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[first(write_off_date#4705, true)]) : +- *(17) HashAggregate(keys=[loan_id#5672], functions=[partial_first(write_off_date#4705, true)]) : +- *(17) Project [loan_id#5672, write_off_date#4705] : +- *(17) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight : :- *(17) Project [loan_id#5672, write_off_date#4705, bill_id#4714] : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4718], LeftOuter, BuildRight : : :- *(17) Project [loan_id#5672, write_off_date#4705] : : : +- *(17) BroadcastHashJoin [loan_id#5672], [loan_id#4708], LeftOuter, BuildRight : : : :- *(17) HashAggregate(keys=[loan_id#5672], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- Exchange hashpartitioning(loan_id#5672, 200) : : : : +- *(13) HashAggregate(keys=[loan_id#5672], functions=[partial_min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : : +- *(13) Project [loan_id#5672, (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) AS is_write_off_bill#4594, date_add(cast(due_date#5686 as date), 91) AS write_off_date#4630] : : : : +- *(13) Filter (CASE WHEN isnotnull(CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 = OVERDUE) THEN datediff(18044, cast(due_date#5686 as date)) END WHEN isnotnull(CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END) THEN CASE WHEN (status#5676 IN (REBALANCED,REPAID) &amp;&amp; (cast(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date) as string) &gt; due_date#5686)) THEN datediff(cast(from_utc_timestamp(cast(from_unixtime(cast((cast(repay_time#5694L as double) / 1000.0) as bigint), yyyy-MM-dd HH:mm:ss, Some(UTC)) as timestamp), Asia/Ho_Chi_Minh) as date), cast(due_date#5686 as date)) END ELSE 0 END &gt;= 91) : : : : +- *(13) FileScan parquet [loan_id#5672,status#5676,due_date#5686,repay_time#5694L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;loan_id:string,status:string,due_date:string,repay_time:bigint&gt; : : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : : +- *(14) Project [id#392 AS loan_id#4708] : : : +- *(14) FileScan parquet [id#392] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string&gt; : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) : : +- *(15) Project [id#4717 AS bill_id#4714, loan_id#4718] : : +- *(15) FileScan parquet [id#4717,loan_id#4718] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,loan_id:string&gt; : +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) : +- *(16) FileScan parquet [bill_id#670] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string&gt; +- *(33) Sort [write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(32) Project [write_off_date#4776, total_write_off_principal#5986] +- Window [sum(write_off_principal#5982) windowspecdefinition(1, write_off_date#4776 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS total_write_off_principal#5986], [1], [write_off_date#4776 ASC NULLS FIRST] +- *(31) Sort [1 ASC NULLS FIRST, write_off_date#4776 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(1, 200) +- *(30) HashAggregate(keys=[write_off_date#4776], functions=[sum(write_off_principal#4779)]) +- Exchange hashpartitioning(write_off_date#4776, 200) +- *(29) HashAggregate(keys=[write_off_date#4776], functions=[partial_sum(write_off_principal#4779)]) +- SortAggregate(key=[loan_id#609], functions=[first(write_off_date#4705, true), first(amount#398, true), sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- SortAggregate(key=[loan_id#609], functions=[partial_first(write_off_date#4705, true), partial_first(amount#398, true), partial_sum(CASE WHEN (isnotnull(repayment_date#4760) &amp;&amp; (repayment_date#4760 &lt; write_off_date#4705)) THEN repaid_principal#684 ELSE 0 END)]) +- *(28) Sort [loan_id#609 ASC NULLS FIRST], false, 0 +- *(28) Project [loan_id#609, write_off_date#4705, amount#398, repaid_principal#684, cast(from_utc_timestamp(repay_time#692, Asia/Ho_Chi_Minh) as date) AS repayment_date#4760] +- *(28) BroadcastHashJoin [bill_id#4714], [bill_id#670], LeftOuter, BuildRight :- *(28) Project [loan_id#609, write_off_date#4705, amount#398, bill_id#4714] : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4718], LeftOuter, BuildRight : :- *(28) Project [loan_id#609, write_off_date#4705, amount#398] : : +- *(28) BroadcastHashJoin [loan_id#609], [loan_id#4708], LeftOuter, BuildRight : : :- *(28) HashAggregate(keys=[loan_id#609], functions=[min(CASE WHEN is_write_off_bill#4594 THEN write_off_date#4630 END)]) : : : +- ReusedExchange [loan_id#609, min#6101], Exchange hashpartitioning(loan_id#5672, 200) : : +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true])) : : +- *(25) Project [id#392 AS loan_id#4708, amount#398] : : +- *(25) FileScan parquet [id#392,amount#398] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:string,amount:string&gt; : +- ReusedExchange [bill_id#4714, loan_id#4718], BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[1, string, true])) +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) +- *(27) Project [bill_id#670, repay_time#692, repaid_principal#684] +- *(27) FileScan parquet [bill_id#670,repaid_principal#684,repay_time#692] Batched: true, Format: Parquet, Location: InMemoryFileIndex[...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;bill_id:string,repaid_principal:decimal(20,0),repay_time:timestamp&gt;","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"广播变量","slug":"广播变量","permalink":"https://shang.at/tags/广播变量/"}]},{"title":"数据结构学习笔记二-算法","slug":"数据结构学习笔记二-算法","date":"2019-05-16T09:54:29.000Z","updated":"2019-05-16T14:39:58.762Z","comments":true,"path":"post/数据结构学习笔记二-算法/","link":"","permalink":"https://shang.at/post/数据结构学习笔记二-算法/","excerpt":"","text":"递归二分查找哈希算法堆排序深度和广度优先搜索字符串匹配贪心算法分治算法回溯算法动态规划拓扑排序最短路径并行算法","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://shang.at/tags/算法/"}]},{"title":"Spark学习笔记-pivot透视图","slug":"Spark学习笔记-pivot透视图","date":"2019-05-09T02:52:07.000Z","updated":"2020-07-01T09:10:25.452Z","comments":true,"path":"post/Spark学习笔记-pivot透视图/","link":"","permalink":"https://shang.at/post/Spark学习笔记-pivot透视图/","excerpt":"","text":"12345678910df = spark.createDataFrame([ ('2018-01','项目1',100, 'xm'), ('2018-01','项目1',100, 'xl'), ('2018-01','项目1',100, 'xp'), ('2018-01','项目2',200, 'ch'), ('2018-01','项目3',300, 'xl'), ('2018-02','项目1',1000, 'xp'), ('2018-02','项目2',2000, 'xl'), ('2018-03','项目x',999, 'xm')], ['date','project','income', 'saler']) 1df.toPandas() date project income saler 0 2018-01 项目1 100 xm 1 2018-01 项目1 100 xl 2 2018-01 项目1 100 xp 3 2018-01 项目2 200 ch 4 2018-01 项目3 300 xl 5 2018-02 项目1 1000 xp 6 2018-02 项目2 2000 xl 7 2018-03 项目x 999 xm pivot1234567df_pivot = df.groupBy('date').pivot( 'project', ['项目1', '项目2', '项目3', '项目x']).agg( sum('income')).na.fill(0)df_pivot.toPandas() date 项目1 项目2 项目3 项目x 0 2018-03 0 0 0 999 1 2018-02 1000 2000 0 0 2 2018-01 300 200 300 0 12345df.groupBy('project').pivot( 'date').agg( sum('income')).na.fill(0).toPandas() project 2018-01 2018-02 2018-03 0 项目2 200 2000 0 1 项目x 0 0 999 2 项目1 300 1000 0 3 项目3 300 0 0 unpivot12345df_pivot.selectExpr(\"date\", \"stack(4, '项目11', `项目1`, '项目22', `项目2`, '项目33', `项目3`, '项目xx', `项目x`) as (`project`,`income`)\")\\ .filter(\"income &gt; 0 \")\\ .orderBy([\"date\", \"project\"])\\ .toPandas() date project income 0 2018-01 项目11 300 1 2018-01 项目22 200 2 2018-01 项目33 300 3 2018-02 项目11 1000 4 2018-02 项目22 2000 5 2018-03 项目xx 999 1stack(n, expr1, ..., exprk) 将k个[expr1, ..., exprk]拆解成n rows","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"pivot透视图","slug":"pivot透视图","permalink":"https://shang.at/tags/pivot透视图/"}]},{"title":"Spark学习笔记-tips","slug":"Spark学习笔记-tips","date":"2019-04-15T05:50:00.000Z","updated":"2020-07-22T10:19:18.118Z","comments":true,"path":"post/Spark学习笔记-tips/","link":"","permalink":"https://shang.at/post/Spark学习笔记-tips/","excerpt":"","text":"写spark dataframe的时候，最好用哪些字段就取哪些字段，否则spark会默认把所有字段都读进内存，如果进行cache操作，就会无故占用大量内存 尽量不要使用json类型的字段存储数据，因为json字符串会存储大量的无用数据，字段名，最好设计有效的struct结构体来存储 没有被明确select的字段依然可以作为filter的条件 获取周的第一天日期和当前日期位于周的第几天，周的第一天定义不同 周日 周一 Spark Shuffle spill (Memory) and (Disk) on SPARK UI? What do they mean? https://community.hortonworks.com/questions/202809/spark-shuffle-spill-memory.html 窗口函数会引起重分区吗？分区数(200)是固定的吗？ 1234567891011test_df = kreditpintar.spark.range(0, end=100, numPartitions=5).toDF(&apos;input&apos;)test_df.rdd.getNumPartitions() # 5test_1_df = test_df.withColumn(&apos;id&apos;, row_number().over(Window.partitionBy(lit(1)).orderBy(&apos;input&apos;)))test_1_df.rdd.getNumPartitions() # 200test_2_df = test_df.withColumn(&apos;id&apos;, monotonically_increasing_id())test_2_df.rdd.getNumPartitions() # 5 通过withColumn(‘group’, lit(‘aaaabbb’))添加的新列，不能最为后续的join操作的condition expression？ groupBy 和 窗口函数的实现原理 哪一个效率更高 groupby 、窗口函数、distinct三种方式去重 哪个效率高 1distinct&gt;groupby&gt;窗口函数 循环的去跑脚本，然后union每次循环的结果。 这样的使用 task可能会失败，需要优化 转化long列类型到时间戳，保留毫秒信息 12345a_df = spark.createDataFrame([[1556613225852]], ['a'])a_df.select((col('a')/1000.0).cast('timestamp')).toPandas()#CAST((a / 1000.0) AS TIMESTAMP)#0 2019-04-30 08:33:45.852 spark进行计算的过程中间检查数据没有问题，但是执行collect后出现数据不一致的情况(丢失数据和union后的数据重复)","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"Spark-tips","slug":"Spark-tips","permalink":"https://shang.at/tags/Spark-tips/"}]},{"title":"BI工具使用之Tableau一","slug":"BI工具使用之Tableau一","date":"2019-04-11T07:08:39.000Z","updated":"2019-05-12T00:22:00.389Z","comments":true,"path":"post/BI工具使用之Tableau一/","link":"","permalink":"https://shang.at/post/BI工具使用之Tableau一/","excerpt":"","text":"","categories":[{"name":"BI","slug":"BI","permalink":"https://shang.at/categories/BI/"}],"tags":[{"name":"Tableau","slug":"Tableau","permalink":"https://shang.at/tags/Tableau/"}]},{"title":"Spark学习笔记-DSL语法","slug":"Spark学习笔记-DSL语法","date":"2019-03-31T02:20:06.000Z","updated":"2020-07-01T09:10:14.978Z","comments":true,"path":"post/Spark学习笔记-DSL语法/","link":"","permalink":"https://shang.at/post/Spark学习笔记-DSL语法/","excerpt":"","text":"","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"sparkSql-DSL语法","slug":"sparkSql-DSL语法","permalink":"https://shang.at/tags/sparkSql-DSL语法/"}]},{"title":"数据结构与算法学习笔记-排序算法","slug":"数据结构与算法学习笔记-排序算法","date":"2019-03-29T00:49:58.000Z","updated":"2020-06-29T17:51:40.125Z","comments":true,"path":"post/数据结构与算法学习笔记-排序算法/","link":"","permalink":"https://shang.at/post/数据结构与算法学习笔记-排序算法/","excerpt":"","text":"算法分类十种常见排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 算法复杂度 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数 参考 O($n^2$)冒泡排序(Bubble Sort) 算法描述 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否符合大小关系要求。如果不满足就互换位置。一次冒泡至少会让一个元素移动到它应该在的位置，重复n次，就完成了n个元素的排序工作。 算法实现 12345678910111213141516171819202122232425262728def bubble_sort(nums): \"\"\" 冒泡排序：从小到大 :param nums: :return: \"\"\" if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(i + 1, len(nums)): if nums[i] &gt; nums[j]: nums[i], nums[j] = nums[j], nums[i] return nums def bubble_sort1(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): for j in range(len(nums) - 1 - i): if nums[j] &gt; nums[j + i]: nums[j], nums[j + 1] = nums[j + 1], nums[j] return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34] print(bubble_sort(nums)) 插入排序 算法描述 将一个元素插入一个已经有序的序列，使其依然有序。首先，将原始的序列分为两个子序列，有序的和无序的，然后，从无序的序列中依次拿出一个元素，插入到有序的序列的合适位置，并保持有序的序列依然有序，直到无序的序列中没有元素了。 算法实现 1234567891011121314151617def insert_sort(nums): if len(nums) &lt;= 1: return nums for i in range(1, len(nums)): # 遍历无序数组的每一个元素 tmp = nums[i] # 待插入元素 j = i - 1 # 待插入子数组 for j in range(i - 1, -1, -1): # 从后往前遍历待插入子数组 if tmp &gt;= nums[j]: break # tmp大于等于当前元素，停止遍历 # 相等元素不会改变其相对位置，故是稳定的 nums[j + 1] = nums[j] # 将nums[j]后移1个位置 nums[j + 1] = tmp # 插入待插入元素 tmp return nums if __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(insert_sort(nums)) 选择排序 算法描述 选择排序是选择无序序列中的最小的元素放到有序序列的末尾，直到无序序列没有元素。 算法实现 123456789101112131415161718def selection_sort(nums): if len(nums) &lt;= 1: return nums for i in range(len(nums) - 1): # 遍历无序数组的每一个元素 # i和nums[i] min_val = nums[i] min_j = i for j in range(i + 1, len(nums)): # 寻找剩余待排数组的最小元素 if min_val &gt; nums[j]: min_val = nums[j] min_j = j nums[i], nums[min_j] = nums[min_j], nums[i] # 交换最小元素和 return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print(selection_sort(nums)) 希尔排序 算法描述 123456希尔排序是对插入排序的优化。希尔排序，通过将原始序列按照一定的步长划分为多个子序列 将原始的一维数组映射成二维数组， 然后按列进行插入排序，这样的话，可以让一个元素在一次比较中跨越较大的区间，随后算法在使用较小的步长，一直到步长为1(已知当对有序度较高数组进行排序时，插入排序的时间复杂度接近O(N)，因此可以大幅度提高插入排序的效率)。 维基百科：https://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F 算法实现 12345678910111213141516171819202122232425262728293031323334353637383940414243def shell_sort(list): n = len(list) # 初始步长 gap = n // 2 while gap &gt; 0: print(gap) for i in range(gap, n): # 每个步长进行插入排序 temp = list[i] j = i # 插入排序 while j &gt;= gap and list[j - gap] &gt; temp: list[j] = list[j - gap] j -= gap print('inner=', list) list[j] = temp print(list) # 得到新的步长 gap = gap // 2 return listdef shell_sort1(collection): # Marcin Ciura's gap sequence gaps = [701, 301, 132, 57, 23, 10, 4, 1] for gap in gaps: i = gap while i &lt; len(collection): temp = collection[i] j = i while j &gt;= gap and collection[j - gap] &gt; temp: collection[j] = collection[j - gap] j -= gap collection[j] = temp i += 1 return collectionif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', shell_sort1(nums)) O($nlogn$)归并排序 算法描述 将数组分为两部分，分别排序，最后将两部分排好序的数组合并成一个有序的数组。利用递归的方式，重复上述过程。 归并排序为什么会高效：合并两个有序数组的时间复杂度是O(N)的，将一个完整的数组递归的一分为二，那么分解到数组中只有一个元素时，分解的次数为logn，故整体的时间复杂度为O(nlogn)。 算法实现 1234567891011121314151617181920212223242526272829303132333435363738def merge_sort(nums): print('before=', nums) length = len(nums) if length &gt; 1: midpoint = length // 2 left_half = merge_sort(nums[:midpoint]) right_half = merge_sort(nums[midpoint:]) i = 0 j = 0 k = 0 left_length = len(left_half) right_length = len(right_half) while i &lt; left_length and j &lt; right_length: if left_half[i] &lt; right_half[j]: nums[k] = left_half[i] i += 1 else: nums[k] = right_half[j] j += 1 k += 1 while i &lt; left_length: nums[k] = left_half[i] i += 1 k += 1 while j &lt; right_length: nums[k] = right_half[j] j += 1 k += 1 print('after=', nums) return numsif __name__ == '__main__': nums = [-23, 0, 6, -4, 34, 2] print('\\n', merge_sort(nums)) 1234567891011121314151617181920212223def merge_sort(nums, left, right): if left &gt;= right: return mid = (left + right) &gt;&gt; 1 # 二分 merge_sort(nums, left, mid) # 排列左半部分 merge_sort(nums, mid + 1, right) # 排列右半部分 merge(nums, left, mid, right) # 合并左右两个已经有序的数组def merge(nums, left, mid, right): tmp = [0] * (right - left + 1) i, j, k = left, mid + 1, 0 while i &lt;= mid and j &lt;= right: if nums[i] &lt; nums[j]: tmp[k], i = nums[i], i + 1 else: tmp[k], j = nums[j], j + 1 k += 1 while i &lt;= mid: tmp[k], k, i = nums[i], k + 1, i + 1 while j &lt;= right: tmp[k], k, j = nums[j], k + 1, j + 1 nums[left:right+1] = tmp[:]nums = [-23, 0, 6, -4, 34, 2]merge_sort(nums, 0, len(nums) - 1)print(nums) 12345678910111213141516171819202122232425def merge_sort(nums, left, right): if left &gt;= right: return mid = (left + right) &gt;&gt; 1 # 二分 merge_sort(nums, left, mid) # 排列左半部分 merge_sort(nums, mid + 1, right) # 排列右半部分 merge(nums, left, mid, right) # 合并左右两个已经有序的数组def merge(nums, left, mid, right): tmp = [] i, j = left, mid + 1 while i &lt;= mid and j &lt;= right: if nums[i] &lt; nums[j]: tmp.append(nums[i]) i += 1 else: tmp.append(nums[j]) j += 1 tmp.extend(nums[i:mid + 1] if i &lt;= mid else nums[j:right + 1]) nums[left:right + 1] = tmp[:]nums = [-23, 88, 4, 2, 99, 12, 0, 6, -4, 34, 2]merge_sort(nums, 0, len(nums) - 1)print(nums) 123456789101112131415161718192021222324252627282930313233343536373839404142// Javapublic static void mergeSort(int[] array, int left, int right) &#123; if (right &lt;= left) return; int mid = (left + right) &gt;&gt; 1; // (left + right) / 2 mergeSort(array, left, mid); mergeSort(array, mid + 1, right); merge(array, left, mid, right);&#125;public static void merge(int[] arr, int left, int mid, int right) &#123; int[] temp = new int[right - left + 1]; // 中间数组 int i = left, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= right) &#123; temp[k++] = arr[i] &lt;= arr[j] ? arr[i++] : arr[j++]; &#125; while (i &lt;= mid) temp[k++] = arr[i++]; while (j &lt;= right) temp[k++] = arr[j++]; for (int p = 0; p &lt; temp.length; p++) &#123; arr[left + p] = temp[p]; &#125; // 也可以用 System.arraycopy(a, start1, b, start2, length)&#125;public static void merge(int[] arr, int left, int mid, int right) &#123; int[] temp = new int[right - left + 1]; // 中间数组 int i = left, j = mid + 1, k = 0; while (i &lt;= mid &amp;&amp; j &lt;= right) &#123; temp[k++] = arr[i] &lt;= arr[j] ? arr[i++] : arr[j++]; &#125; if (i&lt;=mid) System.arraycopy(arr, i, temp, k, mid-i+1); if (j&lt;=right) System.arraycopy(arr, j, temp, k, right-j+1); System.arraycopy(temp, 0, arr, left, temp.length);&#125; 快速排序 算法描述 随机选择一个pivot节点，然后将数组中的数据分成大于pivot和小于pivot的两部分，然后递归地将大于pivot和小于pivot的部分再按照相同的思路处理，直到每个pivot两端的部分都只有最多一个元素 算法实现 123456789def quick_sort(collection): length = len(collection) if length &lt;= 1: return collection else: pivot = collection[0] greater = [element for element in collection[1:] if element &gt; pivot] lesser = [element for element in collection[1:] if element &lt;= pivot] return quick_sort(lesser) + [pivot] + quick_sort(greater) O(n) 时间复杂度内求无序数组中的第 K 大元素 1234567891011121314151617# 选择数组的最后一个元素，作为pivot，然后将数组的所有元素分为大于pivot和小于pivot的两部分，# 如果 len(lesser) == k - 1，则返回pivot# 如果 len(lesser) &gt;= k，则说明要查找的元素在小于pivot的部分，那么继续在lesser中查找# 否则的话，说明要查找的元素在大于pivot的部分，那么继续在greater中查找def find_k_max(nums, k): length = len(nums) if length &lt; k: return None pivot = nums[length - 1] greater = [element for element in nums[:length - 1] if element &gt; pivot] lesser = [element for element in nums[:length - 1] if element &lt;= pivot] if len(lesser) == k - 1: return pivot elif len(lesser) &gt;= k: return find_k_max(lesser, k) else: return find_k_max(greater, k - len(lesser) - 1) In-place算法实现 1234567891011121314151617181920212223def quick_sort(nums, begin, end): if begin&gt;=end: return pivot = partition(nums, begin, end) # 找到[begin,end]的pivot点，pivot点已经固定了，是不需要对它进行排序的 quick_sort(nums, begin, pivot-1) # 递归调用nums[begin, pivot-1] quick_sort(nums, pivot+1, end) # 递归调用nums[pivot+1, end] return numsdef partition(nums, begin, end): # pivot是作为对比的值 # last_smaller是指向小于pivot的子数组的下一个位置的指针 类似于leetcode [移动零] 那一题的解法 pivot, last_smaller = end, begin for i in range(begin, end): # 从前往后遍历待排数组 if nums[i]&lt;nums[pivot]: # 如果当前元素小于pivot，那么调换当前元素和last_smaller位置的元素 nums[i], nums[last_smaller] = nums[last_smaller], nums[i] last_smaller+=1 # last_smaller 向后移动一位 # 把pivot的值移动到last_smaller位置 nums[last_smaller], nums[pivot] = nums[pivot], nums[last_smaller] return last_smaller nums = []quick_sort(nums, 0, len(nums)-1) 123456789101112131415161718192021// Javapublic static void quickSort(int[] array, int begin, int end) &#123; if (end &lt;= begin) return; int pivot = partition(array, begin, end); quickSort(array, begin, pivot - 1); quickSort(array, pivot + 1, end);&#125;static int partition(int[] a, int begin, int end) &#123; // pivot: 标杆位置 // last_smaller是指向小于pivot的子数组的下一个位置的指针 类似于leetcode [移动零] 那一题的解法 int pivot = end, last_smaller = begin; for (int i = begin; i &lt; end; i++) &#123; if (a[i] &lt; a[pivot]) &#123; int temp = a[last_smaller]; a[last_smaller] = a[i]; a[i] = temp; last_smaller++; &#125; &#125; int temp = a[pivot]; a[pivot] = a[last_smaller]; a[last_smaller] = temp; return counter;&#125; 堆排序 算法描述 使用大顶堆和小顶堆的数据结构，进行排序：大顶堆，每次取堆顶元素，遍历完则得到从大到小的序列；小顶堆则相反。 堆有很多种实现，这里只看二叉堆。二叉堆是一种近似完全二叉树，所以可以使用数组存储，每个父亲节点都要比其孩子节点大(大顶堆，小顶堆相反) 利用堆进行排序，包含两个步骤：第一、将数据放入堆中以满足堆的条件；第二、将数据从堆中取出。即为有序数组 算法实现 1234567891011121314151617181920212223242526#Pythondef heapify(parent_index, length, nums): temp = nums[parent_index] child_index = 2*parent_index+1 # 取到左孩子 while child_index &lt; length: # 先检查右孩子，看右孩子是不是比左孩子大，是的话更新孩子节点索引到 右孩子 if child_index+1 &lt; length and nums[child_index+1] &gt; nums[child_index]: child_index = child_index+1 # 如果父结点大于孩子节点，那么停止循环 if temp &gt; nums[child_index]: break # 将父结点更新到新检查的孩子节点，重复上面的操作 nums[parent_index] = nums[child_index] parent_index = child_index child_index = 2*parent_index + 1 nums[parent_index] = temp # 最后，把父结点元素归位def heapsort(nums): ## start from (len(nums)-2)//2:看一下图就知道了 for i in range((len(nums)-2)//2, -1, -1): heapify(i, len(nums), nums) for j in range(len(nums)-1, 0, -1): nums[j], nums[0] = nums[0], nums[j] heapify(0, j, nums) 当len=12，那么从数组下标为5开始调整，图中的6位置 当len=11，那么从数组下标为4开始调整，图中的5位置 依次类推 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// Javastatic void heapify(int[] array, int length, int i) &#123; int left = 2 * i + 1, right = 2 * i + 2； int largest = i; if (left &lt; length &amp;&amp; array[left] &gt; array[largest]) &#123; largest = left; &#125; if (right &lt; length &amp;&amp; array[right] &gt; array[largest]) &#123; largest = right; &#125; if (largest != i) &#123; int temp = array[i]; array[i] = array[largest]; array[largest] = temp; heapify(array, length, largest); &#125;&#125;public static void heapSort(int[] array) &#123; if (array.length == 0) return; int length = array.length; for (int i = length / 2-1; i &gt;= 0; i--) heapify(array, length, i); for (int i = length - 1; i &gt;= 0; i--) &#123; int temp = array[0]; array[0] = array[i]; array[i] = temp; heapify(array, i, 0); &#125;&#125;// 数组中的第K个最大元素class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; int heapSize = nums.length; buildMaxHeap(nums, heapSize); for (int i = nums.length - 1; i &gt;= nums.length - k + 1; --i) &#123; swap(nums, 0, i); --heapSize; maxHeapify(nums, 0, heapSize); &#125; return nums[0]; &#125; public void buildMaxHeap(int[] a, int heapSize) &#123; for (int i = heapSize / 2; i &gt;= 0; --i) &#123; maxHeapify(a, i, heapSize); &#125; &#125; public void maxHeapify(int[] a, int i, int heapSize) &#123; int l = i * 2 + 1, r = i * 2 + 2, largest = i; if (l &lt; heapSize &amp;&amp; a[l] &gt; a[largest]) &#123; largest = l; &#125; if (r &lt; heapSize &amp;&amp; a[r] &gt; a[largest]) &#123; largest = r; &#125; if (largest != i) &#123; swap(a, i, largest); maxHeapify(a, largest, heapSize); &#125; &#125; public void swap(int[] a, int i, int j) &#123; int temp = a[i]; a[i] = a[j]; a[j] = temp; &#125;&#125; O($n$)计数排序 算法描述 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 步骤 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 使用的局限性： 待排序的数组元素只能为有确定范围的整数，因为要使用数组的索引来标识元素的顺序 有时候，小数和负数，可以通过乘以一个倍数或者加上一个正数，来调整成可以使用计数排序的形式 待排序数组的范围不能太大，否则会占用大量的内存，如果数据倾斜严重，可能只会使用到一小部分的位置，会造成大量的内存白白消耗 升级： 可以使用下面要讲的桶排序，来弥补计数排序的两点不足 算法实现 1234567891011121314151617max_value = 1000def count_sort(nums): bucket = [0] * max_value for num in nums: bucket[num] += 1 j = 0 for i in range(max_value): nums[j:j + bucket[i]] = ([i] * bucket[i])[:] j += bucket[i] return numsnums = [9, 2, 3, 1, 12, 3, 0, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 56, 6, 7, 7, 78, 8, 8, 8, 8]count_sort(nums)print(nums) 基数排序 算法描述 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 步骤 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。 算法实现 1234567891011121314151617181920def radix_sort(nums, max_digit): \"\"\" max_digit 表示最大的位数 \"\"\" bucket = [[] for _ in range(10)] # 十进制数每位只能有10个选择：0-9 for digit in range(max_digit): # 从低位开始分桶 for num in nums: # 对每个数整除10的倍数 取余，填入对一个的坑位 bucket[(num // (10 ** digit)) % 10].append(num) j = 0 for i in range(10): # 遍历10个坑位，把数据回填入nums nums[j:j + len(bucket[i])] = bucket[i][:] j += len(bucket[i]) bucket = [[] for _ in range(10)] # 一轮结束后，清空原来的bucket return numsres = radix_sort([32, 43, 4, 54, 24, 1, 34, 3, 1, 13, 4, 4, 134, 4, 53, 34, 1, 34, 3, 3, 4, 2, 1, 4, ], 3)print(res) 桶排序 算法描述 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大 算法实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344import mathfrom TheAlgorithms.sorts.insert_sort import insert_sort1DEFAULT_BUCKET_SIZE = 5def bucket_sort(my_list, bucket_size=DEFAULT_BUCKET_SIZE): if len(my_list) == 0: print('You don\\'t have any elements in array!') minValue = my_list[0] maxValue = my_list[0] # For finding minimum and maximum values for i in range(0, len(my_list)): if my_list[i] &lt; minValue: minValue = my_list[i] elif my_list[i] &gt; maxValue: maxValue = my_list[i] # Initialize buckets bucketCount = math.floor((maxValue - minValue) / bucket_size) + 1 buckets = [] for i in range(0, bucketCount): buckets.append([]) # For putting values in buckets for i in range(0, len(my_list)): buckets[math.floor((my_list[i] - minValue) / bucket_size)].append(my_list[i]) # Sort buckets and place back into input array sorted_array = [] for i in range(0, len(buckets)): insert_sort1(buckets[i]) for j in range(0, len(buckets[i])): sorted_array.append(buckets[i][j]) return sorted_arrayif __name__ == '__main__': sorted_array = bucket_sort([12, 23, 4, 5, 3, 2, 12, 81, 56, 95]) print(sorted_array)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"https://shang.at/tags/排序算法/"}]},{"title":"数据分析-reduce函数引发的","slug":"数据分析-reduce函数引发的","date":"2019-03-28T05:35:25.000Z","updated":"2019-04-10T15:36:43.244Z","comments":true,"path":"post/数据分析-reduce函数引发的/","link":"","permalink":"https://shang.at/post/数据分析-reduce函数引发的/","excerpt":"","text":"reduce in python12345678910111213141516# _functools.reducedef reduce(function, sequence, initial=None): \"\"\" reduce(function, sequence[, initial]) -&gt; value Apply a function of two arguments cumulatively to the items of a sequence, from left to right, so as to reduce the sequence to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). If initial is present, it is placed before the items of the sequence in the calculation, and serves as a default when the sequence is empty. :param function:给定的一个func，func具有两个参数，参数1是临时聚合值，参数2是序列中下一个待聚合的值 :param sequence:待处理的可迭代的序列 :param initial:聚合数据的初始值 \"\"\" pass 工作原理：reduce函数对给定的序列遍历调用func函数，每次调用返回一个临时聚合值，直到整个序列遍历结束。如果设置了初始值，那么在第一次执行func函数的时候，会将func的参数1设置为初始值。 例子： 1234567&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])15&gt;&gt;&gt; reduce(lambda x, y: x+y, [1, 2, 3, 4, 5], 100)115&gt;&gt;&gt; reduce(lambda x, y: str(x)+str(y), [1, 2, 3, 4, 5], '')'12345' reduce函数不仅可以完成这种聚合的功能，还可以完成更加复杂的操作， reduce&amp;foldLeft&amp;foldRight&amp;reduce in scala hive的UDAFspark的UDAF","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"reduce","slug":"reduce","permalink":"https://shang.at/tags/reduce/"},{"name":"数据分析技巧","slug":"数据分析技巧","permalink":"https://shang.at/tags/数据分析技巧/"},{"name":"有初始值的聚合操作","slug":"有初始值的聚合操作","permalink":"https://shang.at/tags/有初始值的聚合操作/"}]},{"title":"信贷数据统计的相关指标","slug":"信贷数据统计的相关指标","date":"2019-03-22T11:19:47.000Z","updated":"2019-03-28T14:24:24.845Z","comments":true,"path":"post/信贷数据统计的相关指标/","link":"","permalink":"https://shang.at/post/信贷数据统计的相关指标/","excerpt":"","text":"贷款类型 等额本息贷款 根据固定的还款时间，计算出应还的总利息，再加上本金，然后每个月平均等额的还款。 等额本金贷款 等额本金相对来说要简单一些，每月所还的本金是相同的，利息由每个月的剩余本金计算得出。 固定点数贷款 按照定义，我们在首次还款时先按固定的点数还一部分贷款，然后再按较低的利率还完剩余的贷款。 双利率贷款 前x个月以较低的r1利率还款，后m-x个月以较高的r2利率还款（假设还款总月数为m） 相关指标 同比增长 环比增长","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"分析指标","slug":"分析指标","permalink":"https://shang.at/tags/分析指标/"}]},{"title":"airflow安装","slug":"airflow安装","date":"2019-03-19T11:24:02.000Z","updated":"2019-03-24T01:33:47.230Z","comments":true,"path":"post/airflow安装/","link":"","permalink":"https://shang.at/post/airflow安装/","excerpt":"","text":"123456789101112131415161718# airflow needs a home, ~/airflow is the default,# but you can lay foundation somewhere else if you prefer# (optional)export AIRFLOW_HOME=~/airflow# install from pypi using pippip install apache-airflow# initialize the databaseairflow initdb# start the web server, default port is 8080airflow webserver -p 8080# start the schedulerairflow scheduler# visit localhost:8080 in the browser and enable the example dag in the home page USE Mysql123vim $AIRFLOW_HOME/airflow.cfg sql_alchemy_conn = mysql+pymysql://root:123456@localhost:3306/airflow 需要pip install pymysql 启动失败1234567891011121314151617181920ERROR [airflow.models.DagBag] Failed to import: /anaconda3/lib/python3.7/site-packages/airflow/example_dags/example_http_operator.pyTraceback (most recent call last): File &quot;/anaconda3/lib/python3.7/site-packages/airflow/models.py&quot;, line 374, in process_file m = imp.load_source(mod_name, filepath) File &quot;/anaconda3/lib/python3.7/imp.py&quot;, line 171, in load_source module = _load(spec) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 696, in _load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;/anaconda3/lib/python3.7/site-packages/airflow/example_dags/example_http_operator.py&quot;, line 27, in &lt;module&gt; from airflow.operators.http_operator import SimpleHttpOperator File &quot;/anaconda3/lib/python3.7/site-packages/airflow/operators/http_operator.py&quot;, line 21, in &lt;module&gt; from airflow.hooks.http_hook import HttpHook File &quot;/anaconda3/lib/python3.7/site-packages/airflow/hooks/http_hook.py&quot;, line 23, in &lt;module&gt; import tenacity File &quot;/anaconda3/lib/python3.7/site-packages/tenacity/__init__.py&quot;, line 352 from tenacity.async import AsyncRetrying ^SyntaxError: invalid syntax 修复方式：修改from tenacity.async import AsyncRetrying为from tenacity.async_a import AsyncRetrying，同时tenacity包下的async文件名为async_a","categories":[],"tags":[]},{"title":"Spark学习笔记-抽样方法和自增ID","slug":"Spark学习笔记-抽样方法和自增ID","date":"2019-03-19T08:33:24.000Z","updated":"2020-07-01T09:09:37.773Z","comments":true,"path":"post/Spark学习笔记-抽样方法和自增ID/","link":"","permalink":"https://shang.at/post/Spark学习笔记-抽样方法和自增ID/","excerpt":"","text":"抽样方法sample(withReplacement=None, fraction=None, seed=None) Returns a sampled subset of this DataFrame. withReplacement – Sample with replacement or not (default False). true时会将抽样的数据放回数据集，导致抽样数据有重复的 false时不会放回 fraction – Fraction of rows to generate, range [0.0, 1.0]. 表示子集占数据集的占比 seed – Seed for sampling (default a random seed). fraction并不能保证完全按照占比抽样数据 自增IDmonotonically_increasing_id() 每个分区分别排序生成一个64位的整数，但不是连续的。会将分区值放到高31位，然后将每条记录的序列放到低33位。限制：分区数不能大于10亿，每个分区的数据量不能大于80亿。","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"抽样方法和自增ID","slug":"抽样方法和自增ID","permalink":"https://shang.at/tags/抽样方法和自增ID/"}]},{"title":"Spark学习笔记-SparkSQL内置函数","slug":"Spark学习笔记-SparkSQL内置函数","date":"2019-03-19T01:27:01.000Z","updated":"2020-07-01T09:10:30.968Z","comments":true,"path":"post/Spark学习笔记-SparkSQL内置函数/","link":"","permalink":"https://shang.at/post/Spark学习笔记-SparkSQL内置函数/","excerpt":"","text":"学习SparkSQL中的一些内置函数 日期函数 获取默认时区 123spark.conf.get('spark.sql.session.timeZone')&gt;&gt; 'Asia/Shanghai' 获取当前时间 获取当前日期：current_date() 12345spark.sql(\"\"\" select current_date()\"\"\").toPandas()&gt;&gt; 2019-03-19 获取当前时间：current_timestamp()/now() 12345spark.sql(\"\"\" select current_timestamp()\"\"\").toPandas()&gt;&gt; 2019-03-19 13:54:22.236 从日期中截取字段 截取年月日、时分秒:year,month,day/dayofmonth,hour,minute,second dayofweek ,dayofyear 11 = Sunday, 2 = Monday, ..., 7 = Saturday weekofyear 1Extract the week number of a given date as integer. trunc截取某部分的日期，其他部分默认为01 123Returns date truncated to the unit specified by the format.Parameters: format – ‘year’, ‘yyyy’, ‘yy’ or ‘month’, ‘mon’, ‘mm’ date_trunc [“YEAR”, “YYYY”, “YY”, “MON”, “MONTH”, “MM”, “DAY”, “DD”, “HOUR”, “MINUTE”, “SECOND”, “WEEK”, “QUARTER”] 123Returns timestamp truncated to the unit specified by the format.Parameters: format – ‘year’, ‘yyyy’, ‘yy’, ‘month’, ‘mon’, ‘mm’, ‘day’, ‘dd’, ‘hour’, ‘minute’, ‘second’, ‘week’, ‘quarter’ date_format将时间转化为某种格式的字符串 123Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.A pattern could be for instance dd.MM.yyyy and could return a string like ‘18.03.1993’. All pattern letters of the Java class java.text.SimpleDateFormat can be used. 日期时间转换 unix_timestamp返回当前时间的unix时间戳 123Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.if timestamp is None, then it returns current timestamp. from_unixtime将时间戳换算成当前时间，to_unix_timestamp将时间转化为时间戳 1Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. to_date/date将字符串转化为日期格式，to_timestamp（Since: 2.2.0） 123Converts a Column of pyspark.sql.types.StringType or pyspark.sql.types.TimestampType into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to SimpleDateFormats. By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted (equivalent to col.cast(&quot;date&quot;)).Converts a Column of pyspark.sql.types.StringType or pyspark.sql.types.TimestampType into pyspark.sql.types.DateType using the optionally specified format. Specify formats according to SimpleDateFormats. By default, it follows casting rules to pyspark.sql.types.TimestampType if the format is omitted (equivalent to col.cast(&quot;timestamp&quot;)). quarter 将1年4等分(range 1 to 4) 1Extract the quarter of a given date as integer. 日期、时间计算 months_between两个日期之间的月数 add_months返回日期后n个月后的日期 last_day(date),next_day(start_date, day_of_week) date_add,date_sub(减) datediff（两个日期间的天数） utc 在集群中对于时间戳的转换，如果不指定时区，默认会采用集群配置的时区，集群默认时区可以通过如下方式获取：spark.conf.get(‘spark.sql.session.timeZone’)。一般而言，这个值应该是集群统一设置，独立提交job的时候，不需要设置。 to_utc_timestamp(timestamp, tz) 1将timestamp按照给定的tz解释，返回utc timestamp from_utc_timestamp(timestamp, tz) 1将timestamp按照utc解释，返回给定tz的timestamp 对于有时区相关的数据统计时，需要注意。比如：集群默认时区设置为UTC，一般将数据存到集群中的时候会将时间戳转为utc timestamp以便后续的操作。此时如果有一个需求是统计北京时间的当天的数据，那么第一个想到的方式是使用current_date()获取当前日期，然后将数据中的时间戳使用to_date(from_utc_timestamp(from_unixtime(ts), ‘Asia/Beijing’))，然后进行比较。但是current_date()获取的日期，是根据集群默认时区得来的，因此会有时区的不同导致的数据统计错误，因此，这种情况不能直接使用current_date()，正确的使用方式是：to_date(from_utc_timestamp(current_timestamp(), ‘Asia/Beijing’))，然后在进行比较。 表关联 Join(other, on=None, how=None) on：a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join. how：str, default inner. Must be one of: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti inner:内连，返回joinDF1和joinDF2合并的rows，如果joinDF2中有多条记录对应于joinDF1的同一条记录，那么返回的row number会大于joinDF1的row number outer,full,full_outer：全连 left, left_outer：左连 right，right_outer:右连 left_semi：过滤出joinDF1中和joinDF2共有的部分，只返回joinDF1中的rows left_anti：过滤出joinDF1中joinDF2没有的部分，只返回joinDF1中的rows crossJoin(other) 返回两个DF的笛卡尔积 Parses the expression expr 将字符串表示的表达式，翻译成DSL 123expr(\"length(name)\")expr(\"array_contains(user_id_set, user_id)\")","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"sparkSql内置函数","slug":"sparkSql内置函数","permalink":"https://shang.at/tags/sparkSql内置函数/"}]},{"title":"数据分析小知识点","slug":"数据分析小知识点","date":"2019-03-19T01:25:56.000Z","updated":"2019-08-03T02:17:49.591Z","comments":true,"path":"post/数据分析小知识点/","link":"","permalink":"https://shang.at/post/数据分析小知识点/","excerpt":"","text":"总结一下在数据分析中需要注意的一些tips，持续更新 Tip1 时区在进行跨境业务处理的时候，时区的控制是十分必要的。平时对于国内的业务，部署在国内的服务器，使用的时区一般都是北京时间(北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间)，在数据库中一般存储相对于unix epoch (1970-01-01 00:00:00 UTC)的毫秒时间戳，做某个地区的数据统计时，需要将时间戳转换成当地的时间(即加一个时区的属性) https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431937554888869fb52b812243dda6103214cd61d0c2000 Tip 2 Excel函数 去重计数：SUMPRODUCT(1/COUNTIF(A2:A20,A2:A20)) VLOOKUP(要查找的值,查找返回,返回查找到的第几列,是否精确查找[1]) 概念落地页，也称：着陆页、引导页，是指访问者在其他地方看到发出的某个具有明确主题的特定营销活动——通过Email、社交媒体或广告发布的诱人优惠信息等，点击后被链接到你网站上的第一个页面 PRD：产品需求文档，产品需求文档是将商业需求文档（BRD）和市场需求文档（MRD）用更加专业的语言进行描述","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"}],"tags":[{"name":"数据分析Tips","slug":"数据分析Tips","permalink":"https://shang.at/tags/数据分析Tips/"}]},{"title":"Spark学习笔记-窗口函数","slug":"Spark学习笔记-窗口函数","date":"2019-03-12T02:15:03.000Z","updated":"2020-07-03T10:37:41.666Z","comments":true,"path":"post/Spark学习笔记-窗口函数/","link":"","permalink":"https://shang.at/post/Spark学习笔记-窗口函数/","excerpt":"","text":"初始化环境1234567891011from pyspark.sql import SparkSessionfrom pyspark.sql.functions import *from pyspark.sql import Windowfrom pyspark.sql.types import StructType, StringType, StructField, IntegerTypeschema = StructType([ StructField('shop_id', StringType()), StructField('date', StringType()), StructField('amount', IntegerType())]) 12345spark = SparkSession \\ .builder \\ .master('local[*]') \\ .enableHiveSupport() \\ .getOrCreate() 123456data = [ &#123;'shop_id': '10006', 'date': '201501120030', 'amount': 2313&#125;, &#123;'shop_id': '10006', 'date': '201501120100', 'amount': 23112&#125;, &#123;'shop_id': '10006', 'date': '201501120130', 'amount': 23112&#125;, &#123;'shop_id': '10006', 'date': '201501120200', 'amount': 24234&#125;, ] 1df = spark.createDataFrame(data, schema) 1df.printSchema() 1234root |-- shop_id: string (nullable = true) |-- date: string (nullable = true) |-- amount: integer (nullable = true) 关于子窗口子窗口需要指定一个边界，有以下两种方式： ROWS between CURRENT ROW | UNBOUNDED PRECEDING | [num] PRECEDING AND UNBOUNDED FOLLOWING | [num] FOLLOWING| CURRENT ROW RANGE between [num] PRECEDING AND [num] FOLLOWING 窗口的含义 ROWS是物理窗口，从行数上控制窗口的尺寸的；RANGE是逻辑窗口，从列值上控制窗口的尺寸 通常会结合order by子句使用，如果在order by子句后面没有指定窗口子句，则默认为：rows between unbounded preceding and current row 12345678910111213141516171819202122spark中关于Window函数的学习在spark中涉及Window函数的主要有以下两个类和一个Column的方法pyspark.sql.column.Column#over 在窗口上应用某一种分析函数pyspark.sql.window.Window 创建WindowSpec的工具类 pyspark.sql.window.Window.unboundedPreceding pyspark.sql.window.Window.unboundedFollowing pyspark.sql.window.Window.currentRow pyspark.sql.window.Window#partitionBy pyspark.sql.window.Window#orderBy pyspark.sql.window.Window#rowsBetween(start, end) pyspark.sql.window.Window#rangeBetween(start, end)pyspark.sql.window.WindowSpec 窗口的规范pyspark.sql.window.Window#rowsBetween(start, end)定义窗口的边界，[start, end]，在边界处是闭区间start和end都是相对于当前row的相对位置，例如：- 0：当前row- -1：当前行的前1row- 5：当前行的后5row- (-1, 5)：窗口的范围为，当前row+当前行的前1row+当前行的后5row = 7rows 使用场景统计截止到当前时间段的店铺累计销售金额123456df.withColumn( 't_amount', sum('amount').over(Window.partitionBy('shop_id').orderBy(asc('date')))).select( 'shop_id', 'date', 't_amount').show(50, truncate=False) 12 分析：根据shop_id分组，根据date正序排列，由于orderBy后面没有追加rowsBetween()，则默认的rowsBetween为：[Window.unboundedPreceding，Window.currentRow]。即会统计根据date排序后，从第一行计算到当前行，从而达到了统计截止到当前时间段的店铺累计销售金额的效果 统计每个时间段的销售占比123456df.withColumn( 't_amount', col('amount')/sum('amount').over(Window.partitionBy('shop_id'))).select( 'shop_id', 'date', 'amount','t_amount').show(50, truncate=False) 12 分析：根据shop_id分组，不排序，窗口大小默认就是整个分组。 找出2点的销售金额及前半小时的销售金额和后1个小时的销售金额1234567891011121314151617df.withColumn( 'pre_half_hour', lag('date', 1).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'pre_half_hour_amount', lag('amount', 1).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'follow_one_hour', lead('date', 2).over(Window.partitionBy('shop_id').orderBy(asc('date')))).withColumn( 'follow_one_hour_amount', lead('amount', 2).over(Window.partitionBy('shop_id').orderBy(asc('date')))).filter( col('date') == '201501120200').select( 'shop_id', 'date', 'amount','pre_half_hour', 'pre_half_hour_amount', 'follow_one_hour', 'follow_one_hour_amount').show(truncate=False) 12345+-------+------------+------+-------------+--------------------+---------------+----------------------+|shop_id|date |amount|pre_half_hour|pre_half_hour_amount|follow_one_hour|follow_one_hour_amount|+-------+------------+------+-------------+--------------------+---------------+----------------------+|10006 |201501120200|24234 |201501120130 |2342 |201501120300 |31232 |+-------+------------+------+-------------+--------------------+---------------+----------------------+ 123456分析：pyspark.sql.functions.lag(col, count=1, default=none)是取前N行的值pyspark.sql.functions.lead(col, count=1, default=none)是取后N行的值。 按照销售金额进行排名，金额最大的排最前（limit可以取topn的数）123456df.withColumn( 'rn', dense_rank().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) 12 123456df.withColumn( 'rn', rank().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) 12 123456df.withColumn( 'rn', row_number().over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'rn' ).show(50, truncate=False) 12 分析：dense_rank和rank都是排名函数，区别在于dense_rank是连续排名，rank遇到排名并列时，下一列排名跳空。row_number是加行号，次序是连续的，不会存在重复的行号 按销售金额排序，取出前20%的时间段和相应金额123456df.withColumn( 'tile', ntile(5).over(Window.partitionBy('shop_id').orderBy(desc('amount')))).select( 'shop_id', 'date', 'amount', 'tile' ).show(50, truncate=False) 12 分析： NTILE就是把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号 设置n=5，那么ntile就会把排好序的数据均分成n个组，ntile函数会返回每条数据所在组的组编号，从而可以达到取前百分比的数据 写在后面思考：在使用row_number函数的时候，并没有指定rowsBetween，那么默认应该是默认的rows between unbounded preceding and current row。但是，结果却是把组内的所有元素都进行了标号 rowsBetween应该是针对于具有聚合性质的函数起作用","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"窗口函数","slug":"窗口函数","permalink":"https://shang.at/tags/窗口函数/"}]},{"title":"Spark学习笔记-union方法","slug":"Spark学习笔记-union方法","date":"2019-03-12T01:53:56.000Z","updated":"2020-07-01T09:10:57.726Z","comments":true,"path":"post/Spark学习笔记-union方法/","link":"","permalink":"https://shang.at/post/Spark学习笔记-union方法/","excerpt":"","text":"方法简介pyspark.sql.dataframe.DataFrame#union(other) 1union两个df，效果相当于union all(pyspark.sql.dataframe.DataFrame#unionAll在2.0以后就被Deprecated了)。 union方法的特点是： schema会使用前面df的schema， 只有两个有相同数量列的df才能进行union， union的时候会根据列的顺序进行union，与属性名无关 案例测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778from pyspark.sql import SparkSessionfrom pyspark.sql.functions import *spark = SparkSession \\ .builder \\ .master('local[*]') \\ .enableHiveSupport() \\ .getOrCreate() df1 = spark.createDataFrame([&#123;'name':'cc', 'age':24&#125;, &#123;'name':'aa', 'age':25&#125;])df1.printSchema() root |-- age: long (nullable = true) |-- name: string (nullable = true)df2 = spark.createDataFrame([&#123;'name1':'bb', 'age1':2&#125;, &#123;'name1':'dd', 'age1':3&#125;])df2.printSchema() root |-- age1: long (nullable = true) |-- name1: string (nullable = true)df1.union(df2) DataFrame[age: bigint, name: string]union_df = df1.union(df2)union_df.printSchema() root |-- age: long (nullable = true) |-- name: string (nullable = true)union_df.show() +---+----+ |age|name| +---+----+ | 24| cc| | 25| aa| | 2| bb| | 3| dd| +---+----+select_union_df = df1.select('name').union(df2.select('age1'))select_union_df.printSchema() root |-- name: string (nullable = true)select_union_df.show() +----+ |name| +----+ | cc| | aa| | 2| | 3| +----+select_union_df = df1.select('name', 'age').union(df2.select('age1')) pyspark.sql.utils.AnalysisException: \"Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 1 columns;;\\n'Union\\n:- Project [name#1, age#0L]\\n: +- LogicalRDD [age#0L, name#1], false\\n+- Project [age1#4L]\\n +- LogicalRDD [age1#4L, name1#5], false\\n\"select_union_df = df1.select('age').union(df2.select('name1'))select_union_df.show() +---+ |age| +---+ | 24| | 25| | bb| | dd| +---+select_union_df.printSchema() root |-- age: string (nullable = true)select_union_df = df1.select('name', 'age').union(df2.select('age1', 'name1'))select_union_df.printSchema() root |-- name: string (nullable = true) |-- age: string (nullable = true)select_union_df.show() +----+---+ |name|age| +----+---+ | cc| 24| | aa| 25| | 2| bb| | 3| dd| +----+---+ 其他案例要想实现sql中union的效果，需要结合distinct()来使用: 12345678910111213141516171819df1 = spark.createDataFrame([&#123;'name':'cc', 'age':24&#125;, &#123;'name':'aa', 'age':25&#125;])df2 = spark.createDataFrame([&#123;'name1':'cc', 'age1':24&#125;, &#123;'name1':'dd', 'age1':3&#125;])df1.union(df2).show() +---+----+ |age|name| +---+----+ | 24| cc| | 25| aa| | 24| cc| | 3| dd| +---+----+df1.union(df2).distinct().show() +---+----+ |age|name| +---+----+ | 24| cc| | 3| dd| | 25| aa| +---+----+","categories":[{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"}],"tags":[{"name":"union","slug":"union","permalink":"https://shang.at/tags/union/"}]},{"title":"Spark学习笔记一","slug":"Spark学习笔记一","date":"2019-03-10T10:45:03.000Z","updated":"2019-03-24T01:33:47.230Z","comments":true,"path":"post/Spark学习笔记一/","link":"","permalink":"https://shang.at/post/Spark学习笔记一/","excerpt":"","text":"版本：pyspark 2.4.0 主要包 pyspark pyspark.sql module pyspark.streaming module pyspark.ml package pyspark.mllib package pyspark pyspark.SparkConf(loadDefaults=True, _jvm=None, _jconf=None) 是spark应用的配置类，默认loadDefaults=True，会自动加载java系统参数中的spark.*的参数，_jconf是一个已经存在的sparkConf句柄 主要api： setMaster() 设置应用的提交类型：local|local[n]|local[*] or 不填，本地测试时可以填local系列，提交到集群运行时可以不用填，提交任务的时候会根据集群的配置，自动选择提交的类型：standalone或者yarn模式 setAppName() 设置应用的名称 pyspark.SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=) spark应用上下文，是spark应用的主要入口。代表了与spark cluster的链接，可以用来在集群中创建RDD和广播变量。 主要api讲解： addFile(self, path, recursive=False) 为spark job添加一个可下载文件，spark的每一个node都会下载一份，可以是local file、hdfs file、http file、https file或ftp file。可以使用SparkFiles通过文件名来读取设置的文件， 注意，每个应用中，每个文件名只能设置一次。recursive设置为True时，传递的path可以是目录，但是目前只支持hdfs file的场景 12345678910111213141516171819202122232425262728import osfrom tempfile import gettempdirfrom pyspark import SparkConffrom pyspark import SparkContextif __name__ == '__main__': from pyspark import SparkFiles conf = SparkConf() conf.setMaster(\"local\").setAppName(\"My app\") sc = SparkContext(conf=conf) path = os.path.join(gettempdir(), \"test.txt\") print(path) with open(path, \"w\") as testFile: _ = testFile.write(\"100\") sc.addFile(path) def func(iterator): with open(SparkFiles.get('test.txt')) as testFile: fileVal = int(testFile.readline()) print(fileVal) return [x * fileVal for x in iterator] result = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect() print(result) accumulator(value, accum_param=None) 创建一个累加器。一个全局共享的可以进行累加的变量，只能在worker上进行update操作，在driver上获取结果值得操作。值类型默认是int和float类型，也可以使用accum_param参数设置为自定义的数据类型 1# 代码后加 broadcast(value) 在集群中广播一个只读的值，返回一个Broadcast对象，以便在分布式方法中调用。被广播的变量只会被发送到集群的各个node上一次","categories":[{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"}],"tags":[{"name":"pyspark官方文档学习","slug":"pyspark官方文档学习","permalink":"https://shang.at/tags/pyspark官方文档学习/"}]},{"title":"数据仓库学习笔记二-建模流程","slug":"数据仓库学习笔记二-建模流程","date":"2019-03-03T11:36:50.000Z","updated":"2019-03-03T12:26:38.927Z","comments":true,"path":"post/数据仓库学习笔记二-建模流程/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记二-建模流程/","excerpt":"","text":"数据建模的基本流程在建模的不同阶段，将数据模型分为三个层次，每层的作用各不相同。 概念模型:确定系统的核心以及划清系统范围和 边界 逻辑模型:梳理业务规则以及对概念模型的求精 物理模型:从性能、访问、开发等多方面考虑， 做系统的实现 概念模型概念建模小贴士1 注重全局的理解而非细节 在概念模型阶段，即需要对整体架构做思考 概念模型通常是自上而下的模式，通过会议等模式反复沟通，澄清需求 在此阶段，应粗略地估算出整个项目需要的时间以及项目计划草案 根据计划粗略地估算出项目的费用 是数据模型工程师与客户沟通的破冰之旅，使他们在此期间达成共识并奠定未来良好的沟通基础以及私人关系 出品的概念模型可以帮助划定系统边界以及避免方向性的错误 商业主导，相比技术专家而言，更需要商业专家 是未来逻辑模型的沟通基础，以及逐步求精的依据 概念模型交付品通常具备如下特点: 与客户一致的商业语言 尽量一页纸描述清楚整个模型 通常用实体关系型图表示，但不需添加实体的属性 允许多对多的关系存在 逻辑模型逻辑建模小提示1 应更精确估算出整个项目需要的时间以及项目计划草案 并且根据计划更精确地估算出项目的费用 当实体数量超过100时，需要定义术语表 规范化 先规范化再逆规范化，不可一步到位 不可缺少约束的定义 使用CASE工具做逻辑模型 多对多关系需要解决 需要同级评审(Peer Review) 确定可信赖数据源，关键属性需用真实数据验证 应用成熟的建模模式(Pattern) 一定程度的抽象化，决定了未来模型的弹性 高质量的模型定义 重要关联关系需要强制建立 与概念模型保持一致 注意模型的版本管理 非常非常注意细节 数据库专家深度介入 占据整个数据建模80%以上时间 不要忽视属性的长度定义和约束定义 不要忽视属性的默认值(Default Value) 使用控制数据范围的域(Domain) 逻辑建模交付品的特点 要像一本书，而非一页纸 所有实体属性均需添加 实体间关系要清晰描述 使用术语表 遵循命名规范 采用CASE工具创建项目文件 对各个实体必须有清晰描述 对关键属性必须有清晰描述 物理模型物理建模小贴士1 使用CASE工具由逻辑模型自动生成 应用术语表自动转换生成字段名称 对表空间、索引、视图、物化视图、主键、外键等都有命名规则 逆规范化在逻辑层完成，而非本层 数据库DBA深度介入，需要DBA的评审(Peer Review) 和数据库的DDL保持一致 注意版本管理 注意开发、测试、生产三个不同版本的模型管理 注意性能 估算数据规模 考虑数据归档 充分考虑未来使用数据库的优点和缺点 物理建模交付品的特点 自动生成基础库表结构，之后适度手动调整 与未来要使用的数据库类型息息相关 生成数据字典并发布 可直接用于生成DDL DDL中注意注释的生成 如何进行高质量数据建模什么样的模型算是高质量数据模型? 对真实世界的抽象正确而完整 用建模语言表达清晰而准确 框架稳定且灵活，满足当下的需求并能够一定程度容纳未来的变化 根据需求尽可能减少数据冗余 充分考虑潜在的性能问题 从企业全局的视角出发构筑模型","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据仓库学习笔记二-重要意义","slug":"数据仓库学习笔记二-重要意义","date":"2019-03-03T10:55:58.000Z","updated":"2019-03-03T11:36:09.787Z","comments":true,"path":"post/数据仓库学习笔记二-重要意义/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记二-重要意义/","excerpt":"","text":"数据时代的演化DIKWdata数据 + information信息 + knowledge知识 + wisdom智慧 描述数据的数据被称为元数据(metadata) 信息（information）= 元数据（metadata）+数据（data） 什么是数据模型数据模型实际上就是为了装载数据，用元数据搭建起来的框子 数据模型是将数据元素以标准化的模式组织起来, 用来模拟现实世界的信息框架蓝图。 数据模型的要求: 直观地模拟世界 容易为人所理解 便于计算机实现 数据模型是整个数据应用的基石，牵一发而动全身。数据模型的小小改动将会导致上层数据应用的大幅度变化 建设高质量数据模型的意义低质量数据模型的十宗罪 没有准确的捕获到需求 数据模型不完整 各层模型与其扮演角色不匹配 数据结构不合理 抽象化不够，造成模型不灵活 没有或者不遵循命名规范 缺少数据模型的定义和描述 数据模型可读性差 元数据与数据不匹配 数据模型与企业标准不一致 低质量数据模型的影响 大量修改和重做 重复建设 知识丢失 下游开发困难 高成本 数据质量低下 新业务无法展开 意义","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据仓库学习笔记一","slug":"数据仓库学习笔记一","date":"2019-03-03T10:24:23.000Z","updated":"2019-03-03T11:12:25.458Z","comments":true,"path":"post/数据仓库学习笔记一/","link":"","permalink":"https://shang.at/post/数据仓库学习笔记一/","excerpt":"","text":"学习路线： 高质量数据建模基础 经典数据仓库架构 EDW建模 维度建模 数据仓库生命周期","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[]},{"title":"数据结构学习笔记二-复杂度分析","slug":"数据结构学习笔记二-复杂度分析","date":"2019-03-03T10:22:47.000Z","updated":"2020-04-10T00:41:06.625Z","comments":true,"path":"post/数据结构学习笔记二-复杂度分析/","link":"","permalink":"https://shang.at/post/数据结构学习笔记二-复杂度分析/","excerpt":"","text":"时间复杂度 通常使用大O时间复杂度表示法：它表示了代码执行时间随数据规模增长的变化趋势， 亦可称为 渐进式时间复杂度 几种常见时间复杂度实例分析 各种时间复杂度执行时间比较 故可以看出来O(logn)的算法要比O(n)的算法效率上快上不少 常见排序算法的时间复杂度 空间复杂度 亦可称为 渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系 空间复杂度基本就只有O(1)、O(n)、O(n^2) 时间复杂度进阶实际上我们前面说的大O表示法只是一个初级的判断方式，此外还有最好情况时间复杂度、最坏情况时间复杂度、平均时间复杂度、均摊时间复杂度。 在大部分的时间大O表示法已经能够分辨出各种算法的时间复杂度了。只有在特殊情况下（同一块代码(或分析出的相同的大O等级)在不同的情况下，时间复杂度有量级的差距），才需要使用最好、最坏、平均三种方法来区分。均摊的情况就更加特殊了，比如向数组加入一个元素，当数组长度不足时，扩充数组再添加元素。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"复杂度分析","slug":"复杂度分析","permalink":"https://shang.at/tags/复杂度分析/"}]},{"title":"数据结构学习笔记一","slug":"数据结构学习笔记一","date":"2019-03-03T09:34:30.000Z","updated":"2020-07-01T09:21:51.746Z","comments":true,"path":"post/数据结构学习笔记一/","link":"","permalink":"https://shang.at/post/数据结构学习笔记一/","excerpt":"","text":"基础知识数组为什么从0开始编号 数组的随机访问的寻址公式： a[i]_address = base_address + i * data_type_size 常见数据类型的占用内存大小 类型 字节大小 位数 表示范围 最大存储量 byte 1byte 8bits -2^7~2^7-1 2^8-1=255 int 4bytes 4*8bits -2^{31}~2^{31}-1 short 2bytes 2*8bits -2^{15}~2^{15}-1 long 8bytes 8*8bits -2^{63}~2^{63}-1 float 4bytes 4*8bits double 8bytes 8*8bits char 2bytes 2*bits 二进制-十进制计算机内部的二进制表示法：原码、反码、补码 数组链表栈队列跳表散列表关键点 散列思想 散列表是利用了数组可以根据下标随机访问的特性，而产生的一种高性能的数据结构(查找的时间复杂度为O(1))。散列表其实就是数组的一种拓展，是由数组演化而来。 其关键的概念有 键-key、 将键映射到数组下标的方法-散列函数(哈希函数)、 散列函数计算来的值-散列值(哈希值) 散列函数 散列冲突 开放寻址 链表法 进阶：链表使用红黑树或者跳表实现 具体实现|打造一个工业级散列表 散列函数 装载因子 散列冲突解决方法 二叉树红黑树递归树堆图Trie数AC自动机位图布隆过滤器B+数索引","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"}]},{"title":"Python数据科学学习笔记一","slug":"Python数据科学学习笔记一","date":"2019-03-03T09:19:14.000Z","updated":"2019-08-03T11:51:48.061Z","comments":true,"path":"post/Python数据科学学习笔记一/","link":"","permalink":"https://shang.at/post/Python数据科学学习笔记一/","excerpt":"","text":"描述性统计和探索型数据分析变量类型 名义变量 等级变量 连续变量 描述名义变量的分布两个统计量 频数 百分比 可视化 柱状图","categories":[{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"}],"tags":[{"name":"描述性统计和探索型数据分析","slug":"描述性统计和探索型数据分析","permalink":"https://shang.at/tags/描述性统计和探索型数据分析/"}]},{"title":"记一次线上JVM问题调试","slug":"记一次线上JVM问题调试","date":"2019-02-27T03:21:32.000Z","updated":"2020-06-03T03:02:27.348Z","comments":true,"path":"post/记一次线上JVM问题调试/","link":"","permalink":"https://shang.at/post/记一次线上JVM问题调试/","excerpt":"发现java进程 使用linux命令：ps 1234ps -ef | grep prcessName[root@YZSJHL81-35 ~]# ps -ef | grep Bootstraproot 118271 1 99 00:10 ? 3-16:08:07 Bootstrap startroot 197682 197016 0 17:08 pts/0 00:00:00 grep Bootstrap 使用java自带的检测工具 jps 123[root@YZSJHL81-35 ~]# jps118271 Bootstrap197101 Jps","text":"发现java进程 使用linux命令：ps 1234ps -ef | grep prcessName[root@YZSJHL81-35 ~]# ps -ef | grep Bootstraproot 118271 1 99 00:10 ? 3-16:08:07 Bootstrap startroot 197682 197016 0 17:08 pts/0 00:00:00 grep Bootstrap 使用java自带的检测工具 jps 123[root@YZSJHL81-35 ~]# jps118271 Bootstrap197101 Jps 检测java进程启动的时间 12ps -p 118271 -o etimeps -p 118271 -o etime= 检测进程打开的文件数 1lsof -n -p 118271 | wc -l 检测系统的进程 top 12345678910top - 17:11:21 up 132 days, 2:14, 1 user, load average: 0.12, 0.21, 0.18Tasks: 298 total, 1 running, 297 sleeping, 0 stopped, 0 zombieCpu(s): 1.0%us, 0.6%sy, 0.0%ni, 98.0%id, 0.3%wa, 0.0%hi, 0.1%si, 0.0%stMem: 99009860k total, 90554384k used, 8455476k free, 333672k buffersSwap: 0k total, 0k used, 0k free, 81820024k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMANDq-退出1-显示所有的cpu及其状态 查看指定进程下所有线程的状态 1top -Hp pid 查看JVM的GC状态1jstat -gcutil -t pid 1000 100 ---查看pid进程的GC状态每隔1000ms一次，一共100次 ​ 查看JVM栈信息1234printf '%x\\n' tid -- 打印指定线程id的十六进制表示jstack pid | grep 'tid十六进制' -C20 -color -- 查看进程的栈信息jmap -histo:live pid --查看进程所有存活的实例jmap -dumap:format=b,file=dump.hprof pid --导出JVM的dump文件","categories":[{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"}],"tags":[{"name":"JVM问题调试","slug":"JVM问题调试","permalink":"https://shang.at/tags/JVM问题调试/"}]},{"title":"辨析 Sass 中的 Map 和 List","slug":"demo","date":"2015-10-21T02:34:12.000Z","updated":"2019-03-01T00:39:39.899Z","comments":true,"path":"post/demo/","link":"","permalink":"https://shang.at/post/demo/","excerpt":"如果你使用过 Sass 3.3 之前的版本，那么你一定对那段时光颇有感触，那时候没有现如今这么好的条件，那时候的 Map 还只能用多重列表（lists of list）来模拟。多重列表可以实现复杂数据的嵌套定义，但却不是以键值对的形式实现的，所有当我们需要获取其中特定的某一项时就会比较麻烦。Map 这种数据类型天生就是基于键值对的形式，非常便于组织数据。 自从可以使用 Map 之后，开发者们开始毫无顾忌地定义 Map 存储数据，比如断点宽度、颜色值、栅格布局等等响应式排版的细节，都被一股脑的塞进了 Map 中。 那么，有了 Map 之后，我们还有必要使用 List 吗？可能某些人会觉得为了保持向后兼容应该继续使用多重列表模拟 Map，因为可能有些开发者仍然在使用老版本的 Sass 编译器，但实际上，这是多此一举了，Sass 的版本通常由 package.json 或者其他同类型的项目配置文件所控制，往往只需一条命令（gem update sass）即可更新 Sass 的版本，因此基本上无需考虑对老版本的兼容问题。","text":"如果你使用过 Sass 3.3 之前的版本，那么你一定对那段时光颇有感触，那时候没有现如今这么好的条件，那时候的 Map 还只能用多重列表（lists of list）来模拟。多重列表可以实现复杂数据的嵌套定义，但却不是以键值对的形式实现的，所有当我们需要获取其中特定的某一项时就会比较麻烦。Map 这种数据类型天生就是基于键值对的形式，非常便于组织数据。 自从可以使用 Map 之后，开发者们开始毫无顾忌地定义 Map 存储数据，比如断点宽度、颜色值、栅格布局等等响应式排版的细节，都被一股脑的塞进了 Map 中。 那么，有了 Map 之后，我们还有必要使用 List 吗？可能某些人会觉得为了保持向后兼容应该继续使用多重列表模拟 Map，因为可能有些开发者仍然在使用老版本的 Sass 编译器，但实际上，这是多此一举了，Sass 的版本通常由 package.json 或者其他同类型的项目配置文件所控制，往往只需一条命令（gem update sass）即可更新 Sass 的版本，因此基本上无需考虑对老版本的兼容问题。 使用多重列表替代 Map 的优势之一就是减少代码量。下面让我们来比较一下多种列表和 Map 的语法结构以及遍历方式。 测试表格 Variable Description site Sitewide information. page Page specific information and custom variables set in front-matter. config Site configuration theme Theme configuration. Inherits from site configuration. _ (single underscore) Lodash library path Path of current page url Full URL of current page env Environment variables 语法比较 测试标题 在下面的示例中，我创建了一个用于控制响应式布局的数据，该数据一共有四个断点，每一个断点都包含了 `min-width`、`max-width`、`font-size` 和 `line-height` 四个样式。 Map 语法下面就是使用 Map 存储的数据，具体来说，该 Map 中首先存储了四个用于标识断点的 Key，相对应的是保存具体属性值得 Value。虽然这种形式可读性更高，但是总体代码量却高达 26 行 450 个字符。 1234567891011121314151617181920212223242526$breakpoint-map: ( small: ( min-width: null, max-width: 479px, base-font: 16px, vertical-rhythm: 1.3 ), medium: ( min-width: 480px, max-width: 959px, base-font: 18px, vertical-rhythm: 1.414 ), large: ( min-width: 960px, max-width: 1099px, base-font: 18px, vertical-rhythm: 1.5 ), xlarge: ( min-width: 1100px, max-width: null, base-font: 21px, vertical-rhythm: 1.618 )); 多重列表语法下面的多重列表存储了和上面 Map 同样的数据，在多重列表中没有 Key-Value 的对应关系，这意味着要想找到特定的值，必须使用遍历或 nth() 的方式来实现了。从另一个角度来看，多种列表又比 Map 的代码量小得多，总共只有六行 180 个字符。 123456$breakpoint-list: ( (small, null, 479px, 16px, 1.3), (medium, 480px, 959px, 18px, 1.414), (large, 960px, 1099px, 18px, 1.5), (xlarge, 1100px, null, 21px, 1.618)); 遍历比较 测试标题 从上面简单地比较中可以粗略的看出，多种列表的代码量明显少于 Map。但是，如果我们需要遍历这些值得话，复杂度又是怎样的呢？ 遍历 Map我们可以使用如下的代码遍历 Map： 1@each $label, $map in $breakpoint-map &#123;&#125; 这里的变量 $label 和 $map 会随着对 $breakpoint-map 的遍历被动态地赋值，$label 将会被赋值为 $breakpoint-map 的 Key，而 $map 会被赋值为 $breakpoint-map 的 Value。为了在遍历过程中获取特定值，我们就需要使用 Sass 原生的 map-get() 函数，使用该函数需要传入两个参数：Map 的名字和求取的 Key，最后返回该 Map 中匹配该 Key 的 Value。 具体的做法就是使用 @each 遍历 Map，然后使用 map-get() 获取特定值，最终只需要六行代码 220 个字符即可完成整个遍历： 123456@each $label, $map in $breakpoint-map &#123; $min-width: map-get($map, min-width); $max-width: map-get($map, max-width); $base-font: map-get($map, base-font); $vertical-rhythm: map-get($map, vertical-rhythm);&#125; 遍历多重列表遍历多重列表不必像遍历 Map 一样动态获取到 Map 后再使用 map-get() 函数取特定值，直接遍历一遍即可获得特定值。 因为多种列表内层的每一个列表结构相同，都有按照相同顺序排列的五个值，所以我们可以持续遍历每个值并赋值给特定的变量。无需调用 map-get()，直接引用这些变量即可进行赋值等裸机操作。最终遍历多重列表只使用了两行代码 100 个字符： 12@each $label, $min-width, $max-width, $base-font, $vertical-rhythm in $breakpoint-list &#123;&#125; 慎用多重列表 测试标题 经过上述的比对，看起来多重列表各方面都在碾压 Map，实则不然，Sass 中添加 Map 有一条非常重要的原因就是：Key-Value 的映射关系。 遗漏键值如果要使用多重列表，那么就必须保证自己非常熟悉多重列表内部的每一项所代表的意义。下面我们举个例子，来看看遗漏了某些值的情况： 1234567891011121314151617181920$breakpoint-list: ( (small, null, 479px, 16px, 1.3), (medium, 480px, 959px, 18px, 1.414), (large, 960px, 1099px, 18px, 1.5), (xlarge, 1100px, 21px, 1.618));p &#123; @each $label, $min-width, $max-width, $base-font, $vertical-rhythm in $breakpoint-list &#123; @if $min-width &#123; @include breakpoint( $min-width ) &#123; font-size: $base-font; line-height: $vertical-rhythm; &#125; &#125; @else &#123; font-size: $base-font; line-height: $vertical-rhythm; &#125; &#125;&#125; 当我们尝试运行这段代码时，结果肯定是错误地，因为在 $breakpoint-list 的最后一行，xlarge 被赋值给了 $label，1100px 被赋值给了 $min-width，21px 被赋值给了 $max-width, 1.618 被赋值给了 $base-font，最终导致 $vertical-rhythm 没有被赋值，结果就是 font-size 的属性值是错的，line-height 的属性值是空的。此外，Sass 还不会对此抛出错误，导致我们无从知晓错误所在。 如果我们使用 Map 来代替这里的多重列表，那么使用 map-get() 函数即使遇见空值也能正确获得想要的结果。这就是值得我们慎重思考的地方：多种列表虽然简单快速，但是丧失了 Map 中的容错能力和快速取值能力。 查找特定列表在多重列表中查找特定列表简直就是一种折磨。如果使用 Map，那么配合 map-get() 函数可以快速定位到特定子 Map： 1$medium-map: map-get($maps, medium); 但如果要获取多种列表 medium 列表，麻烦可就大了： 123456789@function get-list($label) &#123; @each $list in $breakpoint-list &#123; @if nth($list, 1) == $label &#123; @return $list; &#125; &#125; @return null;&#125;$medium-list: get-list(medium); 这段代码的逻辑就是遍历整个多重列表，知道找到第一个匹配项，然后返回，如果一直没有找到匹配项，就一直遍历到末尾，然后返回 null。这实际上就是手工实现了 map-get() 的逻辑。 缺少原生的 Map 函数Sass 提供了诸多的原生函数用于处理 Map 数据类型，但是多重列表是没法调用这些函数的，比如，使用 map-merge() 可以合并两个 Map，如果两个 Map 有相同的值，则取第二个 Map 的值为最终值。当然你也可以在多重列表中使用 join() 或 append() 来增加新列表，从而模拟出 map-merge() 的效果。 另一个实用的 Map 函数就是 map-has-key()，对于依赖 map-get() 的自定义函数来说，map-has-key() 可以用来验证特定的 Key 是否存在。但在列表中是完全没有相似的方法。 总结 Test Title 相比起列表来说，Key-Value 模型的 Map 显然更有力量，原生的 Sass Map 函数更是提供了强力的数据查找和验证工具。 虽然多重列表代码量少，但并不能像 Map 一样进行错误检查或验证参数。在大多数时候，相比较多重列表而言，我相信 Map 是更好的选择。如果是为了更少的代码量和其他简单地调用，那么我偶尔会用用多重列表，但是从项目的宏观控制和数据存储方面显然更优秀。","categories":[],"tags":[{"name":"css","slug":"css","permalink":"https://shang.at/tags/css/"}]}],"categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://shang.at/categories/数据结构与算法/"},{"name":"大数据","slug":"大数据","permalink":"https://shang.at/categories/大数据/"},{"name":"Python","slug":"Python","permalink":"https://shang.at/categories/Python/"},{"name":"操作系统","slug":"操作系统","permalink":"https://shang.at/categories/操作系统/"},{"name":"工具使用","slug":"工具使用","permalink":"https://shang.at/categories/工具使用/"},{"name":"JAVA学习","slug":"JAVA学习","permalink":"https://shang.at/categories/JAVA学习/"},{"name":"虚拟机","slug":"虚拟机","permalink":"https://shang.at/categories/虚拟机/"},{"name":"Linux","slug":"Linux","permalink":"https://shang.at/categories/Linux/"},{"name":"Spark应用","slug":"Spark应用","permalink":"https://shang.at/categories/Spark应用/"},{"name":"Hadoop学习","slug":"Hadoop学习","permalink":"https://shang.at/categories/Hadoop学习/"},{"name":"Hive学习","slug":"Hive学习","permalink":"https://shang.at/categories/Hive学习/"},{"name":"数据库","slug":"数据库","permalink":"https://shang.at/categories/数据库/"},{"name":"Shell编程","slug":"Shell编程","permalink":"https://shang.at/categories/Shell编程/"},{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/categories/大数据生态/"},{"name":"JAVA并发编程","slug":"JAVA并发编程","permalink":"https://shang.at/categories/JAVA并发编程/"},{"name":"分布式","slug":"分布式","permalink":"https://shang.at/categories/分布式/"},{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://shang.at/categories/Python数据分析/"},{"name":"Spark学习","slug":"Spark学习","permalink":"https://shang.at/categories/Spark学习/"},{"name":"BI","slug":"BI","permalink":"https://shang.at/categories/BI/"},{"name":"数据分析","slug":"数据分析","permalink":"https://shang.at/categories/数据分析/"},{"name":"Spark","slug":"Spark","permalink":"https://shang.at/categories/Spark/"},{"name":"数据仓库","slug":"数据仓库","permalink":"https://shang.at/categories/数据仓库/"}],"tags":[{"name":"左神学习笔记","slug":"左神学习笔记","permalink":"https://shang.at/tags/左神学习笔记/"},{"name":"aws","slug":"aws","permalink":"https://shang.at/tags/aws/"},{"name":"文件操作","slug":"文件操作","permalink":"https://shang.at/tags/文件操作/"},{"name":"网络的配置","slug":"网络的配置","permalink":"https://shang.at/tags/网络的配置/"},{"name":"UML","slug":"UML","permalink":"https://shang.at/tags/UML/"},{"name":"设计模式","slug":"设计模式","permalink":"https://shang.at/tags/设计模式/"},{"name":"Docker","slug":"Docker","permalink":"https://shang.at/tags/Docker/"},{"name":"Centos开发环境","slug":"Centos开发环境","permalink":"https://shang.at/tags/Centos开发环境/"},{"name":"centos7时区设置","slug":"centos7时区设置","permalink":"https://shang.at/tags/centos7时区设置/"},{"name":"ssh","slug":"ssh","permalink":"https://shang.at/tags/ssh/"},{"name":"deque","slug":"deque","permalink":"https://shang.at/tags/deque/"},{"name":"Python标准库","slug":"Python标准库","permalink":"https://shang.at/tags/Python标准库/"},{"name":"itertools","slug":"itertools","permalink":"https://shang.at/tags/itertools/"},{"name":"Spark大数据集防止driver OOM","slug":"Spark大数据集防止driver-OOM","permalink":"https://shang.at/tags/Spark大数据集防止driver-OOM/"},{"name":"Python类的特殊方法","slug":"Python类的特殊方法","permalink":"https://shang.at/tags/Python类的特殊方法/"},{"name":"JAVA-JMH","slug":"JAVA-JMH","permalink":"https://shang.at/tags/JAVA-JMH/"},{"name":"动态规划","slug":"动态规划","permalink":"https://shang.at/tags/动态规划/"},{"name":"python的dict","slug":"python的dict","permalink":"https://shang.at/tags/python的dict/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://shang.at/tags/Hadoop/"},{"name":"Yarn-Scheduler","slug":"Yarn-Scheduler","permalink":"https://shang.at/tags/Yarn-Scheduler/"},{"name":"Yarn-Container","slug":"Yarn-Container","permalink":"https://shang.at/tags/Yarn-Container/"},{"name":"实现一个在Yarn上的Application","slug":"实现一个在Yarn上的Application","permalink":"https://shang.at/tags/实现一个在Yarn上的Application/"},{"name":"Mapreduce编程模式","slug":"Mapreduce编程模式","permalink":"https://shang.at/tags/Mapreduce编程模式/"},{"name":"Hive安装","slug":"Hive安装","permalink":"https://shang.at/tags/Hive安装/"},{"name":"mysql环境配置","slug":"mysql环境配置","permalink":"https://shang.at/tags/mysql环境配置/"},{"name":"Hadoop-Shell","slug":"Hadoop-Shell","permalink":"https://shang.at/tags/Hadoop-Shell/"},{"name":"iterm2","slug":"iterm2","permalink":"https://shang.at/tags/iterm2/"},{"name":"Hadoop源码编译","slug":"Hadoop源码编译","permalink":"https://shang.at/tags/Hadoop源码编译/"},{"name":"Hadoop集群搭建","slug":"Hadoop集群搭建","permalink":"https://shang.at/tags/Hadoop集群搭建/"},{"name":"Hadoop配置详解","slug":"Hadoop配置详解","permalink":"https://shang.at/tags/Hadoop配置详解/"},{"name":"常见命令","slug":"常见命令","permalink":"https://shang.at/tags/常见命令/"},{"name":"centos7修改hostname","slug":"centos7修改hostname","permalink":"https://shang.at/tags/centos7修改hostname/"},{"name":"vim","slug":"vim","permalink":"https://shang.at/tags/vim/"},{"name":"大数据端口","slug":"大数据端口","permalink":"https://shang.at/tags/大数据端口/"},{"name":"大数据环境","slug":"大数据环境","permalink":"https://shang.at/tags/大数据环境/"},{"name":"Vagrant","slug":"Vagrant","permalink":"https://shang.at/tags/Vagrant/"},{"name":"Kafka","slug":"Kafka","permalink":"https://shang.at/tags/Kafka/"},{"name":"大数据生态","slug":"大数据生态","permalink":"https://shang.at/tags/大数据生态/"},{"name":"java底层","slug":"java底层","permalink":"https://shang.at/tags/java底层/"},{"name":"class文件格式","slug":"class文件格式","permalink":"https://shang.at/tags/class文件格式/"},{"name":"运行时线程","slug":"运行时线程","permalink":"https://shang.at/tags/运行时线程/"},{"name":"JOIN的算法实现","slug":"JOIN的算法实现","permalink":"https://shang.at/tags/JOIN的算法实现/"},{"name":"JDK环境切换","slug":"JDK环境切换","permalink":"https://shang.at/tags/JDK环境切换/"},{"name":"JVM虚拟机栈","slug":"JVM虚拟机栈","permalink":"https://shang.at/tags/JVM虚拟机栈/"},{"name":"JVM疑问","slug":"JVM疑问","permalink":"https://shang.at/tags/JVM疑问/"},{"name":"基本类型和包装类型","slug":"基本类型和包装类型","permalink":"https://shang.at/tags/基本类型和包装类型/"},{"name":"网络编程-零拷贝","slug":"网络编程-零拷贝","permalink":"https://shang.at/tags/网络编程-零拷贝/"},{"name":"JVM参数","slug":"JVM参数","permalink":"https://shang.at/tags/JVM参数/"},{"name":"常见异常","slug":"常见异常","permalink":"https://shang.at/tags/常见异常/"},{"name":"Disruptor","slug":"Disruptor","permalink":"https://shang.at/tags/Disruptor/"},{"name":"RxJava","slug":"RxJava","permalink":"https://shang.at/tags/RxJava/"},{"name":"协程","slug":"协程","permalink":"https://shang.at/tags/协程/"},{"name":"线程池","slug":"线程池","permalink":"https://shang.at/tags/线程池/"},{"name":"阻塞队列","slug":"阻塞队列","permalink":"https://shang.at/tags/阻塞队列/"},{"name":"Atomic","slug":"Atomic","permalink":"https://shang.at/tags/Atomic/"},{"name":"并发集合","slug":"并发集合","permalink":"https://shang.at/tags/并发集合/"},{"name":"ThreadLocal-Fork&Join","slug":"ThreadLocal-Fork-Join","permalink":"https://shang.at/tags/ThreadLocal-Fork-Join/"},{"name":"并发工具类","slug":"并发工具类","permalink":"https://shang.at/tags/并发工具类/"},{"name":"锁","slug":"锁","permalink":"https://shang.at/tags/锁/"},{"name":"并发基础","slug":"并发基础","permalink":"https://shang.at/tags/并发基础/"},{"name":"内存模型JMM","slug":"内存模型JMM","permalink":"https://shang.at/tags/内存模型JMM/"},{"name":"Scheduler","slug":"Scheduler","permalink":"https://shang.at/tags/Scheduler/"},{"name":"BloomFilter","slug":"BloomFilter","permalink":"https://shang.at/tags/BloomFilter/"},{"name":"外排-分组归并-桶排序","slug":"外排-分组归并-桶排序","permalink":"https://shang.at/tags/外排-分组归并-桶排序/"},{"name":"并发编程-Future","slug":"并发编程-Future","permalink":"https://shang.at/tags/并发编程-Future/"},{"name":"常见操作技巧","slug":"常见操作技巧","permalink":"https://shang.at/tags/常见操作技巧/"},{"name":"lru_cache","slug":"lru-cache","permalink":"https://shang.at/tags/lru-cache/"},{"name":"并发编程-Thread","slug":"并发编程-Thread","permalink":"https://shang.at/tags/并发编程-Thread/"},{"name":"并发编程","slug":"并发编程","permalink":"https://shang.at/tags/并发编程/"},{"name":"JVM","slug":"JVM","permalink":"https://shang.at/tags/JVM/"},{"name":"wait&notify","slug":"wait-notify","permalink":"https://shang.at/tags/wait-notify/"},{"name":"Timer","slug":"Timer","permalink":"https://shang.at/tags/Timer/"},{"name":"NIO","slug":"NIO","permalink":"https://shang.at/tags/NIO/"},{"name":"JAVA-Iterator","slug":"JAVA-Iterator","permalink":"https://shang.at/tags/JAVA-Iterator/"},{"name":"函数式编程","slug":"函数式编程","permalink":"https://shang.at/tags/函数式编程/"},{"name":"JAVA集合类-列表","slug":"JAVA集合类-列表","permalink":"https://shang.at/tags/JAVA集合类-列表/"},{"name":"IPC&RPC","slug":"IPC-RPC","permalink":"https://shang.at/tags/IPC-RPC/"},{"name":"动态代理","slug":"动态代理","permalink":"https://shang.at/tags/动态代理/"},{"name":"mysql事务和隔离级别","slug":"mysql事务和隔离级别","permalink":"https://shang.at/tags/mysql事务和隔离级别/"},{"name":"mysql第三范式","slug":"mysql第三范式","permalink":"https://shang.at/tags/mysql第三范式/"},{"name":"python学习","slug":"python学习","permalink":"https://shang.at/tags/python学习/"},{"name":"OrderedDict","slug":"OrderedDict","permalink":"https://shang.at/tags/OrderedDict/"},{"name":"bisect","slug":"bisect","permalink":"https://shang.at/tags/bisect/"},{"name":"查找算法","slug":"查找算法","permalink":"https://shang.at/tags/查找算法/"},{"name":"Spark的scala隐式转换","slug":"Spark的scala隐式转换","permalink":"https://shang.at/tags/Spark的scala隐式转换/"},{"name":"Hadoop-IPC","slug":"Hadoop-IPC","permalink":"https://shang.at/tags/Hadoop-IPC/"},{"name":"Pandas","slug":"Pandas","permalink":"https://shang.at/tags/Pandas/"},{"name":"python中的时间处理","slug":"python中的时间处理","permalink":"https://shang.at/tags/python中的时间处理/"},{"name":"Configuration","slug":"Configuration","permalink":"https://shang.at/tags/Configuration/"},{"name":"队列","slug":"队列","permalink":"https://shang.at/tags/队列/"},{"name":"广播变量","slug":"广播变量","permalink":"https://shang.at/tags/广播变量/"},{"name":"算法","slug":"算法","permalink":"https://shang.at/tags/算法/"},{"name":"pivot透视图","slug":"pivot透视图","permalink":"https://shang.at/tags/pivot透视图/"},{"name":"Spark-tips","slug":"Spark-tips","permalink":"https://shang.at/tags/Spark-tips/"},{"name":"Tableau","slug":"Tableau","permalink":"https://shang.at/tags/Tableau/"},{"name":"sparkSql-DSL语法","slug":"sparkSql-DSL语法","permalink":"https://shang.at/tags/sparkSql-DSL语法/"},{"name":"排序算法","slug":"排序算法","permalink":"https://shang.at/tags/排序算法/"},{"name":"reduce","slug":"reduce","permalink":"https://shang.at/tags/reduce/"},{"name":"数据分析技巧","slug":"数据分析技巧","permalink":"https://shang.at/tags/数据分析技巧/"},{"name":"有初始值的聚合操作","slug":"有初始值的聚合操作","permalink":"https://shang.at/tags/有初始值的聚合操作/"},{"name":"分析指标","slug":"分析指标","permalink":"https://shang.at/tags/分析指标/"},{"name":"抽样方法和自增ID","slug":"抽样方法和自增ID","permalink":"https://shang.at/tags/抽样方法和自增ID/"},{"name":"sparkSql内置函数","slug":"sparkSql内置函数","permalink":"https://shang.at/tags/sparkSql内置函数/"},{"name":"数据分析Tips","slug":"数据分析Tips","permalink":"https://shang.at/tags/数据分析Tips/"},{"name":"窗口函数","slug":"窗口函数","permalink":"https://shang.at/tags/窗口函数/"},{"name":"union","slug":"union","permalink":"https://shang.at/tags/union/"},{"name":"pyspark官方文档学习","slug":"pyspark官方文档学习","permalink":"https://shang.at/tags/pyspark官方文档学习/"},{"name":"复杂度分析","slug":"复杂度分析","permalink":"https://shang.at/tags/复杂度分析/"},{"name":"数据结构","slug":"数据结构","permalink":"https://shang.at/tags/数据结构/"},{"name":"描述性统计和探索型数据分析","slug":"描述性统计和探索型数据分析","permalink":"https://shang.at/tags/描述性统计和探索型数据分析/"},{"name":"JVM问题调试","slug":"JVM问题调试","permalink":"https://shang.at/tags/JVM问题调试/"},{"name":"css","slug":"css","permalink":"https://shang.at/tags/css/"}]}